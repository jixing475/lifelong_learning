[{"authors":["admin"],"categories":null,"content":"Hello, my name is Jixing Liu. I‚Äôm a Data scientist at Deepdrug. I love to analysis data and make things in R and Python. Working on AI for medical, biology and chemistry, visualization styles, modeling techniques and general workflow problems.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1567641600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hello, my name is Jixing Liu. I‚Äôm a Data scientist at Deepdrug. I love to analysis data and make things in R and Python. Working on AI for medical, biology and chemistry, visualization styles, modeling techniques and general workflow problems.","tags":null,"title":"Jixing Liu","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":["Tools"],"content":" load pkgs library(ggplot2) library(dplyr) #install.packages(\u0026quot;ggcharts\u0026quot;) data(biomedicalrevenue, package = \u0026quot;ggcharts\u0026quot;)  example plot \u0026lt;- biomedicalrevenue %\u0026gt;% filter(company %in% c(\u0026quot;Roche\u0026quot;, \u0026quot;Novartis\u0026quot;)) %\u0026gt;% ggplot(aes(year, revenue, color = company)) + geom_line(size = 1.2) + ggtitle( paste0( \u0026quot;\u0026lt;span style = \u0026#39;color:darkred\u0026#39;\u0026gt;__Roche__\u0026lt;/span\u0026gt;\u0026quot;, \u0026quot; *overtook* \u0026lt;span style = \u0026#39;color:darkorange\u0026#39;\u0026gt;**Novartis**\u0026lt;/span\u0026gt;\u0026quot;, \u0026quot; in 2016\u0026quot; ) ) + scale_color_manual( values = c(\u0026quot;Roche\u0026quot; = \u0026quot;#93C1DE\u0026quot;, \u0026quot;Novartis\u0026quot; = \u0026quot;darkorange\u0026quot;), guide = \u0026quot;none\u0026quot; ) + ggcharts::theme_hermit(ticks = \u0026quot;x\u0026quot;, grid = \u0026quot;X\u0026quot;) + theme(plot.title = ggtext::element_markdown()) plot  ËØ≠Ê≥ï ÁõÆÂâç ggtext ÊîØÊåÅÁöÑ markdwon ËØ≠Ê≥ïÊúâÈôê\n üì¶ {mdthemes} #remotes::install_github(\u0026quot;thomas-neitmann/mdthemes\u0026quot;) Âçï‰∏™‰∏ªÈ¢òÂÖÉÁ¥†ËÆæÁΩÆ Markdown ËØ≠Ê≥ïÂ§™Ë¥π‰∫ã, ‰ΩøÁî® mdthemes üì¶, ‰∏ÄÊ¨°ÊÄßËÆæÁΩÆ.\n Setting individual theme elements to¬†ggtext::element_markdown()¬†can add quite a bit of boilerplate code to your plot.\n plot + mdthemes::md_theme_minimal()  title, the subtitle, axis labels and captions set one-time  Apart from the title, the subtitle, axis labels and captions are set to element_markdown() for all mdthemes.\n plot + labs( x = \u0026quot;**Year**\u0026quot;, y = \u0026quot;Revenue (*Billion* USD)\u0026quot;, caption = \u0026quot;Data Source: *en.wikipedia.org/wiki/List_of_largest_biomedical_companies_by_revenue*\u0026quot; ) + mdthemes::md_theme_minimal()  mdthemes ÂåÖÂê´ÁöÑ‰∏ªÈ¢òÈùûÂ∏∏‰∏∞ÂØå, ‰∏îÂèØËá™ÂÆö‰πâ‰∏ªÈ¢ò  The {mdthemes} packages currently contains all themes from {ggplot2}, {ggthemes}, {hrbrthemes}, {tvthemes} and {cowplot} with support for rendering text as markdown.\n  If you want to turn a theme that is not part of the {mdthemes} package into an md_theme you can use the as_md_theme() function.\n plot + mdthemes::as_md_theme(hrbrthemes::theme_ipsum())   links Themes ‚Ä¢ ggcharts\n ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590978219,"objectID":"edb9f05d30e4151d5c55ad93fd9e6430","permalink":"/post/taste-ggtext/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/post/taste-ggtext/","section":"post","summary":"load pkgs library(ggplot2) library(dplyr) #install.packages(\u0026quot;ggcharts\u0026quot;) data(biomedicalrevenue, package = \u0026quot;ggcharts\u0026quot;)  example plot \u0026lt;- biomedicalrevenue %\u0026gt;% filter(company %in% c(\u0026quot;Roche\u0026quot;, \u0026quot;Novartis\u0026quot;)) %\u0026gt;% ggplot(aes(year, revenue, color = company)) + geom_line(size = 1.2) + ggtitle( paste0( \u0026quot;\u0026lt;span style = \u0026#39;color:darkred\u0026#39;\u0026gt;__Roche__\u0026lt;/span\u0026gt;\u0026quot;, \u0026quot; *overtook* \u0026lt;span style = \u0026#39;color:darkorange\u0026#39;\u0026gt;**Novartis**\u0026lt;/span\u0026gt;\u0026quot;, \u0026quot; in 2016\u0026quot; ) ) + scale_color_manual( values = c(\u0026quot;Roche\u0026quot; = \u0026quot;#93C1DE\u0026quot;, \u0026quot;Novartis\u0026quot; = \u0026quot;darkorange\u0026quot;), guide = \u0026quot;none\u0026quot; ) + ggcharts::theme_hermit(ticks = \u0026quot;x\u0026quot;, grid = \u0026quot;X\u0026quot;) + theme(plot.","tags":["vis"],"title":"taste ggtext","type":"post"},{"authors":[],"categories":["Tools"],"content":" install pkg # remotes::install_github(\u0026quot;rstudio/thematic\u0026quot;) library(thematic) library(thematic) thematic_on( bg = \u0026quot;#222222\u0026quot;, fg = \u0026quot;white\u0026quot;, accent = \u0026quot;#0CE3AC\u0026quot;, font = \u0026quot;auto\u0026quot; ) library(ggplot2) ggplot(mtcars, aes(wt, mpg)) + geom_point() + geom_smooth() ## Error : RStudio not running ggplot(mtcars, aes(wt, mpg)) + geom_point(aes(color = factor(cyl))) + geom_smooth(color = \u0026quot;white\u0026quot;) ## Error : RStudio not running  links Unified and Automatic Theming of ggplot2, lattice, and base R Graphics ‚Ä¢ thematic\n ","date":1588636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588666978,"objectID":"da65bd7c4cc80810c6bb15a564ad5775","permalink":"/post/thematic/","publishdate":"2020-05-05T00:00:00Z","relpermalink":"/post/thematic/","section":"post","summary":"install pkg # remotes::install_github(\u0026quot;rstudio/thematic\u0026quot;) library(thematic) library(thematic) thematic_on( bg = \u0026quot;#222222\u0026quot;, fg = \u0026quot;white\u0026quot;, accent = \u0026quot;#0CE3AC\u0026quot;, font = \u0026quot;auto\u0026quot; ) library(ggplot2) ggplot(mtcars, aes(wt, mpg)) + geom_point() + geom_smooth() ## Error : RStudio not running ggplot(mtcars, aes(wt, mpg)) + geom_point(aes(color = factor(cyl))) + geom_smooth(color = \u0026quot;white\u0026quot;) ## Error : RStudio not running  links Unified and Automatic Theming of ggplot2, lattice, and base R Graphics ‚Ä¢ thematic","tags":["vis"],"title":"ggplot2 thematic","type":"post"},{"authors":[],"categories":["Tools"],"content":" setup  load pkgs library(visR) library(survival) library(gt)  set var # Constants DATASET \u0026lt;- paste0(\u0026quot;NCCTG Lung Cancer Dataset (from survival package \u0026quot;, packageVersion(\u0026quot;survival\u0026quot;), \u0026quot;)\u0026quot;) #attrition_chart_fn \u0026lt;- \u0026quot;data/attrition_diagram.svg\u0026quot; # Globql formatting options options(digits = 3) # Global table settings options(DT.options = list(pageLength = 10, language = list(search = \u0026#39;Filter:\u0026#39;), scrollX = TRUE))  load data data(lung) lung_cohort \u0026lt;- lung %\u0026gt;% rename(ECOG = ph.ecog, Karnofsky = ph.karno, institution = inst ) %\u0026gt;% mutate(patid = paste0(\u0026quot;Pat\u0026quot;, row_number()), institution = factor(institution), sex = factor(if_else(sex == 1, \u0026quot;male\u0026quot;, \u0026quot;female\u0026quot;)), ECOG = factor(case_when(ECOG == 0 ~ \u0026quot;0 asymptomatic\u0026quot;, ECOG == 1 ~ \u0026quot;1 ambulatory\u0026quot;, ECOG == 2 ~ \u0026quot;2 in bed less than 50% of day\u0026quot;, ECOG == 3 ~ \u0026quot;3 in bed more than 50% of day\u0026quot;, ECOG == 4 ~ \u0026quot;4 bedbound\u0026quot;, ECOG == 5 ~ \u0026quot;5 dead\u0026quot;)), dx_age_group = factor(case_when(age \u0026lt; 30 ~ \u0026quot;\u0026lt; 30y\u0026quot;, age \u0026gt;= 30 \u0026amp; age \u0026lt;= 50 ~ \u0026quot;30-50y\u0026quot;, age \u0026gt; 50 \u0026amp; age \u0026lt;= 70 ~ \u0026quot;51-70y\u0026quot;, age \u0026gt; 70 ~ \u0026quot;\u0026gt; 70y\u0026quot;), levels=c(\u0026quot;\u0026lt; 30y\u0026quot;, \u0026quot;30-50y\u0026quot;, \u0026quot;51-70y\u0026quot;, \u0026quot;\u0026gt; 70y\u0026quot;)), status = if_else(status == 1, 0, 1)) %\u0026gt;% select(-meal.cal, -pat.karno)  EDA skimr::skim(lung_cohort)  Table 1: Data summary          Name  lung_cohort    Number of rows  228    Number of columns  10    _______________________     Column type frequency:     character  1    factor  4    numeric  5    ________________________     Group variables  None     Variable type: character\n   skim_variable  n_missing  complete_rate  min  max  empty  n_unique  whitespace      patid  0  1  4  6  0  228  0     Variable type: factor\n   skim_variable  n_missing  complete_rate  ordered  n_unique  top_counts      institution  1  1  FALSE  18  1: 36, 12: 23, 13: 20, 3: 19    sex  0  1  FALSE  2  mal: 138, fem: 90    ECOG  1  1  FALSE  4  1 a: 113, 0 a: 63, 2 i: 50, 3 i: 1    dx_age_group  0  1  FALSE  3  51-: 156, \u0026gt; 7: 46, 30-: 26, \u0026lt; 3: 0     Variable type: numeric\n   skim_variable  n_missing  complete_rate  mean  sd  p0  p25  p50  p75  p100  hist      time  0  1.00  305.23  210.65  5  167  256  396.5  1022  ‚ñá‚ñá‚ñÉ‚ñÇ‚ñÅ    status  0  1.00  0.72  0.45  0  0  1  1.0  1  ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñá    age  0  1.00  62.45  9.07  39  56  63  69.0  82  ‚ñÇ‚ñÖ‚ñá‚ñá‚ñÉ    Karnofsky  1  1.00  81.94  12.33  50  75  80  90.0  100  ‚ñÉ‚ñÉ‚ñá‚ñá‚ñÉ    wt.loss  14  0.94  9.83  13.14  -24  0  7  15.8  68  ‚ñÅ‚ñá‚ñÉ‚ñÅ‚ñÅ      follow chart Cohort Selection For this exercise we will only include patients with (1) ECOG available (2) and non-missing weight-loss data in our analysis\nAttrition Table ËøáÊª§Ë°®\ncohort_attrition \u0026lt;- vr_attrition_table( data = lung_cohort, criteria_descriptions = c(\u0026quot;1. ECOG available\u0026quot;, \u0026quot;2. Weight loss data available\u0026quot;, \u0026quot;3. Non-missing vital status\u0026quot;, \u0026quot;4. Positive follow up time\u0026quot;), criteria_conditions = c(\u0026quot;!is.na(ECOG)\u0026quot;, \u0026quot;!is.na(wt.loss)\u0026quot;, \u0026quot;!is.na(status)\u0026quot;, \u0026quot;time \u0026gt;= 0\u0026quot;), subject_column_name = \u0026#39;patid\u0026#39;) vr_render_table(data = cohort_attrition, title = \u0026quot;Attrition Table\u0026quot;, caption = \u0026quot;Summary of samples fulfilling inclusion/exclusion criteria\u0026quot;, datasource = DATASET, engine = \u0026quot;gt\u0026quot;) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #slpiojvkjb .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #slpiojvkjb .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #slpiojvkjb .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #slpiojvkjb .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #slpiojvkjb .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #slpiojvkjb .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #slpiojvkjb .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #slpiojvkjb .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #slpiojvkjb .gt_column_spanner_outer:first-child { padding-left: 0; } #slpiojvkjb .gt_column_spanner_outer:last-child { padding-right: 0; } #slpiojvkjb .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #slpiojvkjb .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #slpiojvkjb .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #slpiojvkjb .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #slpiojvkjb .gt_from_md  :first-child { margin-top: 0; } #slpiojvkjb .gt_from_md  :last-child { margin-bottom: 0; } #slpiojvkjb .gt_row { padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #slpiojvkjb .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #slpiojvkjb .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #slpiojvkjb .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #slpiojvkjb .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #slpiojvkjb .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #slpiojvkjb .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #slpiojvkjb .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #slpiojvkjb .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #slpiojvkjb .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #slpiojvkjb .gt_sourcenote { font-size: 90%; padding: 4px; } #slpiojvkjb .gt_left { text-align: left; } #slpiojvkjb .gt_center { text-align: center; } #slpiojvkjb .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #slpiojvkjb .gt_font_normal { font-weight: normal; } #slpiojvkjb .gt_font_bold { font-weight: bold; } #slpiojvkjb .gt_font_italic { font-style: italic; } #slpiojvkjb .gt_super { font-size: 65%; } #slpiojvkjb .gt_footnote_marks { font-style: italic; font-size: 65%; }   Attrition Table   Summary of samples fulfilling inclusion/exclusion criteria    Criteria Condition Remaining N Remaining % Excluded N Excluded %    Total cohort size none 228.00 100.00 0.00 0.00   1. ECOG available !is.na(ECOG) 227.00 99.56 1.00 0.44   2. Weight loss data available !is.na(wt.loss) 213.00 93.42 14.00 6.14   3. Non-missing vital status !is.na(status) 213.00 93.42 0.00 0.00   4. Positive follow up time time \u0026gt;= 0 213.00 93.42 0.00 0.00    Data Source: NCCTG Lung Cancer Dataset (from survival package 3.1.11)     flow-chart diagram complement_descriptions \u0026lt;- c( \u0026quot;Having exclusion criterion: non ECOG\u0026quot;, \u0026quot;Having exclusion criterion: missing weight loss \u0026quot;, \u0026quot;Having exclusion criterion: missing vital status\u0026quot;, \u0026quot;Having exclusion criterion: negative follow up time\u0026quot; ) # Create attrition flowchart #attrition_chart_fn \u0026lt;- \u0026quot;attrition_diagram.svg\u0026quot; # attrition_flow \u0026lt;- vr_attrition( # N_array = cohort_attrition$`Remaining N`, # descriptions = cohort_attrition$Criteria, # complement_descriptions = complement_descriptions, # output_path = attrition_chart_fn) # knitr::include_graphics(attrition_chart_fn)  analysis data lung_cohort \u0026lt;- lung_cohort %\u0026gt;% filter(!is.na(ECOG), !is.na(wt.loss), !is.na(status), time \u0026gt;= 0)   Table 1 Cohort Overview\nif the there are systematic differences between the chemotherapy and hormone therapy arms.\n# some modifications to the table for nice printing of categories lung_cohort_tab1 \u0026lt;- lung_cohort %\u0026gt;% mutate(status = factor(case_when(status == 0 ~ \u0026quot;Alive/Censored\u0026quot;, status == 1 ~ \u0026quot;Dead\u0026quot;, is.na(status) ~ \u0026quot;Missing\u0026quot;))) %\u0026gt;% filter(!is.na(time) \u0026amp; !is.na(age) \u0026amp; !is.na(Karnofsky)) %\u0026gt;% select(-institution) # visR table1 convenience function vr_table_one( data = lung_cohort_tab1, title = \u0026quot;Overview over Lung Cancer patients\u0026quot;, caption = \u0026quot;Baseline characteristics of study cohort stratified by treatment type\u0026quot;, datasource = DATASET, groupCols = c(\u0026quot;sex\u0026quot;) ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ohoaeodhnp .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ohoaeodhnp .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ohoaeodhnp .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ohoaeodhnp .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #ohoaeodhnp .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ohoaeodhnp .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ohoaeodhnp .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ohoaeodhnp .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ohoaeodhnp .gt_column_spanner_outer:first-child { padding-left: 0; } #ohoaeodhnp .gt_column_spanner_outer:last-child { padding-right: 0; } #ohoaeodhnp .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #ohoaeodhnp .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #ohoaeodhnp .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ohoaeodhnp .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ohoaeodhnp .gt_from_md  :first-child { margin-top: 0; } #ohoaeodhnp .gt_from_md  :last-child { margin-bottom: 0; } #ohoaeodhnp .gt_row { padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ohoaeodhnp .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #ohoaeodhnp .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ohoaeodhnp .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #ohoaeodhnp .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ohoaeodhnp .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ohoaeodhnp .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ohoaeodhnp .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ohoaeodhnp .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #ohoaeodhnp .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ohoaeodhnp .gt_sourcenote { font-size: 90%; padding: 4px; } #ohoaeodhnp .gt_left { text-align: left; } #ohoaeodhnp .gt_center { text-align: center; } #ohoaeodhnp .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ohoaeodhnp .gt_font_normal { font-weight: normal; } #ohoaeodhnp .gt_font_bold { font-weight: bold; } #ohoaeodhnp .gt_font_italic { font-style: italic; } #ohoaeodhnp .gt_super { font-size: 65%; } #ohoaeodhnp .gt_footnote_marks { font-style: italic; font-size: 65%; }   Overview over Lung Cancer patients   Baseline characteristics of study cohort stratified by treatment type     female male Overall   Sample   N 86 127 213  time   Mean (SD) 345 (205) 295 (216) 315 (213)   Median (IQR) 294 (201-467) 229 (163-392) 269 (176-428)   Min-max 5-965 11-1022 5-1022   Missing 0 (0%) 0 (0%) 0 (0%)  status   Alive/Censored 37 (43%) 25 (19.7%) 62 (29.1%)   Dead 49 (57%) 102 (80.3%) 151 (70.9%)  age   Mean (SD) 61.1 (9.01) 63.5 (9.25) 62.5 (9.21)   Median (IQR) 61 (55-68.8) 64 (57-70) 63 (56-70)   Min-max 41-77 39-82 39-82   Missing 0 (0%) 0 (0%) 0 (0%)  ECOG   0 asymptomatic 26 (30.2%) 35 (27.559%) 61 (28.638%)   1 ambulatory 41 (47.7%) 65 (51.181%) 106 (49.765%)   2 in bed less than 50% of day 19 (22.1%) 26 (20.472%) 45 (21.127%)   3 in bed more than 50% of day NA 1 (0.787%) 1 (0.469%)  Karnofsky   Mean (SD) 82.6 (12) 81.9 (12.5) 82.2 (12.2)   Median (IQR) 80 (80-90) 80 (70-90) 80 (80-90)   Min-max 50-100 50-100 50-100   Missing 0 (0%) 0 (0%) 0 (0%)  wt.loss   Mean (SD) 7.77 (13.2) 11.1 (12.9) 9.73 (13.1)   Median (IQR) 4 (0-11) 8 (0.5-18) 7 (0-15)   Min-max -24-52 -13-68 -24-68   Missing 0 (0%) 0 (0%) 0 (0%)  patid   Unique values 86 127 213   Missing (%) 0 (0%) 0 (0%) 0 (0%)  dx_age_group   30-50y 12 (14.0%) 13 (10.2%) 25 (11.7%)   51-70y 59 (68.6%) 85 (66.9%) 144 (67.6%)   \u0026gt; 70y 15 (17.4%) 29 (22.8%) 44 (20.7%)    Data Source: NCCTG Lung Cancer Dataset (from survival package 3.1.11)     Kaplan-Meier Curve surv_eq \u0026lt;- \u0026quot;survival::Surv(time, status) ~ sex\u0026quot; vr_kaplan_meier( lung_cohort, equation = surv_eq, data_source = DATASET, time_unit = \u0026quot;days\u0026quot;, title = \u0026quot;Comparison of survival in male and female lung cancer patients\u0026quot; ) Summary of Kaplan Meier Curve km_summary \u0026lt;- vr_kaplan_meier_summary(lung_cohort, equation = surv_eq) km_summary[[1]] %\u0026gt;% vr_render_table(\u0026quot;Overall Survival\u0026quot;, \u0026quot;Median survival times in days for each strata\u0026quot;, DATASET) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #sthatfbkcd .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #sthatfbkcd .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #sthatfbkcd .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #sthatfbkcd .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #sthatfbkcd .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #sthatfbkcd .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #sthatfbkcd .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #sthatfbkcd .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #sthatfbkcd .gt_column_spanner_outer:first-child { padding-left: 0; } #sthatfbkcd .gt_column_spanner_outer:last-child { padding-right: 0; } #sthatfbkcd .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #sthatfbkcd .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #sthatfbkcd .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #sthatfbkcd .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #sthatfbkcd .gt_from_md  :first-child { margin-top: 0; } #sthatfbkcd .gt_from_md  :last-child { margin-bottom: 0; } #sthatfbkcd .gt_row { padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #sthatfbkcd .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #sthatfbkcd .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #sthatfbkcd .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #sthatfbkcd .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #sthatfbkcd .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #sthatfbkcd .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #sthatfbkcd .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #sthatfbkcd .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #sthatfbkcd .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #sthatfbkcd .gt_sourcenote { font-size: 90%; padding: 4px; } #sthatfbkcd .gt_left { text-align: left; } #sthatfbkcd .gt_center { text-align: center; } #sthatfbkcd .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #sthatfbkcd .gt_font_normal { font-weight: normal; } #sthatfbkcd .gt_font_bold { font-weight: bold; } #sthatfbkcd .gt_font_italic { font-style: italic; } #sthatfbkcd .gt_super { font-size: 65%; } #sthatfbkcd .gt_footnote_marks { font-style: italic; font-size: 65%; }   Overall Survival   Median survival times in days for each strata    strata # persons # events median survival time 0.95LCL 0.95UCL    female 86.00 49.00 433.00 351.00 641.00   male 127.00 102.00 284.00 223.00 337.00    Data Source: NCCTG Lung Cancer Dataset (from survival package 3.1.11)    km_summary[[2]] %\u0026gt;% vr_render_table(\u0026quot;Equality between Strata\u0026quot;, \u0026quot;Summary table with test of equality over strata\u0026quot;, DATASET) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #npvtebkqev .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #npvtebkqev .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #npvtebkqev .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #npvtebkqev .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #npvtebkqev .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #npvtebkqev .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #npvtebkqev .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #npvtebkqev .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #npvtebkqev .gt_column_spanner_outer:first-child { padding-left: 0; } #npvtebkqev .gt_column_spanner_outer:last-child { padding-right: 0; } #npvtebkqev .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #npvtebkqev .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #npvtebkqev .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #npvtebkqev .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #npvtebkqev .gt_from_md  :first-child { margin-top: 0; } #npvtebkqev .gt_from_md  :last-child { margin-bottom: 0; } #npvtebkqev .gt_row { padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #npvtebkqev .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #npvtebkqev .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #npvtebkqev .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #npvtebkqev .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #npvtebkqev .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #npvtebkqev .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #npvtebkqev .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #npvtebkqev .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #npvtebkqev .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #npvtebkqev .gt_sourcenote { font-size: 90%; padding: 4px; } #npvtebkqev .gt_left { text-align: left; } #npvtebkqev .gt_center { text-align: center; } #npvtebkqev .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #npvtebkqev .gt_font_normal { font-weight: normal; } #npvtebkqev .gt_font_bold { font-weight: bold; } #npvtebkqev .gt_font_italic { font-style: italic; } #npvtebkqev .gt_super { font-size: 65%; } #npvtebkqev .gt_footnote_marks { font-style: italic; font-size: 65%; }   Equality between Strata   Summary table with test of equality over strata    Test statistic df p.value Description    Log-rank 9.60 1.00 0.00 Log-rank gives more weight on higher values of time   Wilcoxon 11.84 1.00 0.00 Wilcoxon gives more weight on lower values of time   Tarone-Ware 11.63 1.00 0.00 Tarone-Ware is in between    Data Source: NCCTG Lung Cancer Dataset (from survival package 3.1.11)      ","date":1588550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588587529,"objectID":"5e01e002053a0261a6efef211eb83041","permalink":"/post/example-time-to-event-survival-analysis-using-visr/","publishdate":"2020-05-04T00:00:00Z","relpermalink":"/post/example-time-to-event-survival-analysis-using-visr/","section":"post","summary":"setup  load pkgs library(visR) library(survival) library(gt)  set var # Constants DATASET \u0026lt;- paste0(\u0026quot;NCCTG Lung Cancer Dataset (from survival package \u0026quot;, packageVersion(\u0026quot;survival\u0026quot;), \u0026quot;)\u0026quot;) #attrition_chart_fn \u0026lt;- \u0026quot;data/attrition_diagram.svg\u0026quot; # Globql formatting options options(digits = 3) # Global table settings options(DT.","tags":["clinical"],"title":"Example Time To Event (Survival) Analysis using visR","type":"post"},{"authors":[],"categories":["Tools"],"content":" SETUP  Load library library(tidymodels)  EDA iris %\u0026gt;% janitor::clean_names() %\u0026gt;% set_names(colnames(.) %\u0026gt;% str_replace_all(., \u0026quot;_\u0026quot;, \u0026quot; \u0026quot;) %\u0026gt;% str_to_title()) %\u0026gt;% select_if(is.numeric) %\u0026gt;% gather() %\u0026gt;% ggplot(aes(x = value)) + facet_wrap(~ key, scales = \u0026quot;free\u0026quot;, ncol = 4) + geom_histogram() iris %\u0026gt;% janitor::clean_names() %\u0026gt;% set_names(colnames(.) %\u0026gt;% str_replace_all(., \u0026quot;_\u0026quot;, \u0026quot; \u0026quot;) %\u0026gt;% str_to_title()) %\u0026gt;% select_if(is.factor) %\u0026gt;% gather() %\u0026gt;% ggplot(aes(x = value)) + geom_bar()  split df \u0026lt;- iris set.seed(seed = 1972) train_test_split \u0026lt;- rsample::initial_split( data = df, prop = 0.6 ) train_test_split ## \u0026lt;90/60/150\u0026gt; train_tbl \u0026lt;- train_test_split %\u0026gt;% training() test_tbl \u0026lt;- train_test_split %\u0026gt;% testing()   recipes recipe_simple \u0026lt;- function(dataset) { recipe(Species ~ ., data = dataset ) %\u0026gt;% step_corr(all_predictors()) %\u0026gt;% step_center(all_predictors(), -all_outcomes()) %\u0026gt;% step_scale(all_predictors(), -all_outcomes()) %\u0026gt;% prep(dataset) } recipe_prepped \u0026lt;- recipe_simple(dataset = train_tbl) train_baked \u0026lt;- bake(recipe_prepped, new_data = train_tbl) test_baked \u0026lt;- bake(recipe_prepped, new_data = test_tbl) recipe_prepped ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 4 ## ## Training data contained 90 data points and no missing data. ## ## Operations: ## ## Correlation filter removed Petal.Length [trained] ## Centering for Sepal.Length, Sepal.Width, Petal.Width [trained] ## Scaling for Sepal.Length, Sepal.Width, Petal.Width [trained]  Model Training iris_ranger \u0026lt;- rand_forest(trees = 100, mode = \u0026quot;classification\u0026quot;) %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;% fit(Species ~ ., data = train_baked)  Predictions iris_ranger %\u0026gt;% predict(test_baked) %\u0026gt;% bind_cols(test_baked) %\u0026gt;% glimpse() ## Rows: 60 ## Columns: 5 ## $ .pred_class \u0026lt;fct\u0026gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, ‚Ä¶ ## $ Sepal.Length \u0026lt;dbl\u0026gt; -0.8625644, -1.4471337, -0.9794783, -0.5118229, -0.97947‚Ä¶ ## $ Sepal.Width \u0026lt;dbl\u0026gt; 1.2051348, 0.2410270, 1.4461618, 2.1692426, 0.9641078, 0‚Ä¶ ## $ Petal.Width \u0026lt;dbl\u0026gt; -1.3494745, -1.3494745, -1.3494745, -1.0831309, -1.34947‚Ä¶ ## $ Species \u0026lt;fct\u0026gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, ‚Ä¶  Model Validation iris_ranger %\u0026gt;% predict(test_baked) %\u0026gt;% bind_cols(test_baked) %\u0026gt;% metrics(truth = Species, estimate = .pred_class) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 0.967 ## 2 kap multiclass 0.950  Per classifier metrics iris_ranger %\u0026gt;% predict(test_baked, type = \u0026quot;prob\u0026quot;) %\u0026gt;% bind_cols(test_baked) %\u0026gt;% roc_curve(Species, .pred_setosa:.pred_virginica) %\u0026gt;% autoplot()  links A Gentle Introduction to tidymodels ¬∑ R Views\n ","date":1587859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587895899,"objectID":"037e7c79f54581506c830bd9872e26b1","permalink":"/post/a-gentle-introduction-to-tidymodels/","publishdate":"2020-04-26T00:00:00Z","relpermalink":"/post/a-gentle-introduction-to-tidymodels/","section":"post","summary":"SETUP  Load library library(tidymodels)  EDA iris %\u0026gt;% janitor::clean_names() %\u0026gt;% set_names(colnames(.) %\u0026gt;% str_replace_all(., \u0026quot;_\u0026quot;, \u0026quot; \u0026quot;) %\u0026gt;% str_to_title()) %\u0026gt;% select_if(is.numeric) %\u0026gt;% gather() %\u0026gt;% ggplot(aes(x = value)) + facet_wrap(~ key, scales = \u0026quot;free\u0026quot;, ncol = 4) + geom_histogram() iris %\u0026gt;% janitor::clean_names() %\u0026gt;% set_names(colnames(.","tags":["Machine learning"],"title":"a gentle introduction to tidymodels","type":"post"},{"authors":[],"categories":["Tools"],"content":" Setup  Load pkg library(tidymodels) # Loads parsnip, rsample, recipes, yardstick library(skimr) # Quickly get a sense of data library(knitr)   Load Data telco \u0026lt;- read_csv(\u0026quot;data/WA_Fn-UseC_-Telco-Customer-Churn.csv\u0026quot;) telco %\u0026gt;% head() %\u0026gt;% kable()    customerID  gender  SeniorCitizen  Partner  Dependents  tenure  PhoneService  MultipleLines  InternetService  OnlineSecurity  OnlineBackup  DeviceProtection  TechSupport  StreamingTV  StreamingMovies  Contract  PaperlessBilling  PaymentMethod  MonthlyCharges  TotalCharges  Churn      7590-VHVEG  Female  0  Yes  No  1  No  No phone service  DSL  No  Yes  No  No  No  No  Month-to-month  Yes  Electronic check  29.85  29.85  No    5575-GNVDE  Male  0  No  No  34  Yes  No  DSL  Yes  No  Yes  No  No  No  One year  No  Mailed check  56.95  1889.50  No    3668-QPYBK  Male  0  No  No  2  Yes  No  DSL  Yes  Yes  No  No  No  No  Month-to-month  Yes  Mailed check  53.85  108.15  Yes    7795-CFOCW  Male  0  No  No  45  No  No phone service  DSL  Yes  No  Yes  Yes  No  No  One year  No  Bank transfer (automatic)  42.30  1840.75  No    9237-HQITU  Female  0  No  No  2  Yes  No  Fiber optic  No  No  No  No  No  No  Month-to-month  Yes  Electronic check  70.70  151.65  Yes    9305-CDSKC  Female  0  No  No  8  Yes  Yes  Fiber optic  No  No  Yes  No  Yes  Yes  Month-to-month  Yes  Electronic check  99.65  820.50  Yes      EDA: Skim the Data telco %\u0026gt;% skim()  Table 1: Data summary          Name  Piped data    Number of rows  7043    Number of columns  21    _______________________     Column type frequency:     character  17    numeric  4    ________________________     Group variables  None     Variable type: character\n   skim_variable  n_missing  complete_rate  min  max  empty  n_unique  whitespace      customerID  0  1  10  10  0  7043  0    gender  0  1  4  6  0  2  0    Partner  0  1  2  3  0  2  0    Dependents  0  1  2  3  0  2  0    PhoneService  0  1  2  3  0  2  0    MultipleLines  0  1  2  16  0  3  0    InternetService  0  1  2  11  0  3  0    OnlineSecurity  0  1  2  19  0  3  0    OnlineBackup  0  1  2  19  0  3  0    DeviceProtection  0  1  2  19  0  3  0    TechSupport  0  1  2  19  0  3  0    StreamingTV  0  1  2  19  0  3  0    StreamingMovies  0  1  2  19  0  3  0    Contract  0  1  8  14  0  3  0    PaperlessBilling  0  1  2  3  0  2  0    PaymentMethod  0  1  12  25  0  4  0    Churn  0  1  2  3  0  2  0     Variable type: numeric\n   skim_variable  n_missing  complete_rate  mean  sd  p0  p25  p50  p75  p100  hist      SeniorCitizen  0  1  0.16  0.37  0.00  0.00  0.00  0.00  1.00  ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÇ    tenure  0  1  32.37  24.56  0.00  9.00  29.00  55.00  72.00  ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÜ    MonthlyCharges  0  1  64.76  30.09  18.25  35.50  70.35  89.85  118.75  ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÖ    TotalCharges  11  1  2283.30  2266.77  18.80  401.45  1397.47  3794.74  8684.80  ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÅ     data processing telco \u0026lt;- telco %\u0026gt;% select(-customerID) %\u0026gt;% drop_na() df \u0026lt;- telco   Tidymodels Workflow  Train/Test Split set.seed(seed = 1972) train_test_split \u0026lt;- rsample::initial_split( data = df, prop = 0.80 ) train_test_split ## \u0026lt;5626/1406/7032\u0026gt; train_tbl \u0026lt;- train_test_split %\u0026gt;% training() test_tbl \u0026lt;- train_test_split %\u0026gt;% testing()   Prepare recipe_simple \u0026lt;- function(dataset) { recipe(Churn ~ ., data = dataset) %\u0026gt;% step_string2factor(all_nominal(), -all_outcomes()) %\u0026gt;% prep(data = dataset) } recipe_prepped \u0026lt;- recipe_simple(dataset = train_tbl) train_baked \u0026lt;- bake(recipe_prepped, new_data = train_tbl) test_baked \u0026lt;- bake(recipe_prepped, new_data = test_tbl) Generalized Linear Model (Baseline) Fit the Model logistic_glm \u0026lt;- logistic_reg(mode = \u0026quot;classification\u0026quot;) %\u0026gt;% set_engine(\u0026quot;glm\u0026quot;) %\u0026gt;% fit(Churn ~ ., data = train_baked)  Assess Performance predictions_glm \u0026lt;- logistic_glm %\u0026gt;% predict(new_data = test_baked) %\u0026gt;% bind_cols(test_baked %\u0026gt;% select(Churn)) predictions_glm %\u0026gt;% head() %\u0026gt;% kable()    .pred_class  Churn      Yes  No    No  No    No  Yes    No  No    Yes  Yes    No  No      Accuracy, Precision , Recall and F1 Score tibble( \u0026quot;accuracy\u0026quot; = accuracy(predictions_glm, Churn, .pred_class) %\u0026gt;% select(.estimate), \u0026quot;precision\u0026quot; = precision(predictions_glm, Churn, .pred_class) %\u0026gt;% select(.estimate), \u0026quot;recall\u0026quot; = recall(predictions_glm, Churn, .pred_class) %\u0026gt;% select(.estimate), \u0026quot;F1 score\u0026quot; = f_meas(predictions_glm, Churn, .pred_class) %\u0026gt;% select(.estimate), ) %\u0026gt;% unnest(cols = c(accuracy, precision, recall, `F1 score`)) %\u0026gt;% kable()    accuracy  precision  recall  F1 score      0.8058321  0.8376916  0.909002  0.8718911       Random Forest Cross Validation - 10-Fold cross_val_tbl \u0026lt;- vfold_cv(train_tbl, v = 10) cross_val_tbl ## # 10-fold cross-validation ## # A tibble: 10 x 2 ## splits id ## \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026lt;split [5.1K/563]\u0026gt; Fold01 ## 2 \u0026lt;split [5.1K/563]\u0026gt; Fold02 ## 3 \u0026lt;split [5.1K/563]\u0026gt; Fold03 ## 4 \u0026lt;split [5.1K/563]\u0026gt; Fold04 ## 5 \u0026lt;split [5.1K/563]\u0026gt; Fold05 ## 6 \u0026lt;split [5.1K/563]\u0026gt; Fold06 ## 7 \u0026lt;split [5.1K/562]\u0026gt; Fold07 ## 8 \u0026lt;split [5.1K/562]\u0026gt; Fold08 ## 9 \u0026lt;split [5.1K/562]\u0026gt; Fold09 ## 10 \u0026lt;split [5.1K/562]\u0026gt; Fold10 cross_val_tbl %\u0026gt;% pluck(\u0026quot;splits\u0026quot;, 1) ## \u0026lt;5063/563/5626\u0026gt;  Random Forest rf_fun \u0026lt;- function(split, id, try, tree) { analysis_set \u0026lt;- split %\u0026gt;% analysis() analysis_prepped \u0026lt;- analysis_set %\u0026gt;% recipe_simple() analysis_baked \u0026lt;- analysis_prepped %\u0026gt;% bake(new_data = analysis_set) model_rf \u0026lt;- rand_forest( mode = \u0026quot;classification\u0026quot;, mtry = try, trees = tree ) %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;, importance = \u0026quot;impurity\u0026quot; ) %\u0026gt;% fit(Churn ~ ., data = analysis_baked) assessment_set \u0026lt;- split %\u0026gt;% assessment() assessment_prepped \u0026lt;- assessment_set %\u0026gt;% recipe_simple() assessment_baked \u0026lt;- assessment_prepped %\u0026gt;% bake(new_data = assessment_set) tibble( \u0026quot;id\u0026quot; = id, \u0026quot;truth\u0026quot; = assessment_baked$Churn, \u0026quot;prediction\u0026quot; = model_rf %\u0026gt;% predict(new_data = assessment_baked) %\u0026gt;% unlist() ) }  Modeling with purrr pred_rf \u0026lt;- map2_df( .x = cross_val_tbl$splits, .y = cross_val_tbl$id, ~ rf_fun(split = .x, id = .y, try = 3, tree = 200) ) head(pred_rf) ## # A tibble: 6 x 3 ## id truth prediction ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Fold01 No No ## 2 Fold01 No No ## 3 Fold01 Yes No ## 4 Fold01 No No ## 5 Fold01 No No ## 6 Fold01 No No  Assess Performance pred_rf %\u0026gt;% conf_mat(truth, prediction) %\u0026gt;% summary() %\u0026gt;% select(-.estimator) %\u0026gt;% filter(.metric %in% c(\u0026quot;accuracy\u0026quot;, \u0026quot;precision\u0026quot;, \u0026quot;recall\u0026quot;, \u0026quot;f_meas\u0026quot;)) %\u0026gt;% kable()    .metric  .estimate      accuracy  0.7964806    precision  0.8320035    recall  0.9065443    f_meas  0.8676760        ","date":1587859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587886281,"objectID":"261a4b6a8174f95b7a39f9d4212425de","permalink":"/post/ml-workflow/","publishdate":"2020-04-26T00:00:00Z","relpermalink":"/post/ml-workflow/","section":"post","summary":"Setup  Load pkg library(tidymodels) # Loads parsnip, rsample, recipes, yardstick library(skimr) # Quickly get a sense of data library(knitr)   Load Data telco \u0026lt;- read_csv(\u0026quot;data/WA_Fn-UseC_-Telco-Customer-Churn.csv\u0026quot;) telco %\u0026gt;% head() %\u0026gt;% kable()    customerID  gender  SeniorCitizen  Partner  Dependents  tenure  PhoneService  MultipleLines  InternetService  OnlineSecurity  OnlineBackup  DeviceProtection  TechSupport  StreamingTV  StreamingMovies  Contract  PaperlessBilling  PaymentMethod  MonthlyCharges  TotalCharges  Churn      7590-VHVEG  Female  0  Yes  No  1  No  No phone service  DSL  No  Yes  No  No  No  No  Month-to-month  Yes  Electronic check  29.","tags":["Machine learning"],"title":"ML Workflow","type":"post"},{"authors":[],"categories":["Tools"],"content":" Setup  function x_to_na \u0026lt;- function (s, x = 0) { sapply(s, function(y) ifelse(y %in% x, NA, y)) }  What is tidymodels Much like the tidyverse consists of many core packages, such as ggplot2 and dplyr, tidymodels also consists of several core packages, including\n rsample: for sample splitting (e.g.¬†train/test or cross-validation)\n recipes: for pre-processing\n parsnip: for specifying the model\n yardstick: for evaluating the model\n  We will also be using the tune package (for parameter tuning procedure) and the workflows package (for putting everything together) that I had thought were a part of CRAN‚Äôs tidymodels package bundle, but apparently they aren‚Äôt. These will need to be loaded separately for now.\n Getting set up # load the relevant tidymodels libraries library(tidymodels) library(tidyverse) library(workflows) library(tune)  Load Data # load the Pima Indians dataset from the mlbench dataset library(mlbench) data(PimaIndiansDiabetes) # rename dataset to have shorter name because lazy df \u0026lt;- PimaIndiansDiabetes head(df) ## pregnant glucose pressure triceps insulin mass pedigree age diabetes ## 1 6 148 72 35 0 33.6 0.627 50 pos ## 2 1 85 66 29 0 26.6 0.351 31 neg ## 3 8 183 64 0 0 23.3 0.672 32 pos ## 4 1 89 66 23 94 28.1 0.167 21 neg ## 5 0 137 40 35 168 43.1 2.288 33 pos ## 6 5 116 74 0 0 25.6 0.201 30 neg  EDA skim skimr::skim(df)  Table 1: Data summary          Name  df    Number of rows  768    Number of columns  9    _______________________     Column type frequency:     factor  1    numeric  8    ________________________     Group variables  None     Variable type: factor\n   skim_variable  n_missing  complete_rate  ordered  n_unique  top_counts      diabetes  0  1  FALSE  2  neg: 500, pos: 268     Variable type: numeric\n   skim_variable  n_missing  complete_rate  mean  sd  p0  p25  p50  p75  p100  hist      pregnant  0  1  3.85  3.37  0.00  1.00  3.00  6.00  17.00  ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ    glucose  0  1  120.89  31.97  0.00  99.00  117.00  140.25  199.00  ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÇ    pressure  0  1  69.11  19.36  0.00  62.00  72.00  80.00  122.00  ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÅ    triceps  0  1  20.54  15.95  0.00  0.00  23.00  32.00  99.00  ‚ñá‚ñá‚ñÇ‚ñÅ‚ñÅ    insulin  0  1  79.80  115.24  0.00  0.00  30.50  127.25  846.00  ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ    mass  0  1  31.99  7.88  0.00  27.30  32.00  36.60  67.10  ‚ñÅ‚ñÉ‚ñá‚ñÇ‚ñÅ    pedigree  0  1  0.47  0.33  0.08  0.24  0.37  0.63  2.42  ‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ    age  0  1  33.24  11.76  21.00  24.00  29.00  41.00  81.00  ‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ      distribution df %\u0026gt;% janitor::clean_names() %\u0026gt;% set_names(colnames(.) %\u0026gt;% str_replace_all(., \u0026quot;_\u0026quot;, \u0026quot; \u0026quot;) %\u0026gt;% str_to_title()) %\u0026gt;% select_if(is.numeric) %\u0026gt;% gather() %\u0026gt;% ggplot(aes(x = value)) + facet_wrap(~ key, scales = \u0026quot;free\u0026quot;, ncol = 4) + geom_histogram() df %\u0026gt;% janitor::clean_names() %\u0026gt;% set_names(colnames(.) %\u0026gt;% str_replace_all(., \u0026quot;_\u0026quot;, \u0026quot; \u0026quot;) %\u0026gt;% str_to_title()) %\u0026gt;% select_if(is.factor) %\u0026gt;% gather() %\u0026gt;% ggplot(aes(x = value)) + geom_bar()  replace NA # df_clean \u0026lt;- df %\u0026gt;% # mutate_at(vars(triceps, glucose, pressure, insulin, mass), # function(.var) { # if_else(condition = (.var == 0), # if true (i.e. the entry is 0) # true = as.numeric(NA), # replace the value with NA # false = .var # otherwise leave it as it is # ) # }) df_clean \u0026lt;- df %\u0026gt;% mutate_at(vars(triceps, glucose, pressure, insulin, mass), ~ x_to_na(.x))   Split into train/test set.seed(234589) # split the data into trainng (75%) and testing (25%) set.seed(seed = 1972) train_test_split \u0026lt;- rsample::initial_split( data = df_clean, prop = 3/4 ) train_test_split ## \u0026lt;576/192/768\u0026gt; train_tbl \u0026lt;- train_test_split %\u0026gt;% training() test_tbl \u0026lt;- train_test_split %\u0026gt;% testing()  cv-fold At some point we‚Äôre going to want to do some parameter tuning, and to do that we‚Äôre going to want to use cross-validation. So we can create a cross-validated version of the training set in preparation for that moment using vfold_cv().\n# create CV object from training data train_cv \u0026lt;- vfold_cv(train_tbl)   Define a recipe # define the recipe function recipe_simple \u0026lt;- function(data) { # which consists of the formula (outcome ~ predictors) recipe(diabetes ~ pregnant + glucose + pressure + triceps + insulin + mass + pedigree + age, data = data) %\u0026gt;% # and some pre-processing steps step_normalize(all_numeric()) %\u0026gt;% step_knnimpute(all_predictors()) %\u0026gt;% prep(data = data) } recipe_prepped \u0026lt;- recipe_simple(data = df_clean) train_baked \u0026lt;- bake(recipe_prepped, new_data = train_tbl) test_baked \u0026lt;- bake(recipe_prepped, new_data = test_tbl)  Specify the model  Parsnip offers a unified interface for the massive variety of models that exist in R.\n The model type: what kind of model you want to fit, set using a different function depending on the model, such as rand_forest() for random forest, logistic_reg() for logistic regression, svm_poly() for a polynomial SVM model etc. The full list of models available via parsnip can be found here.\n The arguments: the model parameter values (now consistently named across different models), set using set_args().\n The engine: the underlying package the model should come from (e.g.¬†‚Äúranger‚Äù for the ranger implementation of Random Forest), set using set_engine().\n The mode: the type of prediction - since several packages can do both classification (binary/categorical prediction) and regression (continuous prediction), set using set_mode().\n  RF  üìå this code doesn‚Äôt actually fit the model. Like the recipe, it just outlines a description of the model.\n  setting a parameter to tune() means that it will be tuned later in the tune stage of the pipeline. You could also just specify a particular value of the parameter if you don‚Äôt want to tune it e.g.¬†using set_args(mtry = 4).\n rf_model \u0026lt;- # specify that the model is a random forest rand_forest() %\u0026gt;% # specify that the `mtry` parameter needs to be tuned set_args(mtry = tune()) %\u0026gt;% # select the engine/package that underlies the model set_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;% # choose either the continuous regression or binary classification mode set_mode(\u0026quot;classification\u0026quot;)  Put it all together in a workflow # set the workflow rf_workflow \u0026lt;- workflow() %\u0026gt;% # add the recipe add_recipe(recipe_prepped) %\u0026gt;% # add the model add_model(rf_model)  Tune the parameters  üìå You can tune multiple parameters at once by providing multiple parameters to the expand.grid() function, e.g.¬†expand.grid(mtry = c(3, 4, 5), trees = c(100, 500)).\n # specify which values eant to try rf_grid \u0026lt;- expand.grid(mtry = c(3, 4, 5)) # extract results rf_tune_results \u0026lt;- rf_workflow %\u0026gt;% tune_grid(resamples = train_cv, #CV object grid = rf_grid, # grid of values to try metrics = metric_set(accuracy, roc_auc) # metrics we care about ) # print results rf_tune_results %\u0026gt;% collect_metrics() ## # A tibble: 6 x 6 ## mtry .metric .estimator mean n std_err ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 3 accuracy binary 0.759 10 0.0216 ## 2 3 roc_auc binary 0.843 10 0.0160 ## 3 4 accuracy binary 0.759 10 0.0211 ## 4 4 roc_auc binary 0.841 10 0.0163 ## 5 5 accuracy binary 0.764 10 0.0200 ## 6 5 roc_auc binary 0.841 10 0.0155   Finalize the workflow param_final \u0026lt;- rf_tune_results %\u0026gt;% select_best(metric = \u0026quot;accuracy\u0026quot;, maximize = TRUE) param_final ## # A tibble: 1 x 1 ## mtry ## \u0026lt;dbl\u0026gt; ## 1 5 # add this parameter to the workflow using the finalize_workflow() function. rf_workflow \u0026lt;- rf_workflow %\u0026gt;% finalize_workflow(param_final)  Fit the final model rf_fit \u0026lt;- rf_workflow %\u0026gt;% # fit on entire training set and evaluate on test set last_fit(train_test_split) rf_fit ## # # Monte Carlo cross-validation (0.75/0.25) with 1 resamples ## # A tibble: 1 x 6 ## splits id .metrics .notes .predictions .workflow ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 \u0026lt;split [576/‚Ä¶ train/test ‚Ä¶ \u0026lt;tibble [2 √ó‚Ä¶ \u0026lt;tibble [0‚Ä¶ \u0026lt;tibble [192 √ó‚Ä¶ \u0026lt;workflo‚Ä¶  Evaluate the model on the test set performance test_performance \u0026lt;- rf_fit %\u0026gt;% collect_metrics() test_performance ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy binary 0.755 ## 2 roc_auc binary 0.829  predict # generate predictions from the test set test_predictions \u0026lt;- rf_fit %\u0026gt;% collect_predictions() test_predictions ## # A tibble: 192 x 6 ## id .pred_neg .pred_pos .row .pred_class diabetes ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 train/test split 0.249 0.751 1 pos pos ## 2 train/test split 0.292 0.708 3 pos pos ## 3 train/test split 0.402 0.598 5 pos pos ## 4 train/test split 0.587 0.413 10 neg pos ## 5 train/test split 0.428 0.572 14 pos pos ## 6 train/test split 0.785 0.215 19 neg neg ## 7 train/test split 0.417 0.583 25 pos pos ## 8 train/test split 0.203 0.797 32 pos pos ## 9 train/test split 0.973 0.0272 34 neg neg ## 10 train/test split 0.277 0.723 37 pos neg ## # ‚Ä¶ with 182 more rows # test_predictions \u0026lt;- rf_fit %\u0026gt;% pull(.predictions) # test_predictions  confusion matrix # generate a confusion matrix test_predictions %\u0026gt;% conf_mat(truth = diabetes, estimate = .pred_class) ## Truth ## Prediction neg pos ## neg 104 25 ## pos 22 41 test_predictions %\u0026gt;% ggplot() + geom_density(aes(x = .pred_pos, fill = diabetes), alpha = 0.5)   ","date":1587859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587888064,"objectID":"3f560cd1144170739cf3424a567557df","permalink":"/post/tidymodels-tidy-machine-learning-in-r/","publishdate":"2020-04-26T00:00:00Z","relpermalink":"/post/tidymodels-tidy-machine-learning-in-r/","section":"post","summary":"Setup  function x_to_na \u0026lt;- function (s, x = 0) { sapply(s, function(y) ifelse(y %in% x, NA, y)) }  What is tidymodels Much like the tidyverse consists of many core packages, such as ggplot2 and dplyr, tidymodels also consists of several core packages, including","tags":["Machine learning"],"title":"Tidymodels: tidy machine learning in R","type":"post"},{"authors":[],"categories":["Tools"],"content":" Load Libraries  Import Data Download theIBM Watson Telco Data Set here. Next, useread_csv()to import the data into a nice tidy data frame. We use theglimpse()function to quickly inspect the data. We have the target ‚ÄúChurn‚Äù and all other variables are potential predictors. The raw data set needs to be cleaned and preprocessed for ML.\nUse sapply to check the number if missing values in each columns\n üìå Preprocess Data  We‚Äôll go through a few steps to preprocess the data for ML. First, we ‚Äúprune‚Äù the data, which is nothing more than removing unnecessary columns and rows. Then we split into training and testing sets. After that we explore the training set to uncover transformations that will be needed for deep learning. We save the best for last. We end by preprocessing the data with the new recipes package.\n  prune: ÂéªÊéâ‰∏çË¶ÅÁöÑË°åÂíåÂàó split: ÂàáÂàÜÊï∞ÊçÆ EDA explore the data  Prune The Data The data has a few columns and rows we‚Äôd like to remove:\n The ‚ÄúcustomerID‚Äù column is a unique identifier for each observation that isn‚Äôt needed for modeling. We can de-select this column. The data has 11 NA values all in the ‚ÄúTotalCharges‚Äù column. Because it‚Äôs such a small percentage of the total population (99.8% complete cases), we can drop these observations with the drop_na() function from tidyr. Note that these may be customers that have not yet been charged, and therefore an alternative is to replace with zero or -99 to segregate this population from the rest. My preference is to have the target in the first column so we‚Äôll include a final select() ooperation to do so.  We‚Äôll perform the cleaning operation with one tidyverse pipe (%\u0026gt;%) chain.\nBar plots of categorical variables\n Split Into Train/Test Sets We have a new package, rsample, which is very useful for sampling methods. It has the initial_split() function for splitting data sets into training and testing sets. The return is a special rsplit object.\n Exploration: What Transformation Steps Are Needed For ML? This phase of the analysis is often called exploratory analysis, but basically we are trying to answer the question, ‚ÄúWhat steps are needed to prepare for ML?‚Äù The key concept is knowing what transformations are needed to run the algorithm most effectively. Artificial Neural Networks are best when the data is one-hot encoded, scaled and centered . In addition, other transformations may be beneficial as well to make relationships easier for the algorithm to identify. A full exploratory analysis is not practical in this article. With that said we‚Äôll cover a few tips on transformations that can help as they relate to this dataset. In the next section, we will implement the preprocessing techniques.\nÂõûÁ≠î‰∏Ä‰∏™ÈóÆÈ¢ò: ML ÈúÄË¶Å‰ªÄ‰πàÊ†∑ÁöÑÊï∞ÊçÆ? Êï∞ÊçÆÈúÄË¶ÅÂÅö‰ªÄ‰πàÊ†∑ÁöÑËΩ¨Êç¢.\n one-hot, scaled and centered ÊØîËæÉÈÄÇÂêàÁ•ûÁªèÁΩëÁªú\n Discretize The ‚Äútenure‚Äù Feature Numeric features like age, years worked, length of time in a position can generalize a group (or cohort ). We see this in marketing a lot (think ‚Äúmillennials‚Äù, which identifies a group born in a certain timeframe). The ‚Äútenure‚Äù feature falls into this category of numeric features that can be discretized into groups.\nËøûÁª≠ÂèòÈáèÁöÑÁ¶ªÊï£Âåñ\n Transform The ‚Äútotalcharges‚Äù Feature log transform\nWhat we don‚Äôt like to see is when a lot of observations are bunched within a small part of the range.\nWe can use a log transformation to even out the data into more of a normal distribution. It‚Äôs not perfect, but it‚Äôs quick and easy to get our data spread out a bit more.\n Êï∞ÊçÆËΩ¨Êç¢ÊîπÂèòÊï∞ÊçÆ‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄß Pro Tip: A quick test is to see if the log transformation increases the magnitude of the correlation between ‚ÄúTotalCharges‚Äù and ‚ÄúChurn‚Äù. We‚Äôll use a few dplyr operations along with the corrr package to perform a quick correlation.\n correlate(): Performs tidy correlations on numeric data focus(): Similar to select(). Takes columns and focuses on only the rows/columns of importance. fashion(): Makes the formatting aesthetically easier to read.  The correlation between ‚ÄúChurn‚Äù and ‚ÄúLogTotalCharges‚Äù is greatest in magnitude indicating the log transformation should improve the accuracy of the ANN model we build. Therefore, we should perform the log transformation.\nx Âíå y ‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄß, ÁªèËøáËΩ¨Êç¢ÂêéÂ¢ûÂº∫‰∫Ü, ËøôÊ†∑ÊúâÂà©‰∫éÊ®°ÂûãÁöÑËÆ≠ÁªÉ\n One-Hot Encoding One-hot encoding is the process of converting categorical data to sparse data, which has columns of only zeros and ones (this is also called creating ‚Äúdummy variables‚Äù or a ‚Äúdesign matrix‚Äù). All non-numeric data will need to be converted to dummy variables. This is simple for binary Yes/No data because we can simply convert to 1‚Äôs and 0‚Äôs. It becomes slightly more complicated with multiple categories, which requires creating new columns of 1‚Äôs and 0`s for each category (actually one less). We have four features that are multi-category: Contract, Internet Service, Multiple Lines, and Payment Method.\n Feature Scaling ANN‚Äôs typically perform faster and often times with higher accuracy when the features are scaled and/or normalized (aka centered and scaled, also known as standardizing). Because ANNs use gradient descent, weights tend to update faster. According to Sebastian Raschka, an expert in the field of Deep Learning, several examples when feature scaling is important are:\n  k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally k-means (see k-nearest neighbors) logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others linear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you‚Äôd emphasize variables on ‚Äúlarger measurement scales‚Äù more. There are many more cases than I can possibly list here ‚Ä¶ I always recommend you to think about the algorithm and what it‚Äôs doing, and then it typically becomes obvious whether we want to scale your features or not.   The interested reader can read Sebastian Raschka‚Äôs article for a full discussion on the scaling/normalization topic. Pro Tip: When in doubt, standardize the data.\n  Preprocessing With Recipes Let‚Äôs implement the preprocessing steps/transformations uncovered during our exploration. Max Kuhn (creator of caret) has been putting some work into Rlang ML tools lately, and the payoff is beginning to take shape. A new package, recipes, makes creating ML data preprocessing workflows a breeze! It takes a little getting used to, but I‚Äôve found that it really helps manage the preprocessing steps. We‚Äôll go over the nitty gritty as it applies to this problem.\nStep 1: Create A Recipe A ‚Äúrecipe‚Äù is nothing more than a series of steps you would like to perform on the training, testing and/or validation sets. Think of preprocessing data like baking a cake (I‚Äôm not a baker but stay with me). The recipe is our steps to make the cake. It doesn‚Äôt do anything other than create the playbook for baking.\nWe use the recipe() function to implement our preprocessing steps. The function takes a familiar object argument, which is a modeling function such as object = Churn ~ . meaning ‚ÄúChurn‚Äù is the outcome (aka response, predictor, target) and all other features are predictors. The function also takes the data argument, which gives the ‚Äúrecipe steps‚Äù perspective on how to apply during baking (next).\nA recipe is not very useful until we add ‚Äústeps‚Äù, which are used to transform the data during baking. The package contains a number of useful ‚Äústep functions‚Äù that can be applied. The entire list of Step Functions can be viewed here. For our model, we use:\nstep_discretize() with the option = list(cuts = 6) to cut the continuous variable for ‚Äútenure‚Äù (number of years as a customer) to group customers into cohorts. Êï∞ÊçÆÁ¶ªÊï£Âåñ step_log() to log transform ‚ÄúTotalCharges‚Äù. step_dummy() to one-hot encode the categorical data. Note that this adds columns of one/zero for categorical data with three or more categories. step_center() to mean-center the data. ÊâÄÊúâÊï∞ÊçÆÂáèÂéªÂπ≥ÂùáÂÄº step_scale() to scale the data.  The last step is to prepare the recipe with the prep() function. This step is used to ‚Äúestimate the required parameters from a training set that can later be applied to other data sets‚Äù. This is important for centering and scaling and other functions that use parameters defined from the training set.\nprep(), Ëøô‰∏ÄÊ≠•ÈùûÂ∏∏ÈáçË¶Å, ÂèØ‰ª•ËØÑ‰º∞ÈúÄË¶ÅÁöÑÂèÇÊï∞‰øùÂ≠ò‰∏ãÊù•Áî®‰∫é‰πãÂêéÁöÑ future raw data, ËøôÂØπ‰∫éÊï∞ÊçÆÊ†áÂáÜÂåñËøô‰∏ÄÊ≠•ÊòØÈùûÂ∏∏ÈáçË¶ÅÁöÑ.\nHere‚Äôs how simple it is to implement the preprocessing steps that we went over!\nWe can print the recipe object if we ever forget what steps were used to prepare the data. Pro Tip: We can save the recipe object as an RDS file using saveRDS(), and then use it to bake() (discussed next) future raw data into ML-ready data in production!\nÊàë‰ª¨ÂèØ‰ª•‰øùÂ≠ò‰∏ãÊù•Ëøô‰∏™ recipes Áî®‰∫é‰πãÂêéÁöÑÊï∞ÊçÆÂáÜÂ§á, ËøôÂØπ‰∫é‰ΩøÁî®Ê®°ÂûãÈ¢ÑÊµãÊï∞ÊçÆÊòØÈùûÂ∏∏ÈáçË¶ÅÁöÑ.\n Step 2: Baking With Your Recipe Now for the fun part! We can apply the ‚Äúrecipe‚Äù to any data set with the bake() function, and it processes the data following our recipe steps. We‚Äôll apply to our training and testing data to convert from raw data to a machine learning dataset. Check our training set out with glimpse(). Now that‚Äôs an ML-ready dataset prepared for ANN modeling!!\nraw dataset to machine learning dataset\n Step 3: Don‚Äôt Forget The Target One last step, we need to store the actual values (truth) as y_train_vec and y_test_vec, which are needed for modeling our ANN. We convert to a series of numeric ones and zeros which can be accepted by the Keras ANN modeling functions. We add ‚Äúvec‚Äù to the name so we can easily remember the class of the object (it‚Äôs easy to get confused when working with tibbles, vectors, and matrix data types).\n   üìå Keras: Model Customer Churn With Keras (Deep Learning) This is super exciting!! Finally, Deep Learning with Keras in R! The team at RStudio has done fantastic work recently to create the keras package, which implements Keras in R. Very cool!\nBackground On Artifical Neural Networks For those unfamiliar with Neural Networks (and those that need a refresher), read this article. It‚Äôs very comprehensive, and you‚Äôll leave with a general understanding of the types of deep learning and how they work.\nSource: Xenon Stack\nDeep Learning has been available in R for some time, but the primary packages used in the wild have not (this includes Keras, Tensor Flow, Theano, etc, which are all Python libraries). It‚Äôs worth mentioning that a number of other Deep Learning packages exist in R including h2o, mxnet, and others. The interested reader can check out this blog post for a comparison of deep learning packages in R.\n Building A Deep Learning Model We‚Äôre going to build a special class of ANN called a Multi-Layer Perceptron (MLP). MLPs are one of the simplest forms of deep learning, but they are both highly accurate and serve as a jumping-off point for more complex algorithms. MLPs are quite versatile as they can be used for regression, binary and multi classification (and are typically quite good at classification problems).\nWe‚Äôll build a three layer MLP with Keras. Let‚Äôs walk-through the steps before we implement in R.\nInitialize a sequential model : The first step is to initialize a sequential model with keras_model_sequential(), which is the beginning of our Keras model. The sequential model is composed of a linear stack of layers.\n Apply layers to the sequential model: Layers consist of the input layer, hidden layers and an output layer. The input layer is the data and provided it‚Äôs formatted correctly there‚Äôs nothing more to discuss. The hidden layers and output layers are what controls the ANN inner workings.\n   Hidden Layers: Hidden layers form the neural network nodes that enable non-linear activation using weights. The hidden layers are created using layer_dense(). We‚Äôll add two hidden layers. We‚Äôll apply units = 16, which is the number of nodes. We‚Äôll select kernel_initializer = \"uniform\" and activation = \"relu\" for both layers. The first layer needs to have the input_shape = 35, which is the number of columns in the training set (the number of feature) . Key Point: While we are arbitrarily selecting the number of hidden layers, units, kernel initializers and activation functions, these parameters can be optimized through a process called hyperparameter tuning that is discussed in Next Steps.   ÂÖ≥ÈîÆÁÇπÔºöÂ∞ΩÁÆ°Êàë‰ª¨ÂèØ‰ª•‰ªªÊÑèÈÄâÊã©ÈöêËóèÂ±ÇÔºåÂçïÂÖÉÔºåÂÜÖÊ†∏ÂàùÂßãÂåñÁ®ãÂ∫èÂíåÊøÄÊ¥ªÂáΩÊï∞ÁöÑÊï∞ÈáèÔºå‰ΩÜÊòØÂèØ‰ª•ÈÄöËøáÁß∞‰∏∫‚ÄúË∂ÖÂèÇÊï∞Ë∞ÉÊï¥‚ÄùÁöÑËøáÁ®ãÊù•‰ºòÂåñËøô‰∫õÂèÇÊï∞ÔºåËØ•ËøáÁ®ãÂ∞ÜÂú®[ÂêéÁª≠Ê≠•È™§]‰∏≠ËøõË°åËÆ®ËÆ∫„ÄÇ\n  Dropout Layers: Dropout layers are used to control overfitting. This eliminates weights below a cutoff threshold to prevent low weights from overfitting the layers. We use the layer_dropout() function add two drop out layers with rate = 0.10 to remove weights below 10%.\n Output Layer: The output layer specifies the shape of the output and the method of assimilating the learned information. The output layer is applied using the layer_dense(). For binary values, the shape should be units = 1. For multi-classification, the units should correspond to the number of classes. We set the kernel_initializer = \"uniform\" and the activation = \"sigmoid\" (common for binary classification).\n  Compile the model: The last step is to compile the model with compile(). We‚Äôll use optimizer = \"adam\", which is one of the most popular optimization algorithms. We select loss = \"binary_crossentropy\" since this is a binary classification problem. We‚Äôll select metrics = c(\"accuracy\") to be evaluated during training and testing. Key Point: The optimizer is often included in the tuning process.  Build model object Let‚Äôs codify the discussion above to build our Keras MLP-flavored ANN model.\n Fit Model We use the fit() function to run the ANN on our training data. The object is our model, and x and y are our training data in matrix and numeric vector forms, respectively. The batch_size = 50 sets the number samples per gradient update within each epoch. We set epochs = 35 to control the number training cycles. Typically we want to keep the batch size high since this decreases the error within each training cycle (epoch). We also want epochs to be large, which is important in visualizing the training history (discussed below). We set validation_split = 0.30 to include 30% of the data for model validation, which prevents overfitting. The training process should complete in 15 seconds or so.\nÊàë‰ª¨Â∞Ü\" validation_split = 0.30\"ËÆæÁΩÆ‰∏∫ÂåÖÊã¨30ÔºÖÁöÑÊï∞ÊçÆÁî®‰∫éÊ®°ÂûãÈ™åËØÅÔºåËøôÂèØ‰ª•Èò≤Ê≠¢ËøáÊãüÂêà„ÄÇ È™åËØÅÈõÜÂºÑÂ§ß‰∏ÄÁÇπÂèØ‰ª•Èò≤Ê≠¢Ê®°ÂûãËøáÊãüÂêà??\n vis history We can inspect the training history. We want to make sure there is minimal difference between the validation accuracy and the training accuracy.\nWe can visualize the Keras training history using the plot() function. What we want to see is the validation accuracy and loss leveling off, which means the model has completed training. We see that there is some divergence between training loss/accuracy and validation loss/accuracy. This model indicates we can possibly stop training at an earlier epoch. Pro Tip: Only use enough epochs to get a high validation accuracy. Once validation accuracy curve begins to flatten or decrease, it‚Äôs time to stop training.\n ‰∏ìÂÆ∂ÊèêÁ§∫ÔºöËØ∑‰ªÖ‰ΩøÁî®Ë∂≥Â§üÁöÑ epochs ‰ª•Ëé∑ÂæóËæÉÈ´òÁöÑÈ™åËØÅÂáÜÁ°ÆÊÄß„ÄÇÈ™åËØÅÂáÜÁ°ÆÊÄßÊõ≤Á∫øÂºÄÂßãÂèòÂπ≥ÊàñÂáèÂ∞èÂêéÔºåÂ∞±ËØ•ÂÅúÊ≠¢ËÆ≠ÁªÉ‰∫Ü„ÄÇ\n   Making Predictions We‚Äôve got a good model based on the validation accuracy. Now let‚Äôs make some predictions from our keras model on the test data set, which was unseen during modeling (we use this for the true performance assessment). We have two functions to generate predictions:\n predict_classes(): Generates class values as a matrix of ones and zeros. Since we are dealing with binary classification, we‚Äôll convert the output to a vector. predict_proba(): Generates the class probabilities as a numeric matrix indicating the probability of being a class. Again, we convert to a numeric vector because there is only one column output.    Inspect Performance With Yardstick The yardstick package has a collection of handy functions for measuring performance of machine learning models. We‚Äôll overview some metrics we can use to understand the performance of our model.\nFirst, let‚Äôs get the data formatted for yardstick. We create a data frame with the truth (actual values as factors), estimate (predicted values as factors), and the class probability (probability of yes as numeric). We use the fct_recode() function from the forcats package to assist with recoding as Yes/No values.\nNow that we have the data formatted, we can take advantage of the yardstick package. The only other thing we need to do is to set options(yardstick.event_first = FALSE). As pointed out by ad1729 in GitHub Issue 13, the default is to classify 0 as the positive class instead of 1.\nËøô‰∏™ÈóÆÈ¢òÂú®ÊúÄÊñ∞ÁâàÁöÑ yardstick Â∑≤ÁªèÂæóÂà∞Ëß£ÂÜ≥‰∫Ü\nConfusion Table We can use the conf_mat() function to get the confusion table. We see that the model was by no means perfect, but it did a decent job of identifying customers likely to churn.\n Accuracy We can use the metrics() function to get an accuracy measurement from the test set. We are getting roughly 82% accuracy.\n AUC We can also get the ROC Area Under the Curve (AUC) measurement. AUC is often a good metric used to compare different classifiers and to compare to randomly guessing (AUC_random = 0.50). Our model has AUC = 0.85, which is much better than randomly guessing. Tuning and testing different classification algorithms may yield even better results.\n Precision And Recall Precision is when the model predicts ‚Äúyes‚Äù, how often is it actually ‚Äúyes‚Äù. Recall (also true positive rate or specificity) is when the actual value is ‚Äúyes‚Äù how often is the model correct. We can get precision() and recall() measurements using yardstick.\nPrecision and recall are very important to the business case: The organization is concerned with balancing the cost of targeting and retaining customers at risk of leaving with the cost of inadvertently targeting customers that are not planning to leave (and potentially decreasing revenue from this group). The threshold above which to predict Churn = ‚ÄúYes‚Äù can be adjusted to optimize for the business problem. This becomes an Customer Lifetime Value optimization problem that is discussed further in Next Steps.\n F1 Score We can also get the F1-score, which is a weighted average between the precision and recall. Machine learning classifier thresholds are often adjusted to maximize the F1-score. However, this is often not the optimal solution to the business problem.\n  üìå Explain The Model With LIME LIME stands for Local Interpretable Model-agnostic Explanations, and is a method for explaining black-box machine learning model classifiers. For those new to LIME, this YouTube video does a really nice job explaining how LIME helps to identify feature importance with black box machine learning models (e.g.¬†deep learning, stacked ensembles, random forest).\nwhy should I trust you\n (28) KDD2016 paper 573 - YouTube\nSetup The lime package implements LIME in R. One thing to note is that it‚Äôs not setup out-of-the-box to work with keras. The good news is with a few functions we can get everything working properly. We‚Äôll need to make two custom functions:\n model_type: Used to tell lime what type of model we are dealing with. It could be classification, regression, survival, etc.\n predict_model: Used to allow lime to perform predictions that its algorithm can interpret.\n  model_type fucntion define The first thing we need to do is identify the class of our model object. We do this with the class() function.\nNext we create our model_type() function. It‚Äôs only input is x the keras model. The function simply returns ‚Äúclassification‚Äù, which tells LIME we are classifying.\n predict_model function define Now we can create our predict_model() function, which wraps keras::predict_proba(). The trick here is to realize that it‚Äôs inputs must be x a model, newdata a dataframe object (this is important), and type which is not used but can be use to switch the output type. The output is also a little tricky because it must be in the format of probabilities by classification (this is important; shown next).\nRun this next script to show you what the output looks like and to test our predict_model() function. See how it‚Äôs the probabilities by classification. It must be in this form for model_type = \"classification\".\n predict_model() function   Enjoy LIME Now the fun part, we create an explainer using the lime() function. Just pass the training data set without the ‚ÄúAttribution column‚Äù. The form must be a data frame, which is OK since our predict_model function will switch it to an keras object. Set model = automl_leader our leader model, and bin_continuous = FALSE. We could tell the algorithm to bin continuous variables, but this may not make sense for categorical numeric data that we didn‚Äôt change to factors.\nNow we run the explain() function, which returns our explanation. This can take a minute to run so we limit it to just the first ten rows of the test data set.\n We set n_labels = 1 because we care about explaining a single class. Setting n_features = 4 returns the top four features that are critical to each case. Finally, setting kernel_width = 0.5 allows us to increase the ‚Äúmodel_r2‚Äù value by shrinking the localized evaluation.   Feature Importance Visualization The payoff for the work we put in using LIME is this feature importance plot. This allows us to visualize each of the first ten cases (observations) from the test data. The top four features for each case are shown. Note that they are not the same for each case.\nThe green bars mean that the feature supports the model conclusion, and the red bars contradict.\nA few important features based on frequency in first ten cases:\n Tenure (7 cases) Senior Citizen (5 cases) Online Security (4 cases)  Another excellent visualization can be performed using plot_explanations(), which produces a facetted heatmap of all case/label/feature combinations. It‚Äôs a more condensed version of plot_features(), but we need to be careful because it does not provide exact statistics and it makes it less easy to investigate binned features (Notice that ‚Äútenure‚Äù would not be identified as a contributor even though it shows up as a top feature in 7 of 10 cases).\n  üìå Check Explanations With Correlation Analysis One thing we need to be careful with the LIME visualization is that we are only doing a sample of the data, in our case the first 10 test observations. Therefore, we are gaining a very localized understanding of how the ANN works. However, we also want to know on from a global perspective what drives feature importance.\nWe can perform a correlation analysis on the training set as well to help glean what features correlate globally to ‚ÄúChurn‚Äù. We‚Äôll use the corrr package, which performs tidy correlations with the function correlate(). We can get the correlations as follows.\nThe correlation visualization helps in distinguishing which features are relavant to Churn.\nThe correlation analysis helps us quickly disseminate which features that the LIME analysis may be excluding. We can see that the following features are highly correlated (magnitude \u0026gt; 0.25):\n Increases Likelihood of Churn (Red): - Tenure = Bin 1 (\u0026lt;12 Months) - Internet Service = ‚ÄúFiber Optic‚Äù - Payment Method = ‚ÄúElectronic Check‚Äù\n Decreases Likelihood of Churn (Blue): - Contract = ‚ÄúTwo Year‚Äù - Total Charges (Note that this may be a biproduct of additional services such as Online Security)\n   üìå Feature Investigation We can investigate features that are most frequent in the LIME feature importance visualization along with those that the correlation analysis shows an above normal magnitude. We‚Äôll investigate:\n Tenure (7/10 LIME Cases, Highly Correlated) Contract (Highly Correlated) Internet Service (Highly Correlated) Payment Method (Highly Correlated) Senior Citizen (5/10 LIME Cases) Online Security (4/10 LIME Cases)  Tenure (7/10 LIME Cases, Highly Correlated) LIME cases indicate that the ANN model is using this feature frequently and high correlation agrees that this is important. Investigating the feature distribution, it appears that customers with lower tenure (bin 1) are more likely to leave. Opportunity: Target customers with less than 12 month tenure.\n Contract (Highly Correlated) While LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with one and two year contracts are much less likely to churn. Opportunity: Offer promotion to switch to long term contracts.\n Internet Service (Highly Correlated) While LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with fiber optic service are more likely to churn while those with no internet service are less likely to churn. Improvement Area: Customers may be dissatisfied with fiber optic service.\n Payment Method (Highly Correlated) While LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with electronic check are more likely to leave. Opportunity: Offer customers a promotion to switch to automatic payments.\n Senior Citizen (5/10 LIME Cases) Senior citizen appeared in several of the LIME cases indicating it was important to the ANN for the 10 samples. However, it was not highly correlated to Churn, which may indicate that the ANN is using in an more sophisticated manner (e.g.¬†as an interaction). It‚Äôs difficult to say that senior citizens are more likely to leave, but non-senior citizens appear less at risk of churning. Opportunity: Target users in the lower age demographic.\n Online Security (4/10 LIME Cases) Customers that did not sign up for online security were more likely to leave while customers with no internet service or online security were less likely to leave. Opportunity: Promote online security and other packages that increase retention rates.\n  üìå Next Steps: Business Science University(Domain knowledge) We‚Äôve just scratched the surface with the solution to this problem, but unfortunately there‚Äôs only so much ground we can cover in an article. Here are a few next steps that I‚Äôm pleased to announce will be covered in a Business Science University course coming in 2018!\nCustomer Lifetime Value Your organization needs to see the financial benefit so always tie your analysis to sales, profitability or ROI. Customer Lifetime Value (CLV) is a methodology that ties the business profitability to the retention rate. While we did not implement the CLV methodology herein, a full customer churn analysis would tie the churn to an classification cutoff (threshold) optimization to maximize the CLV with the predictive ANN model.\nThe simplified CLV model is:\nCLV=GC‚àó11+d‚àírCLV=GC‚àó11+d‚àír\nWhere,\n GC is the gross contribution per customer d is the annual discount rate r is the retention rate   ANN Performance Evaluation and Improvement The ANN model we built is good, but it could be better. How we understand our model accuracy and improve on it is through the combination of two techniques:\n K-Fold Cross-Fold Validation: Used to obtain bounds for accuracy estimates. Hyper Parameter Tuning: Used to improve model performance by searching for the best parameters possible.  We need to implement K-Fold Cross Validation and Hyper Parameter Tuning if we want a best-in-class model.\n Distributing Analytics It‚Äôs critical to communicate data science insights to decision makers in the organization. Most decision makers in organizations are not data scientists, but these individuals make important decisions on a day-to-day basis. The Shiny application below includes a Customer Scorecard to monitor customer health (risk of churn).\n\n Business Science University You‚Äôre probably wondering why we are going into so much detail on next steps. We are happy to announce a new project for 2018: Business Science University, an online school dedicated to helping data science learners.\nBenefits to learners:\n Build your own online GitHub portfolio of data science projects to market your skills to future employers! Learn real-world applications in People Analytics (HR), Customer Analytics, Marketing Analytics, Social Media Analytics, Text Mining and Natural Language Processing (NLP), Financial and Time Series Analytics, and more! Use advanced machine learning techniques for both high accuracy modeling and explaining features that have an effect on the outcome! Create ML-powered web-applications that can be distributed throughout an organization, enabling non-data scientists to benefit from algorithms in a user-friendly way!  Enrollment is open so please signup for special perks. Just go to Business Science University and select enroll.\n  Conclusions Customer churn is a costly problem. The good news is that machine learning can solve churn problems, making the organization more profitable in the process. In this article, we saw how Deep Learning can be used to predict customer churn. We built an ANN model using the new keras package that achieved 82% predictive accuracy (without tuning)! We used three new machine learning packages to help with preprocessing and measuring performance: recipes, rsample and yardstick. Finally we used lime to explain the Deep Learning model, which traditionally was impossible! We checked the LIME results with a Correlation Analysis, which brought to light other features to investigate. For the IBM Telco dataset, tenure, contract type, internet service type, payment menthod, senior citizen status, and online security status were useful in diagnosing customer churn. We hope you enjoyed this article!\n Links  RStudio AI Blog: Deep Learning With Keras To Predict Customer Churn\n Machine_Learning_projects/churn_analysis.Rmd at 6d3814a596bc2f9dee09fffbf45a8e052dcefc09 ¬∑ bennwei/Machine_Learning_projects\n   Error ‚ÄúError in dimnames\u0026lt;-.data.frame(*tmp*, value = list(n)) : invalid ‚Äòdimnames‚Äô given for data frame‚Äù\nkeras explanations with dataframe data ¬∑ Issue #139 ¬∑ thomasp85/lime\nI had the same error, and I managed to solve it by overwriting the correct output of class(model). It seems the newer version uses different name. So, it is fixed for by:\nclass(model_keras) # [1] \u0026quot;keras.engine.sequential.Sequential\u0026quot; # [2] \u0026quot;keras.engine.training.Model\u0026quot; # [3] \u0026quot;keras.engine.network.Network\u0026quot; # [4] \u0026quot;keras.engine.base_layer.Layer\u0026quot; # [5] \u0026quot;python.builtin.object\u0026quot;  Then use keras.engine.sequential.Sequential instead of keras.models.Sequential, i.e.:\n# gives error model_type.keras.models.Sequential \u0026lt;- function(x, ...) { \u0026quot;classification\u0026quot; } # works! model_type.keras.engine.sequential.Sequential \u0026lt;- function(x, ...) { \u0026quot;classification\u0026quot; } The same for predict_model(), use keras.engine.sequential.Sequential, or what is the output of class(model)\n ","date":1587168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587174833,"objectID":"8c2eab0cf1660c81124fa3e605dbce65","permalink":"/post/deep-learning-with-keras-in-r/","publishdate":"2020-04-18T00:00:00Z","relpermalink":"/post/deep-learning-with-keras-in-r/","section":"post","summary":"Load Libraries  Import Data Download theIBM Watson Telco Data Set here. Next, useread_csv()to import the data into a nice tidy data frame. We use theglimpse()function to quickly inspect the data.","tags":["Deep Learning"],"title":"Deep Learning With Keras in R","type":"post"},{"authors":[],"categories":["Tools"],"content":" host your html file You would need to host your HTML on a publicly accessible website somewhere, then use a webpage part object on your dashboard to embed that page.\nSkip that though, generate a clock to embed here -¬†Free Clocks for Your Website\n use flexdashboard flexdashboard Examples\n links Embed HTML Local in Tableau ( Import JS to Tabl‚Ä¶ |Tableau Community Forums\n(28) Integrating Tableau with R for Descriptive, Inferential and Predictive analytics - YouTube\n ","date":1587168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587204824,"objectID":"e2c096a548b552f92e37b3e76d74f8f9","permalink":"/post/integrating-tableau-with-r-for-descriptive-inferential-and-predictive-analytics/","publishdate":"2020-04-18T00:00:00Z","relpermalink":"/post/integrating-tableau-with-r-for-descriptive-inferential-and-predictive-analytics/","section":"post","summary":"host your html file You would need to host your HTML on a publicly accessible website somewhere, then use a webpage part object on your dashboard to embed that page.","tags":["tableau","shiny"],"title":"Integrating Tableau with R for Descriptive, Inferential and Predictive analytics","type":"post"},{"authors":[],"categories":["reading"],"content":" R square Âíå Q square ÂÖ∂ÂÆû‰ªñ‰ª¨ÁöÑËÆ°ÁÆóÊñπÂºèÊòØ‰∏ÄÊ®°‰∏ÄÊ†∑ÁöÑ, Âè™ÊòØ‰∏Ä‰∏™ÊòØÂú®ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏äÁÆóÁöÑÁªìÊûú, ‰∏Ä‰∏™ÊòØÂú®ÊµãËØïÈõÜ‰∏äÁöÑËÆ°ÁÆóÁªìÊûú.\nR2 Âíå Q2 Áõ∏Â∑ÆÂ§™Â§öÁöÑËØùÂàôËØ¥ÊòéÂõûÂΩíÁöÑÊ®°ÂûãÊúâËøáÊãüÂêà\n\\[ \\begin{array}{l}R^{2}=1-R S S / T S S \\\\ R S S=\\sum(y-\\hat{\\mathbf{y}})^{2} \\\\ T S S=\\sum(y-\\overline{\\mathbf{y}})^{2}\\end{array} \\]\n Q2 is the R2 when the PLS built on a training set is applied to a test set. So a good value for Q2 is a value that is close to the R2. That means that your PLS model works independently of the specific data that was used to train the PLS model. Adding more variables always makes R2 go up, but might not make Q2 go up.\n  As I have seen that the ‚ÄúDifference between R2 and Q2 should not be more than 0.3‚Äù [International Journal of Drug Design and Discovery, 2011, 2 (3) 511-519].\n  ","date":1587168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587174059,"objectID":"ff6d062bdd1acf38cc1a77c55fd4f546","permalink":"/post/r-square-and-q-square-what-the-difference/","publishdate":"2020-04-18T00:00:00Z","relpermalink":"/post/r-square-and-q-square-what-the-difference/","section":"post","summary":"R square Âíå Q square ÂÖ∂ÂÆû‰ªñ‰ª¨ÁöÑËÆ°ÁÆóÊñπÂºèÊòØ‰∏ÄÊ®°‰∏ÄÊ†∑ÁöÑ, Âè™ÊòØ‰∏Ä‰∏™ÊòØÂú®ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏äÁÆóÁöÑÁªìÊûú, ‰∏Ä‰∏™ÊòØÂú®ÊµãËØïÈõÜ‰∏äÁöÑËÆ°ÁÆóÁªìÊûú.\nR2 Âíå Q2 Áõ∏Â∑ÆÂ§™Â§öÁöÑËØùÂàôËØ¥ÊòéÂõûÂΩíÁöÑÊ®°ÂûãÊúâËøáÊãüÂêà\n\\[ \\begin{array}{l}R^{2}=1-R S S / T S S \\\\ R S S=\\sum(y-\\hat{\\mathbf{y}})^{2} \\\\ T S S=\\sum(y-\\overline{\\mathbf{y}})^{2}\\end{array} \\]","tags":["Machine learning"],"title":"R square and Q square , what the difference","type":"post"},{"authors":[],"categories":["Tools"],"content":" select variable When recipe steps are used, there are different approaches that can be used to select which variables or features should be used.\nThe three main characteristics of variables that can be queried:\n the name of the variable: the syntax as the same as dplyr the data type (e.g.¬†numeric or nominal): nominal is character or factor the role that was declared by the recipe: can define by user self add_role   basic recipes ordering of steps While your project‚Äôs needs may vary, here is a suggested order of potential steps that should work for most problems:\nImpute Individual transformations for skewness and other issues Discretize (if needed and if you have no other choice) Create dummy variables Create interactions Normalization steps (center, scale, range, etc) Multivariate transformation (e.g.¬†PCA, spatial sign, etc)   step_* function custom define  You can define yourself step_function Be clear about the specific definition of each step_function   links tidymodels/recipes: A preprocessing engine to generate design matrices\n ","date":1587168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587172118,"objectID":"902fb8e8ba43c37426d671c50758d390","permalink":"/post/recipes/","publishdate":"2020-04-18T00:00:00Z","relpermalink":"/post/recipes/","section":"post","summary":"select variable When recipe steps are used, there are different approaches that can be used to select which variables or features should be used.\nThe three main characteristics of variables that can be queried:","tags":["üì¶"],"title":"Recipes for machine learning","type":"post"},{"authors":[],"categories":["reading"],"content":"  Âú®Áº∫‰πèÂ∫¶ÈáèÊàñÂèäÊó∂ÂèçÈ¶àÁöÑÁéØÂ¢É‰∏ã, Â§ßÂ§öÊï∞‰∫∫ÈÉΩ‰ºöÈÄâÊã©ÊúÄÁÆÄÂçïÁöÑË°å‰∏∫Âíå‰∫ãÊÉÖÊù•ÂÅö„ÄÇ\n ‰ªÄ‰πàÊòØÊúÄÂ∞èÈòªÂäõÂéüÂàô  ÊúÄÂ∞èÈòªÂäõÂéüÂàô: ÊòØÊåáÂú®Â∑•‰ΩúÁéØÂ¢É‰∏ã, Ëã•ÂêÑÁßçË°å‰∏∫ÂØπ‰∫éÂ∫ïÁ∫øÁöÑÂΩ±ÂìçÊ≤°ÊúâÂæóÂà∞ÊòéÁ°ÆÁöÑÂèçÈ¶àÊÑèËßÅ , ÈÇ£Êàë‰ª¨ÂàôÂÄæÂêë‰∫éÈááÁî®ÂΩì‰∏ãÊúÄÁÆÄÂçïÊòìË°åÁöÑË°å‰∏∫„ÄÇ\n  ‰øùÊåÅËÅîÁªìÁä∂ÊÄÅÁúüÁöÑÊúâÂä©‰∫éÂ∑•‰Ωú? Á≠îÊ°àÊòØÂê¶ÂÆöÁöÑ.\n‰∏∫‰∫ÜÈ™åËØÅËøô‰∏™ÊÉ≥Ê≥ï, Ê≥¢Â£´È°øÂí®ËØ¢ÈõÜÂõ¢ÈááÁî®‰∫Ü‰∏Ä‰∫õÊûÅÁ´ØÊâãÊÆµ, ‰ªñËø´‰ΩøÂõ¢Èòü‰∏≠ÊØè‰∏ÄÂêçÊàêÂëòÂú®Â∑•‰ΩúÊó•‰∏≠ÈÄâÊã©‰∏ÄÂ§©‰∏ç‰∏äÁè≠, ‰πüÂ∞±ÊòØËØ¥ÂíåÂÖ¨Âè∏ÂÜÖÂ§ñÁöÑ‰ªª‰Ωï‰∫∫ÈÉΩÊñ≠ÂºÄËÅîÁ≥ª.\nÊúÄÂàùËøô‰∏™Âõ¢ÈòüÊúâ‰∫õÊäµÂà∂ËøôÈ°πÂÆûÈ™å, ÊçÆ‰ªñÂõûÂøÜ, ÂÖ∂‰∏≠‰∏Ä‰∫õÂ∞ùËØïË¥üË¥£ÁöÑÂêà‰ºô‰∫∫‰∏ÄÁõ¥ÂØπÊàë‰ª¨ÁöÑÂü∫Êú¨ËßÇÁÇπÂæàÊîØÊåÅ, ‰ΩÜÊòØËøôÊó∂‰πüÁ™ÅÁÑ∂Êúâ‰∫õÁ¥ßÂº†, ÂÆ≥ÊÄïÂëäËØâÂÆ¢Êà∑, ‰ªñÂõ¢Èòü‰∏≠ÁöÑÊØè‰∏Ä‰∏™ÊàêÂëòÊØèÂë®ÈÉΩÊúâ‰∏ÄÂ§©‰∏çÂ∑•‰Ωú, ÈÇ£‰∫õÂí®ËØ¢Âëò‰πüÂêåÊ†∑ÂæàÁ¥ßÂº†ÊãÖÂøß‰ªñ‰ª¨ËøôÊ†∑‰ºöÁΩÆ‰∫ã‰∏ö‰∫éÂç±Êú∫, ‰ΩÜÊòØËøô‰∏™Âõ¢ÈòüÂπ∂Ê≤°ÊúâÂ§±ÂéªÂÆ¢Êà∑, Âõ¢ÈòüÊàêÂëò‰πüÊ≤°Êúâ‰∏¢ÊéâÂ∑•‰Ωú, Áõ∏ÂèçËøô‰∫õÂí®ËØ¢ÂëòÂú®Â∑•‰Ωú‰∏≠ÊâæÂà∞‰∫ÜÊõ¥Â§öÁöÑ‰πêË∂£, ÂÜÖÈÉ®ÁöÑÊ≤üÈÄö‰πüÊõ¥È°∫ÁïÖ, ‰πüÂ≠¶Âà∞‰∫ÜÊõ¥Â§öÁöÑ‰∏úË•ø„ÄÇÊàñËÆ∏ÊúÄÈáçË¶ÅÁöÑÊòØ‰∏∫ÂÆ¢Êà∑Êèê‰æõÊõ¥Â•ΩÁöÑ‰∫ßÂìÅ„ÄÇ\n ‰∏∫‰ªÄ‰πàËÅîÁªìÊñáÂåñÁªè‰πÖ‰∏çË°∞? ‰∏Ä‰∏™ÊúâË∂£ÁöÑÈóÆÈ¢ò: ‰∏∫‰ªÄ‰πàÊúâÈÇ£‰πàÂ§öÁöÑÂÖ¨Âè∏ÂÉèÊ≥¢Â£´È°øÂí®ËØ¢ÈõÜÂõ¢‰∏ÄÊ†∑ÂüπËÇ≤‰∏ÄÁßçËøûÊé•ÁöÑÊñáÂåñ, Â∞ΩÁÆ°Âú®‰ªñÁöÑÁ†îÁ©∂‰∏≠ÂèëÁé∞, Ê≠§‰∏æ‰ºöÊçüÂÆ≥ÂëòÂ∑•ÁöÑÂπ∏Á¶èÊÑüÂíåÁîü‰∫ßÊïàÁéá, ‰∏îÂæàÂèØËÉΩÊó†Âä©‰∫éÊèêÈ´òÂ∫ï.\nÁ≠îÊ°àÊòØËøôÊ†∑Êõ¥ÁÆÄÂçï , ÂéüÂõ†Êúâ‰∫å:\n ÂéüÂõ†‰∏Ä,ÂÖ≥‰πé‰∫éÂØπ‰∏™‰∫∫ÈúÄÊ±ÇÁöÑÂìçÂ∫î, Â¶ÇÊûú‰Ω†ÊâÄÂ§ÑÁöÑÂ∑•‰ΩúÁéØÂ¢É‰∏ãÊèêÂá∫ÈóÆÈ¢òËÉΩÂ§üÁ´ãÂàªÂæóÂà∞Á≠îÊ°à, ÈúÄË¶ÅÊüê‰∏ÄÁâπÂÆö‰ø°ÊÅØËÉΩÂ§üÁ´ãÂàªÂæóÂà∞(Âç≥Êó∂Êª°Ë∂≥), ÈÇ£‰πà‰Ω†ÁöÑÁîüÊ¥ªÂ∞±‰ºöÂèòÂæóÊõ¥ÁÆÄÂçï, Ëá≥Â∞ëÂú®ÂΩìÊó∂ÊòØËøôÊ†∑ÁöÑ, Â¶ÇÊûúÊó†Ê≥ïÂæóÂà∞ËøôÊ†∑ËøÖÈÄüÁöÑÂìçÂ∫î, ‰Ω†Â∞±ÈúÄË¶ÅÂÅöÊõ¥Â§öÁöÑÈ¢ÑÂÖàÂ∑•‰ΩúÂíåËÆ°Âàí, ÈúÄË¶ÅÊ†πÊçÆÊù°ÁêÜÊÄß, ÂêåÊó∂ËøòË¶ÅÊó∂ÂàªÂáÜÂ§áÊöÇÊó∂ÊêÅÁΩÆÂ∑•‰Ωú, Â∞ÜÊ≥®ÊÑèÂäõËΩ¨ÁßªÂà∞Âà´Â§Ñ, Á≠âÂæÖÊâÄÊúâË¶ÅÊ±ÇËææÊàê(Âª∂Êó∂Êª°Ë∂≥), ÊâÄÊúâÁöÑËøô‰∫õÈÉΩÂ∞Ü‰Ωø‰Ω†ÁöÑÊó•Â∏∏Â∑•‰ΩúÁîüÊ¥ªÊõ¥Âä†Ëâ∞Èöæ, Â∞ΩÁÆ°‰ªéÈïøÊúüÊù•ËÆ≤ËøôÊ†∑ÂÅöÊõ¥ËÉΩ‰ª§‰∫∫Êª°ÊÑè, ‰ΩÜÊòØÊàë‰ª¨‰∏çÊÉ≥Ëøô‰πàÂ§çÊùÇ„ÄÇ\n ÂéüÂõ†‰∫å, Âú®‰∫éÂÆÉÂèØ‰ª•ÂàõÈÄ†Âá∫‰∏ÄÁßçÁéØÂ¢É, Âú®ËøôÁßçÁéØÂ¢É‰∏ãÂà©Áî®Êî∂‰ª∂ÁÆ±ÁÆ°ÁêÜ‰∏ÄÂ§©ÁöÑÂ∑•‰ΩúÊòØÂèØ‰ª•Ë¢´Êé•ÂèóÁöÑ, Ê¨£ÁÑ∂ÂõûÂ§çÊúÄÊñ∞ÁöÑÈÇÆ‰ª∂, ÂÖ∂‰ªñÁöÑÂàôÂ†ÜÁßØÂú®ÈÇ£Èáå, ÂêåÊó∂ËøòÊÑüËßâËá™Â∑±ÁöÑÊïàÁéá‰ª§‰∫∫Êª°ÊÑè, Â¶ÇÊûúÂ∞ÜÁîµÂ≠êÈÇÆ‰ª∂ËΩ¨Áßª‰∏∫ËæπËøúÂ∑•‰Ωú, ‰Ω†Â∞±ÈúÄË¶ÅÈááÁî®‰∏ÄÁßçÊõ¥Ê∑±ÊÄùÁÜüËôëÁöÑÊñπÂºè, ÁêÜÊ∏Ö‰∏ã‰∏ÄÊ≠•ÁöÑÂ∑•‰ΩúÂÜÖÂÆπ‰ª•ÂèäÂ∑•‰ΩúÊó∂Èïø, ËøôÁßçËÆ°ÂàíÊòØÂæàÂõ∞ÈöæÁöÑ„ÄÇ\n   ‰æã‰ºöÁöÑÂõ∞Â¢É È°πÁõÆ‰∏äÈ¢ëÁπÅÂè¨ÂºÄÁöÑ‰æã‰ºö, Ëøô‰∫õ‰ºöËÆÆÂæÄÂæÄ‰Ωø‰Ω†Êó†Ê≥ïÊåÅÁª≠‰∏ìÊ≥®, ÂØºËá¥Êó•Á®ãÊó†Ê≥ïÂèäÊó∂ÂÆåÊàê, ÂèòÂæóÊîØÁ¶ªÁ†¥Á¢é, ‰∏∫‰ªÄ‰πàËøòË¶ÅÂùöÊåÅÂë¢Ôºü\nÂõ†‰∏∫Êõ¥ÁÆÄÂçï, ÂØπ‰∫éÂæàÂ§ö‰∫∫ËÄåË®Ä, Ëøô‰∫õ‰æã‰ºöÊòØ‰∏ÄÁßçÁÆÄÂçïÁöÑ(‰ΩÜÂêåÊó∂‰πüÊòØÁ¨®ÊãôÁöÑ‰∫∫ÂëòÁÆ°ÁêÜÂΩ¢Âºè), ‰ªñ‰ª¨‰∏çÊÑøÊÑèËá™Â∑±ÂéªÁÆ°ÁêÜÊó∂Èó¥ÂíåÂ∑•‰Ωú‰ªªÂä°, ËÄåÊòØËÆ©ÊØèÂë®Ëø´ËøëÁöÑ‰æã‰ºöËø´‰Ωø‰ªñ‰ª¨Âú®ÁªôÂÆöÈ°πÁõÆ‰∏äÈááÂèñ‰∏Ä‰∫õË°åÂä®ÊàñËÄÖÊèê‰æõ‰∏ÄÁßçÂèñÂæóËøõÂ±ïÁöÑÂèØËßÜÂåñÂπªË±° „ÄÇ\n Ë¥üÈù¢ÂΩ±Âìç  ÊúÄÂ∞èÈòªÂäõÂéüÂàôÂèóÂà∞Â∫¶ÈáèÈªëÊ¥ûÁöÑ‰øùÊä§, ÂæàÂ∞ëÊúâ‰∫∫ÂØπÂÖ∂Âä†‰ª•ÂÆ°ËßÜ, Âú®ËøôÁßçÂéüÂàôÊîØÈÖç‰∏ãÁöÑÂ∑•‰ΩúÊñáÂåñÂÖçÂéª‰∫ÜÊàë‰ª¨Áü≠ÊúüÂÜÖ‰øùÊåÅ‰∏ìÊ≥®Âêà‰ΩúËÆ°ÂàíÁöÑÂøßËôë, Âç¥Áâ∫Áâ≤‰∫ÜÈïøÊúüÁöÑÊª°Ë∂≥ÊÑüÂíåÁúüÂÆû‰ª∑ÂÄºÁöÑ‰∫ßÂá∫, ËøôÊ†∑‰∏ÄÊù•ÊúÄÂ∞è‰∏ªÂäõÂéüÂàôÂ∞±È©±‰ΩøÊàë‰ª¨Âú®Ê∑±Â∫¶Â∑•‰ΩúÊÑàÂèëÂèóÂà∞ÈùíÁùêÁöÑÁªèÊµéÂΩ¢Âäø‰∏ãÊµÅ‰∫éÊµÆÊµÖÂ∑•‰Ωú„ÄÇ\n  Ëß£ÂÜ≥ÂäûÊ≥ï ÈÄâÊã©‰∏Ä‰ª∂ÂØπ‰Ω†ÂæàÈáçË¶ÅÁöÑ‰∫ãÊÉÖ(ËøôÁßç‰∫ãÊÉÖ‰∏ÄËà¨ÊòØÂæàÈöæÂ∫¶ÈáèÊàêÂäüÂíåÂç≥Êó∂ÂèçÈ¶àÁöÑ), ËÆæÂÆö‰∏Ä‰∫õÂ∞èÁöÑËôöÊãüÂèçÈ¶à, ÊØîÂ¶Ç, ËÉå‰∫ÜÂ§öÂ∞ëÂçïËØç, ÂØÑ‰∫ÜÂ§öÂ∞ëÂàÜ, Âà©Áî®ÊâìÊ∏∏Êàè‰∏äÁòæÁöÑ‰∏Ä‰∫õÂéüÁêÜÊù•‰øÉËøõ‰Ω†Âú®Ëøô‰∫õ‰∫ãÊÉÖ‰∏äÂèñÂæóÊé®Ëøõ.\n ","date":1586995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586994024,"objectID":"5bb1df74a7c823edbccc5e89b5ee8cad","permalink":"/post/principle-of-least-resistance/","publishdate":"2020-04-16T00:00:00Z","relpermalink":"/post/principle-of-least-resistance/","section":"post","summary":"Âú®Áº∫‰πèÂ∫¶ÈáèÊàñÂèäÊó∂ÂèçÈ¶àÁöÑÁéØÂ¢É‰∏ã, Â§ßÂ§öÊï∞‰∫∫ÈÉΩ‰ºöÈÄâÊã©ÊúÄÁÆÄÂçïÁöÑË°å‰∏∫Âíå‰∫ãÊÉÖÊù•ÂÅö„ÄÇ\n ‰ªÄ‰πàÊòØÊúÄÂ∞èÈòªÂäõÂéüÂàô  ÊúÄÂ∞èÈòªÂäõÂéüÂàô: ÊòØÊåáÂú®Â∑•‰ΩúÁéØÂ¢É‰∏ã, Ëã•ÂêÑÁßçË°å‰∏∫ÂØπ‰∫éÂ∫ïÁ∫øÁöÑÂΩ±ÂìçÊ≤°ÊúâÂæóÂà∞ÊòéÁ°ÆÁöÑÂèçÈ¶àÊÑèËßÅ , ÈÇ£Êàë‰ª¨ÂàôÂÄæÂêë‰∫éÈááÁî®ÂΩì‰∏ãÊúÄÁÆÄÂçïÊòìË°åÁöÑË°å‰∏∫„ÄÇ\n  ‰øùÊåÅËÅîÁªìÁä∂ÊÄÅÁúüÁöÑÊúâÂä©‰∫éÂ∑•‰Ωú? Á≠îÊ°àÊòØÂê¶ÂÆöÁöÑ.\n‰∏∫‰∫ÜÈ™åËØÅËøô‰∏™ÊÉ≥Ê≥ï, Ê≥¢Â£´È°øÂí®ËØ¢ÈõÜÂõ¢ÈááÁî®‰∫Ü‰∏Ä‰∫õÊûÅÁ´ØÊâãÊÆµ, ‰ªñËø´‰ΩøÂõ¢Èòü‰∏≠ÊØè‰∏ÄÂêçÊàêÂëòÂú®Â∑•‰ΩúÊó•‰∏≠ÈÄâÊã©‰∏ÄÂ§©‰∏ç‰∏äÁè≠, ‰πüÂ∞±ÊòØËØ¥ÂíåÂÖ¨Âè∏ÂÜÖÂ§ñÁöÑ‰ªª‰Ωï‰∫∫ÈÉΩÊñ≠ÂºÄËÅîÁ≥ª.\nÊúÄÂàùËøô‰∏™Âõ¢ÈòüÊúâ‰∫õÊäµÂà∂ËøôÈ°πÂÆûÈ™å, ÊçÆ‰ªñÂõûÂøÜ, ÂÖ∂‰∏≠‰∏Ä‰∫õÂ∞ùËØïË¥üË¥£ÁöÑÂêà‰ºô‰∫∫‰∏ÄÁõ¥ÂØπÊàë‰ª¨ÁöÑÂü∫Êú¨ËßÇÁÇπÂæàÊîØÊåÅ, ‰ΩÜÊòØËøôÊó∂‰πüÁ™ÅÁÑ∂Êúâ‰∫õÁ¥ßÂº†, ÂÆ≥ÊÄïÂëäËØâÂÆ¢Êà∑, ‰ªñÂõ¢Èòü‰∏≠ÁöÑÊØè‰∏Ä‰∏™ÊàêÂëòÊØèÂë®ÈÉΩÊúâ‰∏ÄÂ§©‰∏çÂ∑•‰Ωú, ÈÇ£‰∫õÂí®ËØ¢Âëò‰πüÂêåÊ†∑ÂæàÁ¥ßÂº†ÊãÖÂøß‰ªñ‰ª¨ËøôÊ†∑‰ºöÁΩÆ‰∫ã‰∏ö‰∫éÂç±Êú∫, ‰ΩÜÊòØËøô‰∏™Âõ¢ÈòüÂπ∂Ê≤°ÊúâÂ§±ÂéªÂÆ¢Êà∑, Âõ¢ÈòüÊàêÂëò‰πüÊ≤°Êúâ‰∏¢ÊéâÂ∑•‰Ωú, Áõ∏ÂèçËøô‰∫õÂí®ËØ¢ÂëòÂú®Â∑•‰Ωú‰∏≠ÊâæÂà∞‰∫ÜÊõ¥Â§öÁöÑ‰πêË∂£, ÂÜÖÈÉ®ÁöÑÊ≤üÈÄö‰πüÊõ¥È°∫ÁïÖ, ‰πüÂ≠¶Âà∞‰∫ÜÊõ¥Â§öÁöÑ‰∏úË•ø„ÄÇÊàñËÆ∏ÊúÄÈáçË¶ÅÁöÑÊòØ‰∏∫ÂÆ¢Êà∑Êèê‰æõÊõ¥Â•ΩÁöÑ‰∫ßÂìÅ„ÄÇ","tags":["deep work"],"title":"Principle of least resistance","type":"post"},{"authors":[],"categories":["reading"],"content":"  ËÑëÂäõÊ¥ªÂä®ÈúÄË¶ÅÁöÑÊòØÂèòÂåñÂíåÂàáÊç¢, ‰∏ç‰ºöÊÑüÂà∞Áñ≤ÂÄ¶\n  ‰Ωï‰∏∫ÁîüÊ¥ª? ËÄå‰∏ç‰ªÖ‰ªÖÊòØÁîüÂ≠ò\n ‰∏∫‰ªÄ‰πà‰∏çËÉΩÁî®ÁΩëÁªúÊù•Ê∂àÈÅ£? Â¶Ç‰ΩïÂ∫¶Ëøá‰Ω†ÁöÑ‰∏ÄÂ§© 24 Â∞èÊó∂, How To Live On 24 Hours A Day?\nÂ∞ΩÁÆ°‰∏çÂ§™ÂñúÊ¨¢Ëá™Â∑±ÁöÑËøô‰∏ÄÂ§©ÁöÑÁîüÊ¥ª, ‰ΩÜÊòØËøòÊòØÊää 8:30-17:30 ËøôÊÆµÊó∂Èó¥ÂΩìÂÅöÊòØËøô‰∏ÄÂ§©ÁöÑÁîüÊ¥ª, Ëøô‰πãÂâçÁöÑÂíå‰πãÂêéÁöÑÊó∂Èó¥ÊòØËøô‰∏ÄÂ§©ÁöÑÂâçÂ•èÂíåÂ∞æÂ£∞.\nÂ∫îËØ•Ë¶ÅÂÉèË¥µÊóè‰∏ÄÊ†∑‰ΩøÁî®Ëá™Â∑±ÁöÑÊó∂Èó¥\nÂ®±‰πêÊó∂Èó¥‰ªçÁÑ∂ÊòØÂ†ïËêΩÁöÑ, ‰∏ªË¶ÅÊòØËÉ°‰π±ÁÇπÂáª‰∏Ä‰∫õÊ≤°Êúâ‰ª∑ÂÄºÁöÑÊï∞Â≠óÂ®±‰πê‰ø°ÊÅØ\n‰Ω†Â∫îËØ•ËÉΩÂ§ü‰∏ªÂä®Âà©Áî®Ëá™Â∑±Â∑•‰ΩúÂ§ñÁöÑÊó∂Èó¥ÂíåÁ≤æÂäõ\nÂáèÂ∞ëÁΩëÁªúÂ∑•ÂÖ∑ÂØπ‰Ω†Ê∑±Â∫¶Â∑•‰ΩúËÉΩÂäõÁöÑÂπ≤Êâ∞\nÂ∞ΩÂèØËÉΩÂê∏Âºï‰Ω†Êó∂Èó¥ÂíåÊ≥®ÊÑèÂäõÁöÑÁΩëÁ´ô  facebook reddit wechat Twitter ‰ºòÈÖ∑ Áà±Â•áËâ∫ ÂìîÂì©ÂìîÂì© ÊäñÈü≥ Âø´Êâã  Ëøô‰∫õÁΩëÁ´ôÁöÑÂÖ±ÂêåÁÇπ: ‰ΩøÁî®Á≤æÂøÉÈõïÁê¢ÁöÑÊ†áÈ¢òÂíåÂÆπÊòìÁêÜËß£ÁöÑÂÜÖÂÆπ, ËæÖ‰ª•Êï∞Â≠¶ÁÆóÊ≥ïÊâìÁ£®, ÊúÄÂ§ßÈôêÂ∫¶ÁöÑÂê∏Âºï‰Ω†ÁöÑÁúºÁêÉ\n‰∏ÄÂë®ÁöÑÂ∑•‰ΩúÊó∂Èó¥ÁªìÊùü, Ëøô‰∫õÁΩëÁ´ôÂ∞±Êàê‰∫Ü‰Ω†ÁöÑ‰∏ªË¶ÅÂ®±‰πê.\nËøô‰∫õ‰∫ãÊÉÖÈÉΩÊòØÊçüÂÆ≥‰Ω†ÊäµÊäóÂàÜÂøÉ‰∫ãÁâ©ÁöÑËÉΩÂäõ, ‰Ωø‰Ω†Âú®ËØïÂõæÊ∑±Â∫¶Â∑•‰ΩúÁöÑÊó∂ÂÄôÊõ¥ÈöæÈõÜ‰∏≠Ê≥®ÊÑèÂäõ.\nËøô‰∫õ‰∫ãÊÉÖÊÄªÊòØÂîæÊâãÂèØÂæó, ËÆ©‰Ω†Âç≥Êó∂Êª°Ë∂≥.(‰πüËÆ∏Â¢ûÂä†Ëøô‰∫õ‰∫ãÊÉÖËé∑ÂæóÈöæÂ∫¶ÊòØ‰∏Ä‰∏™ÂäûÊ≥ï, ÊúÄÂ∞èÈòªÂäõ‰πãË∑Ø)\n  Áî®‰ªÄ‰πàÊù•Ê∂àÈÅ£?  Âú®‰Ω†ÂÖ®ÈÉ®ÁöÑÊ∏ÖÈÜíÊó∂Èó¥ÈÉΩÁªôËá™Â∑±ÁöÑÂ§ßËÑëÊâæÂà∞ÊúâÊÑè‰πâÁöÑ‰∫ãÊÉÖÂéªÂÅöÔºåËÄå‰∏çÊòØÊîæ‰ªªËá™Â∑±Âú®Ëø∑Á≥äÁöÑÁä∂ÊÄÅ‰∏ãÊº´Êó†ÁõÆÁöÑÁöÑÊµèËßàÂá†‰∏™Â∞èÊó∂ÁöÑÁΩëÈ°µÔºåÈÇ£‰πàÂú®Ëøô‰∏ÄÂ§©ÁªìÊùüÁöÑÊó∂ÂÄôÔºå‰Ω†‰ºöËßâÂæóÊõ¥Âä†ÂÖÖÂÆûÔºåÁ¨¨2Â§©ÂºÄÂßã‰πüÊõ¥Âä†ËΩªÊùæ„ÄÇ\n Ê†∏ÂøÉÊÄùÊÉ≥ Âú®‰Ω†ÁöÑÂ®±‰πêÊó∂Èó¥ÂÅöÊõ¥Â§öÁöÑÊÄùËÄÉ, ‰∏ªÂä®ÊÄùËÄÉÊàëÂ∫îËØ•ÊÄé‰πàÂ∫¶ËøáËøô‰∏ÄÂ§©?\n Ëøô‰∫õËá¥ÁòæÁöÑÁΩëÂùÄÂè™ÊúâÂú®ÁúüÁ©∫‰∏≠ÊâçËÉΩÁîüÂ≠ò: Â¶ÇÊûú‰Ω†Âú®Êüê‰∏ÄÊÆµÊó∂Èó¥Ê≤°ÊúâÁªôËá™Â∑±ÂÆâÊéí‰ªªÂä°, Ëøô‰∫õÁΩëÁ´ôÊÄªÊòØ‰∏ÄÁßçÊúâËØ±ÊÉëÁöÑÈÄâÊã©. Â¶ÇÊûú‰Ω†Âú®Ëá™Áî±Êó∂Èó¥ÊúâÈ´òË¥®ÈáèÁöÑ‰∫ãÊÉÖÂéªÂÅö, Ëøô‰∫õÁΩëÁ´ôÂØπ‰Ω†ÁöÑÊ≥®ÊÑèÂäõÁöÑÊéßÂà∂Â∞±‰ºöÂáèÂº±.\n  Êìç‰Ωú Âú®Âë®Êú´Âà∞Êù•‰πãÂâçÁ°ÆÂÆöË¶ÅÂÅöÁöÑ‰∫ãÊÉÖÊòØÂçÅÂàÜÈáçË¶ÅÁöÑ\n‰∏Ä‰∫õÂÆâÊéíÂ•ΩÁöÑÁà±Â•Ω‰∏∫Ëøô‰∫õÊó∂Èó¥Êèê‰æõ‰∫ÜÂÖÖÂàÜÁöÑÂÖªÊñô ‰∏∫‰∫ÜÁâπÂÆöÁöÑÁõÆÊ†áÂÆåÊàêÁâπÂÆöÁöÑÊ¥ªÂä®, ËøôÂ∞ÜÂ°´Êª°‰Ω†ÁöÑÊó∂Èó¥\n ÈòÖËØª ÈîªÁÇº Âî±Ê≠å Áà±Â•Ω Êñ∞ÁöÑÊäÄËÉΩ ÂíåËÄÅÂèãËßÅÈù¢    ÊÄªÁªì Â¶ÇÊûú‰Ω†ÊÉ≥ÊäµÂæ°Â®±‰πêÁΩëÁ´ôÂØπ‰Ω†Êó∂Èó¥ÂíåÁ≤æÂäõÁöÑËØ±ÊÉëÔºåÈÇ£‰πàÂ∞±ÁªôÂ§ßËÑëÊâæ‰∏Ä‰∫õÈ´òË¥®ÈáèÁöÑÊõø‰ª£Ê¥ªÂä®ÔºåËøôÊ†∑‰∏ç‰ªÖÂèØ‰ª•‰ΩøÊàë‰ª¨ÈÅøÂÖçÂàÜÂøÉÔºå‰øùÊåÅ‰∏ìÊ≥®ÁöÑËÉΩÂäõÔºåÂêåÊó∂ËøòÊúâÂèØËÉΩÂÆûÁé∞‰∏Ä‰∏™ÂÆè‰ºüÁõÆÊ†áÔºå‰ΩìÈ™åÂà∞‰Ωï‰∏∫ÁîüÊ¥ªËÄå‰∏ç‰ªÖ‰ªÖÊòØÁîüÂ≠ò„ÄÇ\n ","date":1586908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586905640,"objectID":"cf75c1a8843435e8308aa2993a027357","permalink":"/post/don-t-use-the-internet-for-entertainment/","publishdate":"2020-04-15T00:00:00Z","relpermalink":"/post/don-t-use-the-internet-for-entertainment/","section":"post","summary":"ËÑëÂäõÊ¥ªÂä®ÈúÄË¶ÅÁöÑÊòØÂèòÂåñÂíåÂàáÊç¢, ‰∏ç‰ºöÊÑüÂà∞Áñ≤ÂÄ¶\n  ‰Ωï‰∏∫ÁîüÊ¥ª? ËÄå‰∏ç‰ªÖ‰ªÖÊòØÁîüÂ≠ò\n ‰∏∫‰ªÄ‰πà‰∏çËÉΩÁî®ÁΩëÁªúÊù•Ê∂àÈÅ£? Â¶Ç‰ΩïÂ∫¶Ëøá‰Ω†ÁöÑ‰∏ÄÂ§© 24 Â∞èÊó∂, How To Live On 24 Hours A Day?\nÂ∞ΩÁÆ°‰∏çÂ§™ÂñúÊ¨¢Ëá™Â∑±ÁöÑËøô‰∏ÄÂ§©ÁöÑÁîüÊ¥ª, ‰ΩÜÊòØËøòÊòØÊää 8:30-17:30 ËøôÊÆµÊó∂Èó¥ÂΩìÂÅöÊòØËøô‰∏ÄÂ§©ÁöÑÁîüÊ¥ª, Ëøô‰πãÂâçÁöÑÂíå‰πãÂêéÁöÑÊó∂Èó¥ÊòØËøô‰∏ÄÂ§©ÁöÑÂâçÂ•èÂíåÂ∞æÂ£∞.\nÂ∫îËØ•Ë¶ÅÂÉèË¥µÊóè‰∏ÄÊ†∑‰ΩøÁî®Ëá™Â∑±ÁöÑÊó∂Èó¥\nÂ®±‰πêÊó∂Èó¥‰ªçÁÑ∂ÊòØÂ†ïËêΩÁöÑ, ‰∏ªË¶ÅÊòØËÉ°‰π±ÁÇπÂáª‰∏Ä‰∫õÊ≤°Êúâ‰ª∑ÂÄºÁöÑÊï∞Â≠óÂ®±‰πê‰ø°ÊÅØ\n‰Ω†Â∫îËØ•ËÉΩÂ§ü‰∏ªÂä®Âà©Áî®Ëá™Â∑±Â∑•‰ΩúÂ§ñÁöÑÊó∂Èó¥ÂíåÁ≤æÂäõ","tags":["deep work"],"title":"‰∏çË¶ÅÁî®ÁΩëÁªúÊù•Ê∂àÈÅ£, Áî®‰ªÄ‰πàÊù•Ê∂àÈÅ£?","type":"post"},{"authors":[],"categories":["Tools"],"content":" introduction This document contains all the code that is displayed during the workshop. The document is an RMarkdown document which means that it can be compiled, along with the code chunks thus executing and capturing the output of the code within the document. To read more about RMarkdown see the website for the package, as well as the Get Started guide.\nExercises While it is encouraged to follow along in this document as the workshop progresses and execute the code to see the result, an important part is also to experiment and play, thus learning how the different settings affect the output.\nThe document will contain code chunks with the code examples discussed during the talk, but it will also contain chunks intended for completing small exercises. These will use the examples as a starting point and ask you to modify the code to achieve a given output. Completing these are optional, but highly recommended, either during or after the workshop.\n Dependencies This document comes with a list of required packages.\n Datasets We will use an assortment of datasets throughout the document. The purpose is mostly to showcase different plots, and less on getting some divine insight into the world. While not necessary we will call data(\u0026lt;dataset\u0026gt;) before using a new dataset to indicate the introduction of a new dataset.\n  Introduction We will look at the basic ggplot2 use using the faithful dataset, giving information on the eruption pattern of the Old Faithful geyser in Yellowstone National Park.\nIf an aesthetic is linked to data it is put into aes()\nÂ¶ÇÊûúÊï∞ÊçÆÁöÑÈ¢úËâ≤ÊòØÊò†Â∞ÑÂà∞‰∏ÄÂàóÊï∞ÊçÆÂàôÊîæÂú® aes() ÈáåÈù¢\nIf you simple want to set it to a value, put it outside of aes()\nSome geoms only need a single mapping and will calculate the rest for you\nÂçïÂèòÈáè mapping: histogram\ngeoms are drawn in the order they are added. The point layer is thus drawn on top of the density contours in the example below.\nËøô‰∏™ÊúâÁÇπÂÉè phoshop ‰∏≠ÁöÑÂõæÂ±ÇÊ¶ÇÂøµ, geometry ÊòØ‰∏ÄÂ±Ç‰∏ÄÂ±ÇÂæÄ‰∏äÂ†ÜÂè†ÁöÑ\nExercise Modify the code below to make the points larger squares and slightly transparent. See ?geom_point for more information on the point layer.\nHint 1: transparency is controlled with alpha, and shape with shape Hint 2: rememberthe difference between mapping and setting aesthetics\nColour the two distributions in the histogram with different colours\nHint 1: For polygons you can map two different colour-like aesthetics: colour (the colour of the stroke) and fill (the fill colour)\nColour the distributions in the histogram by whether waiting is above or below 60. What happens?\nChange the plot above by setting position = 'dodge' in geom_histogram() (while keeping the colouring by waiting). What do position control?\nAdd a line that separates the two point distributions. See ?geom_abline for how to draw straight lines from a slope and intercept.\n Stat We will use the mpg dataset giving information about fuel economy on different car models.\nEvery geom has a stat . This is why new data (count) can appear when using geom_bar()., the default stat for geom_bar is count\nThe stat can be overwritten. If we have precomputed count we don‚Äôt want any additional computations to perform and we use the identity stat to leave the data alone\nMost obvious geom+stat combinations have a dedicated geom constructor. The one above is available directly as geom_col()\nValues calculated by the stat is available with the after_stat() function inside aes(). You can do all sorts of computations inside that.\nMany stats provide multiple variations of the same calculation, and provides a default (here, density)\nWhile the others must be used with the after_stat() function\nExercises While most people use geom_*() when adding layers, it is just as valid to add a stat_*() with an attached geom.\nÂ§ßÂ§öÊï∞‰∫∫‰ΩøÁî® geom Êó∂, ‰ΩøÁî®‰∫ÜÈªòËÆ§ÁöÑ stat , Âè™ÊòØ‰∏çÁü•ÈÅìËÄåÂ∑≤\nLook at geom_bar() and figure out which stat it uses as default. Then modify the code to use the stat directly instead (i.e.¬†adding stat_*() instead of geom_bar())\nUse stat_summary() to add a red dot at the mean hwy for each group\nstat_summary is like dplyr::summarise\nfun ‰ΩúÁî®‰∫é aes ‰∏≠ÁöÑ y, x Á±ª‰ºº‰∫é group_by ‰∏≠ÁöÑ factor variable.\nHint: You will need to change the default geom of stat_summary()\n  Scales Scales define how the mapping you specify inside aes() should happen. All mappings have an associated scale even if not specified.\ntake control by adding one explicitly. All scales follow the same naming conventions.\nscale_mapping_* : scale_x, y, colour, fill etc.\nPositional mappings (x and y) also have associated scales.\nExercises Use RColorBrewer::display.brewer.all() to see all the different palettes from Color Brewer and pick your favourite. Modify the code below to use it\nModify the code below to create a bubble chart (scatterplot with size mapped to a continuous variable) showing cyl with size. Make sure that only the present amount of cylinders (4, 5, 6, and 8) are present in the legend.\nHint: The breaks argument in the scale is used to control which values are present in the legend.\nExplore the different types of size scales available in ggplot2. Is the default the most appropriate here?\nModify the code below so that colour is no longer mapped to the discrete class variable, but to the continuous cty variable. What happens to the guide?\nThe type of guide can be controlled with the guide argument in the scale, or with the guides() function. Continuous colours have a gradient colour bar by default, but setting it to legend will turn it back to the standard look. What happens when multiple aesthetics are mapped to the same variable and uses the guide type?\n  Facets The facet defines how data is split among panels. The default facet (facet_null()) puts all the data in a single panel, while facet_wrap() and facet_grid() allows you to specify different types of small multiples\nExercises One of the great things about facets is that they share the axes between the different panels. Sometimes this is undiserable though, and the behaviour can be changed with the scales argument . Experiment with the different possible settings in the plot below:\nUsually the space occupied by each panel is equal. This can create problems when different scales are used. Modify the code below so that the y scale differs between the panels in the plot. What happens?\nUse the space argument in facet_grid() to change the plot above so each bar has the same width again.\nFacets can be based on multiple variables by adding them together. Try to recreate the same panels present in the plot below by using facet_wrap()\n  Coordinates The coordinate system is the fabric you draw your layers on in the end. The default `coord_cartesion provides the standard rectangular x-y coordinate system. Changing the coordinate system can have dramatic effects\nYou can zoom both on the scale‚Ä¶\nand in the coord. You usually want the latter as it avoids changing the plottet data\n Setting limits on the coordinate system will zoom the plot (like you‚Äôre looking at it with a magnifying glass ), and will not change the underlying data like setting limits on a scale will.\n Exercises In the same way as limits can be set in both the positional scale and the coord, so can transformations, using coord_trans(). Modify the code below to apply a log transformation to the y axis; first using scale_y_continuous(), and then using coord_trans(). Compare the results ‚Äî how do they differ?\nCoordinate systems are particularly important in cartography. While we will not spend a lot of time with it in this workshop, spatial plotting is well supported in ggplot2 with geom_sf() and coord_sf() (which interfaces with the sf package). The code below produces a world map. Try changing the crs argument in coord_sf() to be '+proj=robin' (This means using the Robinson projection).\nMaps are a huge area in data visualisation and simply too big to cover in this workshop. If you want to explore further I advice you to explore the r-spatial wbsite as well as the website for the sf package\n  Theme Theming defines the feel and look of your final visualisation and is something you will normally defer to the final polishing of the plot. It is very easy to change looks with a prebuild theme\nFurther adjustments can be done in the end to get exactly the look you want\nExercises Themes can be overwhelming, especially as you often try to optimise for beauty while you learn. To remove the last part of the equation, the exercise is to take the plot given below and make it as hideous as possible using the theme function. Go absolutely crazy, but take note of the effect as you change different settings.\n   üìå Extensions While ggplot2 comes with a lot of batteries included, the extension ecosystem provides priceless additinal features\nPlot composition We start by creating 3 separate plots\nCombining them with patchwork is a breeze using the different operators\nExcercises Patchwork will assign the same amount of space to each plot by default, but this can be controlled with the widths and heights argument in plot_layout(). This can take a numeric vector giving their relative sizes (e.g.¬†c(2, 1) will make the first plot twice as big as the second). Modify the code below so that the middle plot takes up half of the total space:\nThe \u0026amp; operator can be used with any type of ggplot2 object, not just themes. Modify the code below so the two plots share the same y-axis (same limits)\nPatchwork contains many features for fine tuning the layout and annotation. Very complex layouts can be obtained by providing a design specification to the design argument in plot_layout(). The design can be defined as a textual representation of the cells. Use the layout given below. How should the textual representation be undertood.\n  Animation ggplot2 is usually focused on static plots, but gganimate extends the API and grammar to describe animations. As such it feels like a very natural extension of using ggplot2\nThere are many different transitions that control how data is interpreted for animation, as well as a range of other animation specific features\nExercises The animation below will animate between points showing cars with different cylinders.\ngganimate uses the group aesthetic to match observations between states. By default the group aesthetic is set to the same value, so observations are matched by their position (first row of 4 cyl is matched to first row of 5 cyl etc.). This is clearly wrong here (why?). Add a mapping to the group aesthetic to ensure that points do not move between the different states.\nIn the presence of discrete aesthetic mappings (colour below), the group is deduced if not given. The default behaviour of objects that appear and disappear during the animation is to simply pop in and out of existance. enter_*() and exit_*() functions can be used to control this behaviour. Experiment with the different enter and exit functions provided by gganimate below. What happens if you add multiple enter or exit functions to the same animation?\nIn the animation below (as in all the other animations) the changes happens at constant speed. How values change during an animation is called easing and can be controlled using the ease_aes() function. Read the documentation for ease_aes() and experiment with different easings in the animation.\n  Annotation Text is a huge part of storytelling with your visualisation. Historically, textual annotations has not been the best part of ggplot2 but new extensions make up for that.\nStandard geom_text will often result in overlaping labels\nggrepel takes care of that\nIf you want to highlight certain parts of your data and describe it, the geom_mark_*() family of geoms have your back\nExercises ggrepel has a tonne of settings for controlling how text labels move. Often, though, the most effective is simply to not label everything. There are two strategies for that: Either only use a subset of the data for the repel layer, or setting the label to \"\" for those you don‚Äôt want to plot. Try both in the plot below where you only label 10 random points.\nExplore the documentation for geom_text_repel. Find a way to ensure that the labels in the plot below only repels in the vertical direction\nggforce comes with 4 different types of mark geoms. Try them all out in the code below:\n  Networks ggplot2 has been focused on tabular data. Network data in any shape and form is handled by ggraph\ndendrograms are just a specific type of network\nExercies Most network plots are defined by a layout algorithm, which takes the network structure and calculate a position for each node. The layout algorithm is global and set in the ggraph(). The default auto layout will inspect the network object and try to choose a sensible layout for it (e.g.¬†dendrogram for a hierarchical clustering as above). There is, however no optimal layout and it is often a good idea to try out different layouts. Try out different layouts in the graph below. See the the website for an overview of the different layouts.\nThere are many different ways to draw edges. Try to use geom_edge_parallel() in the graph below to show the presence of multiple edges\nFaceting works in ggraph as it does in ggplot2, but you must choose to facet by either nodes or edges. Modify the graph below to facet the edges by the year variable (using facet_edges())\n  Looks Many people have already desgned beautiful (and horrible) themes for you. Use them as a base\n  Drawing anything  ","date":1586736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586764493,"objectID":"aff6dd0f5c504eebcd56d1794a2ce45b","permalink":"/post/ggplot2-workshop/","publishdate":"2020-04-13T00:00:00Z","relpermalink":"/post/ggplot2-workshop/","section":"post","summary":"introduction This document contains all the code that is displayed during the workshop. The document is an RMarkdown document which means that it can be compiled, along with the code chunks thus executing and capturing the output of the code within the document.","tags":["vis"],"title":"ggplot2 Workshop","type":"post"},{"authors":[],"categories":["Tools"],"content":" set environment  links  dformoso/sklearn-classification: Data Science Notebook on a Classification Task, using sklearn and Tensorflow.\n dformoso/sklearn-classification: Data Science Notebook on a Classification Task, using sklearn and Tensorflow.\n   ","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586156091,"objectID":"49fc4d170c416264b12eebf29e461153","permalink":"/post/sklearn-classification/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/post/sklearn-classification/","section":"post","summary":"set environment  links  dformoso/sklearn-classification: Data Science Notebook on a Classification Task, using sklearn and Tensorflow.\n dformoso/sklearn-classification: Data Science Notebook on a Classification Task, using sklearn and Tensorflow.","tags":["Machine learning","sklearn"],"title":"sklearn classification ","type":"post"},{"authors":[],"categories":["Tools"],"content":" library(reticulate) reticulate::use_python(\u0026quot;/Users/zero/anaconda3/envs/reticulate/bin/python3\u0026quot;) ","date":1586044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586086617,"objectID":"74bb914ab61a07ce0b0fc4b51d42ddb0","permalink":"/post/use-selenium-in-python/","publishdate":"2020-04-05T00:00:00Z","relpermalink":"/post/use-selenium-in-python/","section":"post","summary":" library(reticulate) reticulate::use_python(\u0026quot;/Users/zero/anaconda3/envs/reticulate/bin/python3\u0026quot;) ","tags":["spider","python"],"title":"Use selenium in python","type":"post"},{"authors":[],"categories":["Tools"],"content":" load data  Packages The packages that are required to build animated plots in R are:\n ggplot2 gganimate  While those above two are the essential packages, We have also used the entire tidyverse, janitor and scales in this project for Data Manipulation, Cleaning and Formatting.\ndata preprocessing gdp \u0026lt;- gdp_data[1:217,] gdp_tidy \u0026lt;- gdp %\u0026gt;% mutate_at(vars(contains(\u0026quot;YR\u0026quot;)), as.numeric) %\u0026gt;% #gather(year, value, 3:13) %\u0026gt;% pivot_longer(cols = contains(\u0026quot;YR\u0026quot;), names_to = \u0026quot;year\u0026quot;, values_to = \u0026quot;value\u0026quot;) %\u0026gt;% janitor::clean_names(.) %\u0026gt;% mutate(year = as.numeric(stringr::str_sub(year, 1, 4))) # write_csv(gdp_tidy,\u0026quot;./data/gdp_tidy.csv\u0026quot;)  Animated Plot An Animated Plot building process involves two primary sections:\n Building the entire set of actual static plots using ggplot2 Animating the static plots with desired parameters using gganimate  The final step after these two primary steps is to render the animation in the desired file format, like GIF or MP4 (Video).\n create label columns create a few more columns that will help us display labels in the plot.\ngdp_formatted \u0026lt;- gdp_tidy %\u0026gt;% group_by(year) %\u0026gt;% # The * 1 makes it possible to have non-integer ranks while sliding mutate(rank = rank(-value), # -100 ÊúÄÂ∞è, ÊéíÁ¨¨‰∏Ä Value_rel = value/value[rank==1], Value_lbl = paste0(\u0026quot; \u0026quot;,round(value/1e9))) %\u0026gt;% group_by(country_name) %\u0026gt;% filter(rank \u0026lt;=10) %\u0026gt;% ungroup() %\u0026gt;% filter(!is.na(value)) %\u0026gt;% select(-series_code, -series_name) staticplot \u0026lt;- gdp_formatted %\u0026gt;% ggplot(aes( rank, group = country_name, fill = as.factor(country_name), color = as.factor(country_name) )) + geom_tile(aes(y = value/2, height = value, width = 0.9), alpha = 0.8, color = NA) + geom_text(aes(y = 0, label = paste(country_name, \u0026quot; \u0026quot;)), vjust = 0.2, hjust = 1) + geom_text(aes(y=value,label = Value_lbl, hjust=0)) + coord_flip(clip = \u0026quot;off\u0026quot;, expand = FALSE) + scale_y_continuous(labels = scales::comma) + scale_x_reverse() + guides(color = FALSE, fill = FALSE) + theme(axis.line=element_blank(), axis.text.x=element_blank(), axis.text.y=element_blank(), axis.ticks=element_blank(), axis.title.x=element_blank(), axis.title.y=element_blank(), legend.position=\u0026quot;none\u0026quot;, panel.background=element_blank(), panel.border=element_blank(), panel.grid.major=element_blank(), panel.grid.minor=element_blank(), panel.grid.major.x = element_line( size=.1, color=\u0026quot;grey\u0026quot; ), panel.grid.minor.x = element_line( size=.1, color=\u0026quot;grey\u0026quot; ), plot.title=element_text(size=25, hjust=0.5, face=\u0026quot;bold\u0026quot;, colour=\u0026quot;grey\u0026quot;, vjust=-1), plot.subtitle=element_text(size=18, hjust=0.5, face=\u0026quot;italic\u0026quot;, color=\u0026quot;grey\u0026quot;), plot.caption =element_text(size=8, hjust=0.5, face=\u0026quot;italic\u0026quot;, color=\u0026quot;grey\u0026quot;), plot.background=element_blank(), plot.margin = margin(2,2, 2, 4, \u0026quot;cm\u0026quot;))  Animation The key function here is transition_states() which stitches the individual static plots together by year. view_follow() is used to give a view as if the background lines (gridlines) are moving as the animation is progressing.\nanim \u0026lt;- staticplot + transition_states(year, transition_length = 4, state_length = 2) + view_follow(fixed_x = TRUE) + labs(title = \u0026#39;GDP per Year : {closest_state}\u0026#39;, subtitle = \u0026quot;Top 10 Countries\u0026quot;, caption = \u0026quot;GDP in Billions USD | Data Source: World Bank Data\u0026quot;)  Rendering With the animation being built (ready) and saved in the object anim , It‚Äôs time for us to render the animation using animate() function. The renderer used in the animate() differs based on the type of output file required.\nFor GIF File Format:\nFor GIF: animate(anim, 200, fps = 20, width = 1200, height = 1000, renderer = gifski_renderer(here(\u0026quot;figure/gganim.gif\u0026quot;)))  For MP4 # animate(anim, 200, fps = 20, width = 1200, height = 1000, # renderer = ffmpeg_renderer()) -\u0026gt; for_mp4anim_save(\u0026quot;animation.mp4\u0026quot;, animation = for_mp4 )    Links  How to create Bar Race Animation Charts in R ¬∑ Programming with R   ","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699864,"objectID":"18846f1d076c6517c641a579d592156f","permalink":"/post/how-to-create-bar-race-animation-charts-in-r/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/post/how-to-create-bar-race-animation-charts-in-r/","section":"post","summary":"load data  Packages The packages that are required to build animated plots in R are:\n ggplot2 gganimate  While those above two are the essential packages, We have also used the entire tidyverse, janitor and scales in this project for Data Manipulation, Cleaning and Formatting.","tags":["vis"],"title":"How To Create Bar Race Animation Charts In R","type":"post"},{"authors":[],"categories":["reading"],"content":"  Balanced accuracy  Accuracy is the proportion of the data that are predicted correctly.\n  Balanced accuracy is computed here as the average of sens() and spec()\n \\[\\mathrm{BA}=\\frac{0.5 \\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}+\\frac{0.5 \\mathrm{TN}}{\\mathrm{TN}+\\mathrm{FP}}\\]\nMulticlass  Macro, micro, and macro-weighted averaging is available for this metric.\n library(dplyr) data(hpc_cv) hpc_cv %\u0026gt;% count(obs, sort = TRUE) %\u0026gt;% kable(\u0026quot;html\u0026quot;) %\u0026gt;% kable_styling(\u0026quot;hover\u0026quot;, full_width = F)   obs  n      VF  1769    F  1078    M  412    L  208     hpc_cv %\u0026gt;% filter(Resample == \u0026quot;Fold01\u0026quot;) %\u0026gt;% #bal_accuracy(obs, pred, estimator = \u0026quot;macro\u0026quot;) bal_accuracy(obs, pred, estimator = \u0026quot;macro_weighted\u0026quot;)  ## # A tibble: 1 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 bal_accuracy macro_weighted 0.771  #bal_accuracy(obs, pred, estimator = \u0026quot;micro\u0026quot;)  multi-class prob sum is equal 1? The Answer Is Yes.\nhpc_cv %\u0026gt;% head(6) %\u0026gt;% rowwise() %\u0026gt;% mutate(prob = sum(VF, `F`, M, L)) %\u0026gt;% select(prob) ## Source: local data frame [6 x 1] ## Groups: \u0026lt;by row\u0026gt; ## ## # A tibble: 6 x 1 ## prob ## \u0026lt;dbl\u0026gt; ## 1 1 ## 2 1.00 ## 3 1.00 ## 4 1.00 ## 5 1.00 ## 6 1.  what is the mean of Macro, micro, and macro-weighted averaging? Macro-averaging\nÂÆèÂπ≥ÂùáÔºàMacro-averagingÔºâÊòØÊåáÊâÄÊúâÁ±ªÂà´ÁöÑÊØè‰∏Ä‰∏™ÁªüËÆ°ÊåáÊ†áÂÄºÁöÑÁÆóÊï∞Âπ≥ÂùáÂÄºÔºå‰πüÂ∞±ÊòØÂÆèÁ≤æÁ°ÆÁéáÔºàMacro-PrecisionÔºâÔºåÂÆèÂè¨ÂõûÁéáÔºàMacro-RecallÔºâÔºåÂÆèFÂÄºÔºàMacro-F ScoreÔºâÔºåÂÖ∂ËÆ°ÁÆóÂÖ¨ÂºèÂ¶Ç‰∏ãÔºö\n\\[ P_{\\text {macro}}=\\frac{1}{n} \\sum_{i=1}^{n} P_{i} \\]\nMicro-averaging\nÂæÆÂπ≥ÂùáÔºàMicro-averagingÔºâÊòØÂØπÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÊØè‰∏Ä‰∏™Á§∫‰æã‰∏çÂàÜÁ±ªÂà´ËøõË°åÁªüËÆ°Âª∫Á´ãÂÖ®Â±ÄÊ∑∑Ê∑ÜÁü©ÈòµÔºåÁÑ∂ÂêéËÆ°ÁÆóÁõ∏Â∫îÁöÑÊåáÊ†á„ÄÇÂÖ∂ËÆ°ÁÆóÂÖ¨ÂºèÂ¶Ç‰∏ãÔºö\n\\[ P_{\\text {micro}}=\\frac{T \\bar{P}}{T \\bar{P}+\\overline{F P}}=\\frac{\\sum_{i=1}^{n} T P_{i}}{\\sum_{i=1}^{n} T P_{i}+\\sum_{i=1}^{n} F P_{i}} \\]\n Macro-averaging‰∏éMicro-averagingÁöÑ‰∏çÂêå‰πãÂ§ÑÂú®‰∫éÔºöMacro-averagingËµã‰∫àÊØè‰∏™Á±ªÁõ∏ÂêåÁöÑÊùÉÈáçÔºåÁÑ∂ËÄåMicro-averagingËµã‰∫àÊØè‰∏™Ê†∑Êú¨ÂÜ≥Á≠ñÁõ∏ÂêåÁöÑÊùÉÈáç„ÄÇ\n  summary   MCC: Matthews correlation coefficient \\[ \\mathrm{MCC}=\\frac{\\mathrm{TP} \\times \\mathrm{TN}-\\mathrm{FP} \\times \\mathrm{FN}}{\\sqrt{(\\mathrm{TP}+\\mathrm{FP})(\\mathrm{TP}+\\mathrm{FN})(\\mathrm{TN}+\\mathrm{FP})(\\mathrm{TN}+\\mathrm{FN})}} \\]\n# Multiclass # mcc() has a natural multiclass extension # pred is predicted class results (that is also factor) hpc_cv %\u0026gt;% group_by(Resample) %\u0026gt;% mcc(obs, pred) ## # A tibble: 10 x 4 ## Resample .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Fold01 mcc multiclass 0.542 ## 2 Fold02 mcc multiclass 0.521 ## 3 Fold03 mcc multiclass 0.602 ## 4 Fold04 mcc multiclass 0.519 ## 5 Fold05 mcc multiclass 0.520 ## 6 Fold06 mcc multiclass 0.494 ## 7 Fold07 mcc multiclass 0.461 ## 8 Fold08 mcc multiclass 0.538 ## 9 Fold09 mcc multiclass 0.459 ## 10 Fold10 mcc multiclass 0.498  notes where TP means ‚Äútrue positives‚Äù, TN ‚Äútrue negatives‚Äù, FP ‚Äúfalse positives‚Äù, and FN ‚Äúfalse negatives‚Äù.\n links  Balanced accuracy ‚Äî bal_accuracy ‚Ä¢ yardstick\n Matthews correlation coefficient ‚Äî mcc ‚Ä¢ yardstick\n   ","date":1585612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585644913,"objectID":"9f3fd9643486cb22d5c1e087fef3cbcf","permalink":"/post/balance-accuracy-and-mcc/","publishdate":"2020-03-31T00:00:00Z","relpermalink":"/post/balance-accuracy-and-mcc/","section":"post","summary":"Balanced accuracy  Accuracy is the proportion of the data that are predicted correctly.\n  Balanced accuracy is computed here as the average of sens() and spec()","tags":["Machine learning"],"title":"Balance Accuracy and MCC","type":"post"},{"authors":[],"categories":[],"content":" barlpot # how to create a bar plot with ggplot2 library(ggthemes) library(ggplot2) #how to create a barplot with label using ggplot2 package iris %\u0026gt;% group_by(Species) %\u0026gt;% summarise(counts = n()) %\u0026gt;% mutate(Species = fct_reorder(Species, counts)) %\u0026gt;% ggplot(aes(x = Species, y = counts, fill = Species)) + geom_bar(stat = \u0026quot;identity\u0026quot;) + geom_text(aes(label = counts), hjust = 1.6, color = \u0026quot;white\u0026quot;, size = 5) + coord_flip() + theme_minimal()+ scale_color_tableau() + scale_fill_tableau()+ labs(caption = \u0026quot;figure 01\u0026quot;) + theme(axis.text.x = element_text(angle = 0, hjust = 1), legend.position = \u0026quot;bottom\u0026quot;, plot.caption=element_text(size=12,family = \u0026quot;Arial\u0026quot;,face = \u0026quot;bold\u0026quot;, hjust=0, margin=margin(t=15)))  ","date":1585612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585631091,"objectID":"c12dd6a666679d33b3c322e6b1179e9c","permalink":"/post/just-say-hello/","publishdate":"2020-03-31T00:00:00Z","relpermalink":"/post/just-say-hello/","section":"post","summary":"barlpot # how to create a bar plot with ggplot2 library(ggthemes) library(ggplot2) #how to create a barplot with label using ggplot2 package iris %\u0026gt;% group_by(Species) %\u0026gt;% summarise(counts = n()) %\u0026gt;% mutate(Species = fct_reorder(Species, counts)) %\u0026gt;% ggplot(aes(x = Species, y = counts, fill = Species)) + geom_bar(stat = \u0026quot;identity\u0026quot;) + geom_text(aes(label = counts), hjust = 1.","tags":[],"title":"Just say hello","type":"post"},{"authors":[],"categories":["Tools"],"content":" Introduction  {tidyverse} ecosystem that has become the defacto standard for data importation, manipulation, and visualization in R.\n  My graduate training left me with a deep understanding of linear models and design-based causal inference, but with little or no training in other types of predictive modeling, unsupervised machine learning, version control, or putting models into production.\n  I chose the Applied Machine Learning workshop in order to fill the gap in my knowledge about machine learning models beyond OLS and logistic regression.\n  Max Kuhn and Davis Vaughn were the two workshop leaders and I knew they were in the process of developing the {tidymodels} ecosystem, which stands to be a successor to their popular {caret} package and promises fill the modeling gap in the {tidyverse} ecosystem.\n  load data library(tidymodels) library(AmesHousing) data \u0026lt;- make_ames() data %\u0026gt;% head() %\u0026gt;% str() ## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 6 obs. of 81 variables: ## $ MS_SubClass : Factor w/ 16 levels \u0026quot;One_Story_1946_and_Newer_All_Styles\u0026quot;,..: 1 1 1 1 6 6 ## $ MS_Zoning : Factor w/ 7 levels \u0026quot;Floating_Village_Residential\u0026quot;,..: 3 2 3 3 3 3 ## $ Lot_Frontage : num 141 80 81 93 74 78 ## $ Lot_Area : int 31770 11622 14267 11160 13830 9978 ## $ Street : Factor w/ 2 levels \u0026quot;Grvl\u0026quot;,\u0026quot;Pave\u0026quot;: 2 2 2 2 2 2 ## $ Alley : Factor w/ 3 levels \u0026quot;Gravel\u0026quot;,\u0026quot;No_Alley_Access\u0026quot;,..: 2 2 2 2 2 2 ## $ Lot_Shape : Factor w/ 4 levels \u0026quot;Regular\u0026quot;,\u0026quot;Slightly_Irregular\u0026quot;,..: 2 1 2 1 2 2 ## $ Land_Contour : Factor w/ 4 levels \u0026quot;Bnk\u0026quot;,\u0026quot;HLS\u0026quot;,\u0026quot;Low\u0026quot;,..: 4 4 4 4 4 4 ## $ Utilities : Factor w/ 3 levels \u0026quot;AllPub\u0026quot;,\u0026quot;NoSeWa\u0026quot;,..: 1 1 1 1 1 1 ## $ Lot_Config : Factor w/ 5 levels \u0026quot;Corner\u0026quot;,\u0026quot;CulDSac\u0026quot;,..: 1 5 1 1 5 5 ## $ Land_Slope : Factor w/ 3 levels \u0026quot;Gtl\u0026quot;,\u0026quot;Mod\u0026quot;,\u0026quot;Sev\u0026quot;: 1 1 1 1 1 1 ## $ Neighborhood : Factor w/ 28 levels \u0026quot;North_Ames\u0026quot;,\u0026quot;College_Creek\u0026quot;,..: 1 1 1 1 7 7 ## $ Condition_1 : Factor w/ 9 levels \u0026quot;Artery\u0026quot;,\u0026quot;Feedr\u0026quot;,..: 3 2 3 3 3 3 ## $ Condition_2 : Factor w/ 8 levels \u0026quot;Artery\u0026quot;,\u0026quot;Feedr\u0026quot;,..: 3 3 3 3 3 3 ## $ Bldg_Type : Factor w/ 5 levels \u0026quot;OneFam\u0026quot;,\u0026quot;TwoFmCon\u0026quot;,..: 1 1 1 1 1 1 ## $ House_Style : Factor w/ 8 levels \u0026quot;One_and_Half_Fin\u0026quot;,..: 3 3 3 3 8 8 ## $ Overall_Qual : Factor w/ 10 levels \u0026quot;Very_Poor\u0026quot;,\u0026quot;Poor\u0026quot;,..: 6 5 6 7 5 6 ## $ Overall_Cond : Factor w/ 10 levels \u0026quot;Very_Poor\u0026quot;,\u0026quot;Poor\u0026quot;,..: 5 6 6 5 5 6 ## $ Year_Built : int 1960 1961 1958 1968 1997 1998 ## $ Year_Remod_Add : int 1960 1961 1958 1968 1998 1998 ## $ Roof_Style : Factor w/ 6 levels \u0026quot;Flat\u0026quot;,\u0026quot;Gable\u0026quot;,..: 4 2 4 4 2 2 ## $ Roof_Matl : Factor w/ 8 levels \u0026quot;ClyTile\u0026quot;,\u0026quot;CompShg\u0026quot;,..: 2 2 2 2 2 2 ## $ Exterior_1st : Factor w/ 16 levels \u0026quot;AsbShng\u0026quot;,\u0026quot;AsphShn\u0026quot;,..: 4 14 15 4 14 14 ## $ Exterior_2nd : Factor w/ 17 levels \u0026quot;AsbShng\u0026quot;,\u0026quot;AsphShn\u0026quot;,..: 11 15 16 4 15 15 ## $ Mas_Vnr_Type : Factor w/ 5 levels \u0026quot;BrkCmn\u0026quot;,\u0026quot;BrkFace\u0026quot;,..: 5 4 2 4 4 2 ## $ Mas_Vnr_Area : num 112 0 108 0 0 20 ## $ Exter_Qual : Factor w/ 4 levels \u0026quot;Excellent\u0026quot;,\u0026quot;Fair\u0026quot;,..: 4 4 4 3 4 4 ## $ Exter_Cond : Factor w/ 5 levels \u0026quot;Excellent\u0026quot;,\u0026quot;Fair\u0026quot;,..: 5 5 5 5 5 5 ## $ Foundation : Factor w/ 6 levels \u0026quot;BrkTil\u0026quot;,\u0026quot;CBlock\u0026quot;,..: 2 2 2 2 3 3 ## $ Bsmt_Qual : Factor w/ 6 levels \u0026quot;Excellent\u0026quot;,\u0026quot;Fair\u0026quot;,..: 6 6 6 6 3 6 ## $ Bsmt_Cond : Factor w/ 6 levels \u0026quot;Excellent\u0026quot;,\u0026quot;Fair\u0026quot;,..: 3 6 6 6 6 6 ## $ Bsmt_Exposure : Factor w/ 5 levels \u0026quot;Av\u0026quot;,\u0026quot;Gd\u0026quot;,\u0026quot;Mn\u0026quot;,..: 2 4 4 4 4 4 ## $ BsmtFin_Type_1 : Factor w/ 7 levels \u0026quot;ALQ\u0026quot;,\u0026quot;BLQ\u0026quot;,\u0026quot;GLQ\u0026quot;,..: 2 6 1 1 3 3 ## $ BsmtFin_SF_1 : num 2 6 1 1 3 3 ## $ BsmtFin_Type_2 : Factor w/ 7 levels \u0026quot;ALQ\u0026quot;,\u0026quot;BLQ\u0026quot;,\u0026quot;GLQ\u0026quot;,..: 7 4 7 7 7 7 ## $ BsmtFin_SF_2 : num 0 144 0 0 0 0 ## $ Bsmt_Unf_SF : num 441 270 406 1045 137 ... ## $ Total_Bsmt_SF : num 1080 882 1329 2110 928 ... ## $ Heating : Factor w/ 6 levels \u0026quot;Floor\u0026quot;,\u0026quot;GasA\u0026quot;,..: 2 2 2 2 2 2 ## $ Heating_QC : Factor w/ 5 levels \u0026quot;Excellent\u0026quot;,\u0026quot;Fair\u0026quot;,..: 2 5 5 1 3 1 ## $ Central_Air : Factor w/ 2 levels \u0026quot;N\u0026quot;,\u0026quot;Y\u0026quot;: 2 2 2 2 2 2 ## $ Electrical : Factor w/ 6 levels \u0026quot;FuseA\u0026quot;,\u0026quot;FuseF\u0026quot;,..: 5 5 5 5 5 5 ## $ First_Flr_SF : int 1656 896 1329 2110 928 926 ## $ Second_Flr_SF : int 0 0 0 0 701 678 ## $ Low_Qual_Fin_SF : int 0 0 0 0 0 0 ## $ Gr_Liv_Area : int 1656 896 1329 2110 1629 1604 ## $ Bsmt_Full_Bath : num 1 0 0 1 0 0 ## $ Bsmt_Half_Bath : num 0 0 0 0 0 0 ## $ Full_Bath : int 1 1 1 2 2 2 ## $ Half_Bath : int 0 0 1 1 1 1 ## $ Bedroom_AbvGr : int 3 2 3 3 3 3 ## $ Kitchen_AbvGr : int 1 1 1 1 1 1 ## $ Kitchen_Qual : Factor w/ 5 levels \u0026quot;Excellent\u0026quot;,\u0026quot;Fair\u0026quot;,..: 5 5 3 1 5 3 ## $ TotRms_AbvGrd : int 7 5 6 8 6 7 ## $ Functional : Factor w/ 8 levels \u0026quot;Maj1\u0026quot;,\u0026quot;Maj2\u0026quot;,..: 8 8 8 8 8 8 ## $ Fireplaces : int 2 0 0 2 1 1 ## $ Fireplace_Qu : Factor w/ 6 levels \u0026quot;Excellent\u0026quot;,\u0026quot;Fair\u0026quot;,..: 3 4 4 6 6 3 ## $ Garage_Type : Factor w/ 7 levels \u0026quot;Attchd\u0026quot;,\u0026quot;Basment\u0026quot;,..: 1 1 1 1 1 1 ## $ Garage_Finish : Factor w/ 4 levels \u0026quot;Fin\u0026quot;,\u0026quot;No_Garage\u0026quot;,..: 1 4 4 1 1 1 ## $ Garage_Cars : num 2 1 1 2 2 2 ## $ Garage_Area : num 528 730 312 522 482 470 ## $ Garage_Qual : Factor w/ 6 levels \u0026quot;Excellent\u0026quot;,\u0026quot;Fair\u0026quot;,..: 6 6 6 6 6 6 ## $ Garage_Cond : Factor w/ 6 levels \u0026quot;Excellent\u0026quot;,\u0026quot;Fair\u0026quot;,..: 6 6 6 6 6 6 ## $ Paved_Drive : Factor w/ 3 levels \u0026quot;Dirt_Gravel\u0026quot;,..: 2 3 3 3 3 3 ## $ Wood_Deck_SF : int 210 140 393 0 212 360 ## $ Open_Porch_SF : int 62 0 36 0 34 36 ## $ Enclosed_Porch : int 0 0 0 0 0 0 ## $ Three_season_porch: int 0 0 0 0 0 0 ## $ Screen_Porch : int 0 120 0 0 0 0 ## $ Pool_Area : int 0 0 0 0 0 0 ## $ Pool_QC : Factor w/ 5 levels \u0026quot;Excellent\u0026quot;,\u0026quot;Fair\u0026quot;,..: 4 4 4 4 4 4 ## $ Fence : Factor w/ 5 levels \u0026quot;Good_Privacy\u0026quot;,..: 5 3 5 5 3 5 ## $ Misc_Feature : Factor w/ 6 levels \u0026quot;Elev\u0026quot;,\u0026quot;Gar2\u0026quot;,..: 3 3 2 3 3 3 ## $ Misc_Val : int 0 0 12500 0 0 0 ## $ Mo_Sold : int 5 6 6 4 3 6 ## $ Year_Sold : int 2010 2010 2010 2010 2010 2010 ## $ Sale_Type : Factor w/ 10 levels \u0026quot;COD\u0026quot;,\u0026quot;Con\u0026quot;,\u0026quot;ConLD\u0026quot;,..: 10 10 10 10 10 10 ## $ Sale_Condition : Factor w/ 6 levels \u0026quot;Abnorml\u0026quot;,\u0026quot;AdjLand\u0026quot;,..: 5 5 5 5 5 5 ## $ Sale_Price : int 215000 105000 172000 244000 189900 195500 ## $ Longitude : num -93.6 -93.6 -93.6 -93.6 -93.6 ... ## $ Latitude : num 42.1 42.1 42.1 42.1 42.1 ...  EDA # data %\u0026gt;% # select_if(is.factor) %\u0026gt;% # #select(-State_Of_Origin) %\u0026gt;% # gather() %\u0026gt;% # ggplot(aes(x = value)) + # facet_wrap( ~ key, scales = \u0026quot;free\u0026quot;, ncol = 3) + # geom_bar() # data %\u0026gt;% # select_if(is.numeric) %\u0026gt;% # gather() %\u0026gt;% # ggplot(aes(x = value)) + # facet_wrap( ~ key, scales = \u0026quot;free\u0026quot;, ncol = 4) + # geom_density()  data type # dt_new \u0026lt;- # dt_new %\u0026gt;% # mutate(Promoted_or_Not = factor(Promoted_or_Not)) # numeric_variables \u0026lt;- c( # \u0026quot;total_years_dispatcher\u0026quot;, \u0026quot;total_years_present_job\u0026quot;, # \u0026quot;childrendependents\u0026quot;, \u0026quot;children_under_2_yrs\u0026quot;, # \u0026quot;sick_days_in_last_year\u0026quot;, \u0026quot;avg_work_hrs_week\u0026quot; # ) # # factor_variables \u0026lt;- setdiff(colnames(sleep), numeric_variables) # # sleep \u0026lt;- mutate_at(sleep, vars(factor_variables), as.factor)  Stratified training/test splits ## Setting seed set.seed(1) ## Generate split data_split \u0026lt;- initial_split(data, prop = 4/5, strata = \u0026quot;Sale_Price\u0026quot;) ## Printing the function gives us \u0026lt;Num Rows in Training Set/Num Rows in Testing Set/Total Num Rows\u0026gt; data_split ## \u0026lt;2346/584/2930\u0026gt; ## Calling training() on this object will give us our training set, and calling testing() on it will give us our testing set data_train \u0026lt;- training(data_split) data_test \u0026lt;- testing(data_split) data_train %\u0026gt;% head() %\u0026gt;% knitr::kable()   MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape Land_Contour Utilities Lot_Config Land_Slope Neighborhood Condition_1 Condition_2 Bldg_Type House_Style Overall_Qual Overall_Cond Year_Built Year_Remod_Add Roof_Style Roof_Matl Exterior_1st Exterior_2nd Mas_Vnr_Type Mas_Vnr_Area Exter_Qual Exter_Cond Foundation Bsmt_Qual Bsmt_Cond Bsmt_Exposure BsmtFin_Type_1 BsmtFin_SF_1 BsmtFin_Type_2 BsmtFin_SF_2 Bsmt_Unf_SF Total_Bsmt_SF Heating Heating_QC Central_Air Electrical First_Flr_SF Second_Flr_SF Low_Qual_Fin_SF Gr_Liv_Area Bsmt_Full_Bath Bsmt_Half_Bath Full_Bath Half_Bath Bedroom_AbvGr Kitchen_AbvGr Kitchen_Qual TotRms_AbvGrd Functional Fireplaces Fireplace_Qu Garage_Type Garage_Finish Garage_Cars Garage_Area Garage_Qual Garage_Cond Paved_Drive Wood_Deck_SF Open_Porch_SF Enclosed_Porch Three_season_porch Screen_Porch Pool_Area Pool_QC Fence Misc_Feature Misc_Val Mo_Sold Year_Sold Sale_Type Sale_Condition Sale_Price Longitude Latitude    One_Story_1946_and_Newer_All_Styles Residential_Low_Density 141 31770 Pave No_Alley_Access Slightly_Irregular Lvl AllPub Corner Gtl North_Ames Norm Norm OneFam One_Story Above_Average Average 1960 1960 Hip CompShg BrkFace Plywood Stone 112 Typical Typical CBlock Typical Good Gd BLQ 2 Unf 0 441 1080 GasA Fair Y SBrkr 1656 0 0 1656 1 0 1 0 3 1 Typical 7 Typ 2 Good Attchd Fin 2 528 Typical Typical Partial_Pavement 210 62 0 0 0 0 No_Pool No_Fence None 0 5 2010 WD Normal 215000 -93.61975 42.05403  One_Story_1946_and_Newer_All_Styles Residential_High_Density 80 11622 Pave No_Alley_Access Regular Lvl AllPub Inside Gtl North_Ames Feedr Norm OneFam One_Story Average Above_Average 1961 1961 Gable CompShg VinylSd VinylSd None 0 Typical Typical CBlock Typical Typical No Rec 6 LwQ 144 270 882 GasA Typical Y SBrkr 896 0 0 896 0 0 1 0 2 1 Typical 5 Typ 0 No_Fireplace Attchd Unf 1 730 Typical Typical Paved 140 0 0 0 120 0 No_Pool Minimum_Privacy None 0 6 2010 WD Normal 105000 -93.61976 42.05301  One_Story_1946_and_Newer_All_Styles Residential_Low_Density 81 14267 Pave No_Alley_Access Slightly_Irregular Lvl AllPub Corner Gtl North_Ames Norm Norm OneFam One_Story Above_Average Above_Average 1958 1958 Hip CompShg Wd Sdng Wd Sdng BrkFace 108 Typical Typical CBlock Typical Typical No ALQ 1 Unf 0 406 1329 GasA Typical Y SBrkr 1329 0 0 1329 0 0 1 1 3 1 Good 6 Typ 0 No_Fireplace Attchd Unf 1 312 Typical Typical Paved 393 36 0 0 0 0 No_Pool No_Fence Gar2 12500 6 2010 WD Normal 172000 -93.61939 42.05266  Two_Story_1946_and_Newer Residential_Low_Density 74 13830 Pave No_Alley_Access Slightly_Irregular Lvl AllPub Inside Gtl Gilbert Norm Norm OneFam Two_Story Average Average 1997 1998 Gable CompShg VinylSd VinylSd None 0 Typical Typical PConc Good Typical No GLQ 3 Unf 0 137 928 GasA Good Y SBrkr 928 701 0 1629 0 0 2 1 3 1 Typical 6 Typ 1 Typical Attchd Fin 2 482 Typical Typical Paved 212 34 0 0 0 0 No_Pool Minimum_Privacy None 0 3 2010 WD Normal 189900 -93.63893 42.06090  Two_Story_1946_and_Newer Residential_Low_Density 78 9978 Pave No_Alley_Access Slightly_Irregular Lvl AllPub Inside Gtl Gilbert Norm Norm OneFam Two_Story Above_Average Above_Average 1998 1998 Gable CompShg VinylSd VinylSd BrkFace 20 Typical Typical PConc Typical Typical No GLQ 3 Unf 0 324 926 GasA Excellent Y SBrkr 926 678 0 1604 0 0 2 1 3 1 Good 7 Typ 1 Good Attchd Fin 2 470 Typical Typical Paved 360 36 0 0 0 0 No_Pool No_Fence None 0 6 2010 WD Normal 195500 -93.63893 42.06078  One_Story_PUD_1946_and_Newer Residential_Low_Density 41 4920 Pave No_Alley_Access Regular Lvl AllPub Inside Gtl Stone_Brook Norm Norm TwnhsE One_Story Very_Good Average 2001 2001 Gable CompShg CemntBd CmentBd None 0 Good Typical PConc Good Typical Mn GLQ 3 Unf 0 722 1338 GasA Excellent Y SBrkr 1338 0 0 1338 1 0 2 0 2 1 Good 6 Typ 0 No_Fireplace Attchd Fin 2 582 Typical Typical Paved 0 0 170 0 0 0 No_Pool No_Fence None 0 4 2010 WD Normal 213500 -93.63379 42.06298     Data pre-processing recipe data_rec \u0026lt;- recipe( Sale_Price ~ ., data = data_train ) %\u0026gt;% step_log(Sale_Price, base = 10) %\u0026gt;% step_rm(matches(\u0026quot;Qual\u0026quot;), matches(\u0026quot;Cond\u0026quot;)) %\u0026gt;% # Remove unwanted variables step_dummy(all_nominal()) %\u0026gt;% #step_downsample(Promoted_or_Not) %\u0026gt;% step_center(all_predictors()) %\u0026gt;% step_scale(all_predictors()) %\u0026gt;% step_pca(contains(\u0026quot;SF\u0026quot;), contains(\u0026quot;Area\u0026quot;), threshold = .75) %\u0026gt;% #will convert numeric data into one or more principal components. step_nzv(all_predictors()) # üëç # data_rec ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 80 ## ## Operations: ## ## Log transformation on Sale_Price ## Delete terms matches, Qual, matches, Cond ## Dummy variables from all_nominal ## Centering for all_predictors ## Scaling for all_predictors ## No PCA components were extracted. ## Sparse, unbalanced variable filter on all_predictors  prep data_rec_trained \u0026lt;- prep(data_rec, training = data_train, verbose = TRUE) ## oper 1 step log [training] ## oper 2 step rm [training] ## oper 3 step dummy [training] ## oper 4 step center [training] ## oper 5 step scale [training] ## oper 6 step pca [training] ## oper 7 step nzv [training] ## The retained training set is ~ 1.57 Mb in memory. data_rec_trained ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 80 ## ## Training data contained 2346 data points and no missing data. ## ## Operations: ## ## Log transformation on Sale_Price [trained] ## Variables removed Overall_Qual, Exter_Qual, Bsmt_Qual, ... [trained] ## Dummy variables from MS_SubClass, MS_Zoning, Street, Alley, ... [trained] ## Centering for Lot_Frontage, Lot_Area, ... [trained] ## Scaling for Lot_Frontage, Lot_Area, ... [trained] ## PCA extraction with BsmtFin_SF_1, BsmtFin_SF_2, ... [trained] ## Sparse, unbalanced variable filter removed Kitchen_AbvGr, ... [trained]  juice  Not bad, we‚Äôve reduced 13 variables down to 7. This probably wasn‚Äôt the best use case of PCA, but it provides a good example of some advanced preprocessing made simple in {recipes}.\n data_rec_trained %\u0026gt;% juice() %\u0026gt;% select(starts_with(\u0026quot;PC\u0026quot;)) # select principal component ## # A tibble: 2,346 x 7 ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -1.63 1.16 0.650 -0.440 -0.264 -0.601 -2.05 ## 2 0.874 0.305 0.311 -0.656 -0.345 -0.475 0.143 ## 3 -0.613 1.57 0.0386 -0.106 -0.610 -1.44 -0.0504 ## 4 0.381 0.953 0.421 0.341 -0.105 -0.989 -0.548 ## 5 0.166 0.957 0.363 0.458 -0.486 -1.57 0.0933 ## 6 -0.0448 -0.268 -0.665 -0.337 -0.121 0.258 0.389 ## 7 -0.174 -0.249 -0.612 0.215 0.607 0.664 0.287 ## 8 -1.61 0.459 -0.0692 0.765 0.622 0.0958 0.471 ## 9 0.0192 -1.53 0.449 0.0537 -0.312 -0.149 0.251 ## 10 -0.0231 1.90 0.0478 0.197 -0.799 -1.95 0.531 ## # ‚Ä¶ with 2,336 more rows   Modeling  The beauty of {parsnip} is that it unifies the interface for model specifications so that you don‚Äôt need to remember dozens of different interfaces for each implementation of a model.\n lasso Now let‚Äôs specify our model. We‚Äôre going to go with a Lasso model with a penalty of 0.001 using the {parsnip} package.\nmodel data_lasso \u0026lt;- linear_reg(penalty = 0.001, mixture = 1) %\u0026gt;% set_engine(\u0026quot;glmnet\u0026quot;)  workflow Using workflows, we don‚Äôt need to go through the prep() and juice() steps we went through earlier when we go to fit our model (I demonstrated prep() and juice() as they can be useful for being able to inspect your pre-processed data as we did earlier).\ndata_lasso_wfl \u0026lt;- workflow() %\u0026gt;% add_recipe(data_rec) %\u0026gt;% #recipe add_model(data_lasso) # model data_lasso_wfl ## ‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ## Preprocessor: Recipe ## Model: linear_reg() ## ## ‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ## 7 Recipe Steps ## ## ‚óè step_log() ## ‚óè step_rm() ## ‚óè step_dummy() ## ‚óè step_center() ## ‚óè step_scale() ## ‚óè step_pca() ## ‚óè step_nzv() ## ## ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = 0.001 ## mixture = 1 ## ## Computational engine: glmnet  fit model data_lasso_fit \u0026lt;- fit(data_lasso_wfl, data_train)  predict predict(data_lasso_fit, data_train) %\u0026gt;% slice(1:5) ## # A tibble: 5 x 1 ## .pred ## \u0026lt;dbl\u0026gt; ## 1 5.32 ## 2 5.04 ## 3 5.17 ## 4 5.30 ## 5 5.31    Model evaluation using metrics from the {yardstick} package\nmetrics:\nRoot Mean Squared Error (RMSE) R squared the concordance correlation coefficient (ccc)  all in sample perf_metrics \u0026lt;- metric_set(rmse, rsq, ccc) perf_lasso \u0026lt;- data_lasso_fit %\u0026gt;% predict(data_train) %\u0026gt;% bind_cols(juice(data_rec_trained)) %\u0026gt;% perf_metrics(truth = Sale_Price, estimate = .pred) perf_lasso %\u0026gt;% arrange(.metric) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ccc standard 0.925 ## 2 rmse standard 0.0648 ## 3 rsq standard 0.864  cross-validation # create 10-fold cross-validation sets for evaluating our training set models using vfold_cv(), which defaults to creating 10 folds. cv_splits \u0026lt;- vfold_cv(data_train) cv_splits ## # 10-fold cross-validation ## # A tibble: 10 x 2 ## splits id ## \u0026lt;named list\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026lt;split [2.1K/235]\u0026gt; Fold01 ## 2 \u0026lt;split [2.1K/235]\u0026gt; Fold02 ## 3 \u0026lt;split [2.1K/235]\u0026gt; Fold03 ## 4 \u0026lt;split [2.1K/235]\u0026gt; Fold04 ## 5 \u0026lt;split [2.1K/235]\u0026gt; Fold05 ## 6 \u0026lt;split [2.1K/235]\u0026gt; Fold06 ## 7 \u0026lt;split [2.1K/234]\u0026gt; Fold07 ## 8 \u0026lt;split [2.1K/234]\u0026gt; Fold08 ## 9 \u0026lt;split [2.1K/234]\u0026gt; Fold09 ## 10 \u0026lt;split [2.1K/234]\u0026gt; Fold10 # take our workflow and use it to fit 10 models on these 10 splits using the fit_resamples() function from the {tune} package (also a part of the tidymodels ecosystem), as well as tell it to compute the performance metrics we set earlier. cv_eval \u0026lt;- fit_resamples(data_lasso_wfl, resamples = cv_splits, metrics = perf_metrics) cv_eval ## # 10-fold cross-validation ## # A tibble: 10 x 4 ## splits id .metrics .notes ## * \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 \u0026lt;split [2.1K/235]\u0026gt; Fold01 \u0026lt;tibble [3 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; ## 2 \u0026lt;split [2.1K/235]\u0026gt; Fold02 \u0026lt;tibble [3 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; ## 3 \u0026lt;split [2.1K/235]\u0026gt; Fold03 \u0026lt;tibble [3 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; ## 4 \u0026lt;split [2.1K/235]\u0026gt; Fold04 \u0026lt;tibble [3 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; ## 5 \u0026lt;split [2.1K/235]\u0026gt; Fold05 \u0026lt;tibble [3 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; ## 6 \u0026lt;split [2.1K/235]\u0026gt; Fold06 \u0026lt;tibble [3 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; ## 7 \u0026lt;split [2.1K/234]\u0026gt; Fold07 \u0026lt;tibble [3 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; ## 8 \u0026lt;split [2.1K/234]\u0026gt; Fold08 \u0026lt;tibble [3 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; ## 9 \u0026lt;split [2.1K/234]\u0026gt; Fold09 \u0026lt;tibble [3 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; ## 10 \u0026lt;split [2.1K/234]\u0026gt; Fold10 \u0026lt;tibble [3 √ó 3]\u0026gt; \u0026lt;tibble [0 √ó 1]\u0026gt; collect_metrics(cv_eval) ## # A tibble: 3 x 5 ## .metric .estimator mean n std_err ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ccc standard 0.915 10 0.00660 ## 2 rmse standard 0.0673 10 0.00345 ## 3 rsq standard 0.854 10 0.0122  in-sample VS cross-validation perf_lasso %\u0026gt;% arrange(.metric) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ccc standard 0.925 ## 2 rmse standard 0.0648 ## 3 rsq standard 0.864 collect_metrics(cv_eval) ## # A tibble: 3 x 5 ## .metric .estimator mean n std_err ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ccc standard 0.915 10 0.00660 ## 2 rmse standard 0.0673 10 0.00345 ## 3 rsq standard 0.854 10 0.0122   Model tuning  its real power comes in allowing us to easily tune the hyperparameters in our model.\n üìå tuning lasso data_mixture \u0026lt;- linear_reg(penalty = tune(), mixture = tune()) %\u0026gt;% set_engine(\u0026quot;glmnet\u0026quot;) data_mixture_wfl \u0026lt;- update_model(data_lasso_wfl, data_mixture)  set regular paraments for grid search Next, we will define a parameter space to search. {tune} allows you to perform either grid search (where the candidate values are pre-defined) or iterative search (ex: Bayesian optimization) where the results of the previous model are used to select the next parameter values to try.\nThere are pros/cons to each. A big plus of grid search is that it allows you to take advantage of parallel processing to speed up your search, while iterative search is, by construction, sequential. A big plus of iterative search is that it can quickly rule out areas of parameter space which can be efficient when covering many values of a high dimensional parameter space (where a grid may require many, many models to comfortably cover the entire parameter space, where many of them may turn out to be redundant).\nFor this post, we‚Äôre going to stick with grid search. The simplest form of grid search uses regular grids, where you provide a vector of values for each parameter and the grid is composed of every possible value combination.\n{tune} provides useful defaults for searching parameter spaces of many common hyperparameters, for example, creating grids for the ‚Äúpenalty‚Äù parameter in log-10 space. We can simply specify the parameters, pass these to grid_regular(), and specify that we want 5 levels of penalization and 5 levels of mixture.\nmixture_param \u0026lt;- parameters(penalty(), mixture()) regular_grid \u0026lt;- grid_regular(mixture_param, levels = c(5, 5)) regular_grid %\u0026gt;% ggplot(aes(x = mixture, y = penalty)) + geom_point() + scale_y_log10()  set non-regular paraments for grid search {tune} also provides ways to create non-regular grids as well.\nRandom grids generated using grid_random() will uniformly sample the parameter space.\nSpace-filling designs (SFD) generated using grid_max_entropy() will try to keep candidate values away from one another in order to more efficiently cover the parameter space.\nThe below shows how to create a SFD grid and plots 25 candidate values.\nsfd_grid \u0026lt;- grid_max_entropy(mixture_param, size = 25) sfd_grid ## # A tibble: 25 x 2 ## penalty mixture ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1.19e- 7 0.997 ## 2 6.06e- 1 0.0986 ## 3 5.44e- 9 0.718 ## 4 2.27e- 5 0.215 ## 5 2.20e- 7 0.826 ## 6 1.24e- 1 0.929 ## 7 3.25e- 4 0.693 ## 8 6.89e- 3 0.00913 ## 9 1.01e- 6 0.517 ## 10 1.71e-10 0.970 ## # ‚Ä¶ with 15 more rows sfd_grid %\u0026gt;% ggplot(aes(x = mixture, y = penalty)) + geom_point() + scale_y_log10()  parallelization set up library(doParallel) all_cores \u0026lt;- parallel::detectCores(logical = FALSE) cl \u0026lt;- makePSOCKcluster(all_cores-1) registerDoParallel(cl) clusterEvalQ(cl, {library(tidymodels)}) ## [[1]] ## [1] \u0026quot;yardstick\u0026quot; \u0026quot;workflows\u0026quot; \u0026quot;tune\u0026quot; \u0026quot;tibble\u0026quot; \u0026quot;rsample\u0026quot; ## [6] \u0026quot;tidyr\u0026quot; \u0026quot;recipes\u0026quot; \u0026quot;purrr\u0026quot; \u0026quot;parsnip\u0026quot; \u0026quot;infer\u0026quot; ## [11] \u0026quot;ggplot2\u0026quot; \u0026quot;dplyr\u0026quot; \u0026quot;dials\u0026quot; \u0026quot;scales\u0026quot; \u0026quot;broom\u0026quot; ## [16] \u0026quot;tidymodels\u0026quot; \u0026quot;stats\u0026quot; \u0026quot;graphics\u0026quot; \u0026quot;grDevices\u0026quot; \u0026quot;utils\u0026quot; ## [21] \u0026quot;datasets\u0026quot; \u0026quot;methods\u0026quot; \u0026quot;base\u0026quot; ## ## [[2]] ## [1] \u0026quot;yardstick\u0026quot; \u0026quot;workflows\u0026quot; \u0026quot;tune\u0026quot; \u0026quot;tibble\u0026quot; \u0026quot;rsample\u0026quot; ## [6] \u0026quot;tidyr\u0026quot; \u0026quot;recipes\u0026quot; \u0026quot;purrr\u0026quot; \u0026quot;parsnip\u0026quot; \u0026quot;infer\u0026quot; ## [11] \u0026quot;ggplot2\u0026quot; \u0026quot;dplyr\u0026quot; \u0026quot;dials\u0026quot; \u0026quot;scales\u0026quot; \u0026quot;broom\u0026quot; ## [16] \u0026quot;tidymodels\u0026quot; \u0026quot;stats\u0026quot; \u0026quot;graphics\u0026quot; \u0026quot;grDevices\u0026quot; \u0026quot;utils\u0026quot; ## [21] \u0026quot;datasets\u0026quot; \u0026quot;methods\u0026quot; \u0026quot;base\u0026quot; ## ## [[3]] ## [1] \u0026quot;yardstick\u0026quot; \u0026quot;workflows\u0026quot; \u0026quot;tune\u0026quot; \u0026quot;tibble\u0026quot; \u0026quot;rsample\u0026quot; ## [6] \u0026quot;tidyr\u0026quot; \u0026quot;recipes\u0026quot; \u0026quot;purrr\u0026quot; \u0026quot;parsnip\u0026quot; \u0026quot;infer\u0026quot; ## [11] \u0026quot;ggplot2\u0026quot; \u0026quot;dplyr\u0026quot; \u0026quot;dials\u0026quot; \u0026quot;scales\u0026quot; \u0026quot;broom\u0026quot; ## [16] \u0026quot;tidymodels\u0026quot; \u0026quot;stats\u0026quot; \u0026quot;graphics\u0026quot; \u0026quot;grDevices\u0026quot; \u0026quot;utils\u0026quot; ## [21] \u0026quot;datasets\u0026quot; \u0026quot;methods\u0026quot; \u0026quot;base\u0026quot; ## ## [[4]] ## [1] \u0026quot;yardstick\u0026quot; \u0026quot;workflows\u0026quot; \u0026quot;tune\u0026quot; \u0026quot;tibble\u0026quot; \u0026quot;rsample\u0026quot; ## [6] \u0026quot;tidyr\u0026quot; \u0026quot;recipes\u0026quot; \u0026quot;purrr\u0026quot; \u0026quot;parsnip\u0026quot; \u0026quot;infer\u0026quot; ## [11] \u0026quot;ggplot2\u0026quot; \u0026quot;dplyr\u0026quot; \u0026quot;dials\u0026quot; \u0026quot;scales\u0026quot; \u0026quot;broom\u0026quot; ## [16] \u0026quot;tidymodels\u0026quot; \u0026quot;stats\u0026quot; \u0026quot;graphics\u0026quot; \u0026quot;grDevices\u0026quot; \u0026quot;utils\u0026quot; ## [21] \u0026quot;datasets\u0026quot; \u0026quot;methods\u0026quot; \u0026quot;base\u0026quot; ## ## [[5]] ## [1] \u0026quot;yardstick\u0026quot; \u0026quot;workflows\u0026quot; \u0026quot;tune\u0026quot; \u0026quot;tibble\u0026quot; \u0026quot;rsample\u0026quot; ## [6] \u0026quot;tidyr\u0026quot; \u0026quot;recipes\u0026quot; \u0026quot;purrr\u0026quot; \u0026quot;parsnip\u0026quot; \u0026quot;infer\u0026quot; ## [11] \u0026quot;ggplot2\u0026quot; \u0026quot;dplyr\u0026quot; \u0026quot;dials\u0026quot; \u0026quot;scales\u0026quot; \u0026quot;broom\u0026quot; ## [16] \u0026quot;tidymodels\u0026quot; \u0026quot;stats\u0026quot; \u0026quot;graphics\u0026quot; \u0026quot;grDevices\u0026quot; \u0026quot;utils\u0026quot; ## [21] \u0026quot;datasets\u0026quot; \u0026quot;methods\u0026quot; \u0026quot;base\u0026quot;  tuning by parallelization Now we‚Äôre going to create our tuning object, which will take our recipe, our model, our resamples, and our metrics, to fit our 25 models over 10 resamples and compute our performance metrics, then we‚Äôll stop our parallelization.\ndata_tune \u0026lt;- tune_grid( data_rec, model = data_mixture, resamples = cv_splits, grid = regular_grid, metrics = perf_metrics ) stopCluster(cl) # Naive Lasso performance collect_metrics(cv_eval) ## # A tibble: 3 x 5 ## .metric .estimator mean n std_err ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ccc standard 0.915 10 0.00660 ## 2 rmse standard 0.0673 10 0.00345 ## 3 rsq standard 0.854 10 0.0122  show best # Best tuned models show_best(data_tune, \u0026quot;ccc\u0026quot;) ## # A tibble: 5 x 7 ## penalty mixture .metric .estimator mean n std_err ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.0000000001 0.25 ccc standard 0.917 10 0.00660 ## 2 0.0000000316 0.25 ccc standard 0.917 10 0.00660 ## 3 0.00001 0.25 ccc standard 0.917 10 0.00660 ## 4 0.0000000001 0.75 ccc standard 0.917 10 0.00661 ## 5 0.0000000316 0.75 ccc standard 0.917 10 0.00661 show_best(data_tune, \u0026quot;rmse\u0026quot;, maximize = FALSE) ## # A tibble: 5 x 7 ## penalty mixture .metric .estimator mean n std_err ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.00316 0.25 rmse standard 0.0672 10 0.00334 ## 2 0.0000000001 0 rmse standard 0.0672 10 0.00302 ## 3 0.0000000316 0 rmse standard 0.0672 10 0.00302 ## 4 0.00001 0 rmse standard 0.0672 10 0.00302 ## 5 0.00316 0 rmse standard 0.0672 10 0.00302 show_best(data_tune, \u0026quot;rsq\u0026quot;) ## # A tibble: 5 x 7 ## penalty mixture .metric .estimator mean n std_err ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.00316 0.25 rsq standard 0.855 10 0.0116 ## 2 0.0000000001 0 rsq standard 0.854 10 0.00997 ## 3 0.0000000316 0 rsq standard 0.854 10 0.00997 ## 4 0.00001 0 rsq standard 0.854 10 0.00997 ## 5 0.00316 0 rsq standard 0.854 10 0.00997  vis collect_metrics(data_tune) %\u0026gt;% filter(.metric == \u0026quot;rmse\u0026quot;) %\u0026gt;% mutate(mixture = format(mixture)) %\u0026gt;% ggplot(aes(x = penalty, y = mean, col = mixture)) + geom_line() + geom_point() + scale_x_log10() + geom_vline(xintercept = 0.001, color = \u0026quot;purple\u0026quot;, lty = \u0026quot;dotted\u0026quot;)  select best model best_mixture \u0026lt;- select_best(data_tune, metric = \u0026quot;rmse\u0026quot;, maximize = FALSE) best_mixture ## # A tibble: 1 x 2 ## penalty mixture ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.00316 0.25 data_mixture_final \u0026lt;- data_mixture_wfl %\u0026gt;% finalize_workflow(best_mixture) %\u0026gt;% fit(data = data_train)  how our model did again our test set data_mixture_final %\u0026gt;% predict(data_test) %\u0026gt;% bind_cols(select(data_test, Sale_Price)) %\u0026gt;% mutate(Sale_Price = log10(Sale_Price)) %\u0026gt;% perf_metrics(truth = Sale_Price, estimate = .pred) ## # A tibble: 3 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 rmse standard 0.0885 ## 2 rsq standard 0.768 ## 3 ccc standard 0.867   What variables turned out to be the most important in predicting sale price? tidy_coefs \u0026lt;- data_mixture_final$fit$fit$fit %\u0026gt;% broom::tidy() %\u0026gt;% filter(term != \u0026quot;(Intercept)\u0026quot;) %\u0026gt;% select(-step, -dev.ratio) delta \u0026lt;- abs(tidy_coefs$lambda - best_mixture$penalty) lambda_opt \u0026lt;- tidy_coefs$lambda[which.min(delta)] label_coefs \u0026lt;- tidy_coefs %\u0026gt;% mutate(abs_estimate = abs(estimate)) %\u0026gt;% filter(abs_estimate \u0026gt;= 0.01) %\u0026gt;% distinct(term) %\u0026gt;% inner_join(tidy_coefs, by = \u0026quot;term\u0026quot;) %\u0026gt;% filter(lambda == lambda_opt) label_coefs ## # A tibble: 16 x 3 ## term estimate lambda ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 PC1 -0.0340 0.00306 ## 2 Garage_Cars 0.0132 0.00306 ## 3 Year_Built 0.00666 0.00306 ## 4 Year_Remod_Add 0.0247 0.00306 ## 5 Fireplaces 0.0117 0.00306 ## 6 TotRms_AbvGrd 0.0128 0.00306 ## 7 BsmtFin_Type_1_GLQ 0.00617 0.00306 ## 8 Full_Bath 0.0119 0.00306 ## 9 Central_Air_Y 0.0114 0.00306 ## 10 MS_Zoning_Residential_Low_Density 0.0138 0.00306 ## 11 Functional_Typ 0.0105 0.00306 ## 12 Neighborhood_Edwards -0.0102 0.00306 ## 13 Sale_Type_New 0.00905 0.00306 ## 14 Mas_Vnr_Type_None 0.00765 0.00306 ## 15 Neighborhood_Somerset 0.00908 0.00306 ## 16 Mas_Vnr_Type_Stone 0.00171 0.00306 tidy_coefs %\u0026gt;% ggplot(aes(x = lambda, y = estimate, group = term, col = term, label = term)) + geom_vline(xintercept = lambda_opt, lty = 3) + geom_line(alpha = .4) + theme(legend.position = \u0026quot;none\u0026quot;) + scale_x_log10() + ggrepel::geom_text_repel(data = label_coefs) The above shows the coefficient estimates plotted against lambda, the dotted line indicating the optimal lambda that we selected during our tuning. Nice to see that one of our principal components ended up being important!\n links  Introduction to the {tidymodels} Machine Learning Ecosystem | David Nield   ","date":1585612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585632294,"objectID":"54af6cccb78cc1f8435899119d5f1417","permalink":"/post/tidymodels-machine-learning-ecosystem/","publishdate":"2020-03-31T00:00:00Z","relpermalink":"/post/tidymodels-machine-learning-ecosystem/","section":"post","summary":"Introduction  {tidyverse} ecosystem that has become the defacto standard for data importation, manipulation, and visualization in R.\n  My graduate training left me with a deep understanding of linear models and design-based causal inference, but with little or no training in other types of predictive modeling, unsupervised machine learning, version control, or putting models into production.","tags":["Machine learning"],"title":"Tidymodels Machine Learning Ecosystem","type":"post"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\n1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a shortcode for asides, also referred to as notices, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /alert %}}  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.   Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R  renders as\n  Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it üôå ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Jixing Liu"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Jixing Liu"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab  Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata ( front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Jixing Liu"],"categories":["Demo"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\n Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n üëâ Get Started üìö View the documentation üí¨ Ask a question on the forum üë• Chat with the community üê¶ Twitter: @source_themes @GeorgeCushen #MadeWithAcademic üí° Request a feature or report a bug ‚¨ÜÔ∏è Updating? View the Update Guide and Release Notes ‚ù§ Support development of Academic:  ‚òïÔ∏è Donate a coffee üíµ Become a backer on Patreon üñºÔ∏è Decorate your laptop or journal with an Academic sticker üëï Wear the T-shirt üë©‚Äçüíª Contribute      Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\n Choose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem   Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site  Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n  one-click install using your web browser (recommended)  install on your computer using Git with the Command Prompt/Terminal app  install on your computer by downloading the ZIP files  install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating  View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic"],"title":"Academic: the website builder for Hugo","type":"post"},{"authors":["Jixing Liu","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Jixing Liu","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]