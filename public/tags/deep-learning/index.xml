<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Jixing Liu</title>
    <link>/tags/deep-learning/</link>
      <atom:link href="/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 18 Apr 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Deep Learning</title>
      <link>/tags/deep-learning/</link>
    </image>
    
    <item>
      <title>Deep Learning With Keras in R</title>
      <link>/post/deep-learning-with-keras-in-r/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/deep-learning-with-keras-in-r/</guid>
      <description>


&lt;div id=&#34;load-libraries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Load Libraries&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;import-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import Data&lt;/h2&gt;
&lt;p&gt;Download the&lt;a href=&#34;https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/&#34;&gt;IBM Watson Telco Data Set here&lt;/a&gt;. Next, use&lt;code&gt;read_csv()&lt;/code&gt;to import the data into a nice tidy data frame. We use the&lt;code&gt;glimpse()&lt;/code&gt;function to quickly inspect the data. We have the target “Churn” and all other variables are potential predictors. The raw data set needs to be cleaned and preprocessed for ML.&lt;/p&gt;
&lt;p&gt;Use sapply to check the number if missing values in each columns&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preprocess-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;📌 Preprocess Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;We’ll go through a few steps to preprocess the data for ML. First, we “prune” the data, which is nothing more than removing unnecessary columns and rows. Then we split into training and testing sets. After that we explore the training set to uncover transformations that will be needed for deep learning. We save the best for last. We end by preprocessing the data with the new &lt;a href=&#34;https://topepo.github.io/recipes&#34;&gt;recipes&lt;/a&gt; package.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;prune: 去掉不要的行和列&lt;/li&gt;
&lt;li&gt;split: 切分数据&lt;/li&gt;
&lt;li&gt;EDA explore the data&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;prune-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prune The Data&lt;/h3&gt;
&lt;p&gt;The data has a few columns and rows we’d like to remove:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The “customerID” column is a unique identifier for each observation that isn’t needed for modeling. We can de-select this column.&lt;/li&gt;
&lt;li&gt;The data has 11 &lt;code&gt;NA&lt;/code&gt; values all in the “TotalCharges” column. Because it’s such a small percentage of the total population (99.8% complete cases), we can drop these observations with the &lt;code&gt;drop_na()&lt;/code&gt; function from &lt;a href=&#34;http://tidyr.tidyverse.org/&#34;&gt;tidyr&lt;/a&gt;. Note that these may be customers that have not yet been charged, and therefore an alternative is to replace with zero or -99 to segregate this population from the rest.&lt;/li&gt;
&lt;li&gt;My preference is to have the target in the first column so we’ll include a final select() ooperation to do so.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We’ll perform the cleaning operation with one tidyverse pipe (%&amp;gt;%) chain.&lt;/p&gt;
&lt;p&gt;Bar plots of categorical variables&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;split-into-traintest-sets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Split Into Train/Test Sets&lt;/h3&gt;
&lt;p&gt;We have a new package, &lt;a href=&#34;https://topepo.github.io/rsample/&#34;&gt;rsample&lt;/a&gt;, which is very useful for sampling methods. It has the &lt;code&gt;initial_split()&lt;/code&gt; function for splitting data sets into training and testing sets. The return is a special &lt;code&gt;rsplit&lt;/code&gt; object.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploration-what-transformation-steps-are-needed-for-ml&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploration: What Transformation Steps Are Needed For ML?&lt;/h3&gt;
&lt;p&gt;This phase of the analysis is often called exploratory analysis, but basically &lt;strong&gt;we are trying to answer the question, “What steps are needed to prepare for ML?” The key concept is knowing what transformations are needed to run the algorithm most effectively&lt;/strong&gt;. &lt;font color=&#34;darkred&#34;&gt; &lt;strong&gt;Artificial Neural Networks are best when the data is one-hot encoded, scaled and centered&lt;/strong&gt;&lt;/font&gt; . In addition, other transformations may be beneficial as well to make relationships easier for the algorithm to identify. A full exploratory analysis is not practical in this article. With that said we’ll cover a few tips on transformations that can help as they relate to this dataset. In the next section, we will implement the preprocessing techniques.&lt;/p&gt;
&lt;p&gt;回答一个问题: ML 需要什么样的数据? 数据需要做什么样的转换.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;font color=&#34;darkred&#34;&gt;one-hot, scaled and centered&lt;/font&gt; 比较适合神经网络&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;discretize-the-tenure-feature&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Discretize The “tenure” Feature&lt;/h4&gt;
&lt;p&gt;Numeric features like age, years worked, length of time in a position can generalize a group (or &lt;font color=&#34;darkred&#34;&gt; &lt;strong&gt;cohort&lt;/strong&gt;&lt;/font&gt; ). We see this in marketing a lot (think “millennials”, which identifies a group born in a certain timeframe). The “tenure” feature falls into this category of numeric features that can be discretized into groups.&lt;/p&gt;
&lt;p&gt;连续变量的离散化&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transform-the-totalcharges-feature&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Transform The “totalcharges” Feature&lt;/h4&gt;
&lt;p&gt;log transform&lt;/p&gt;
&lt;p&gt;What we don’t like to see is when a lot of observations are bunched within a small part of the range.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-11-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We can use a log transformation to even out the data into more of a normal distribution.&lt;/strong&gt; It’s not perfect, but it’s quick and easy to get our data spread out a bit more.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-12-1.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;数据转换改变数据之间的相关性&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;数据转换改变数据之间的相关性&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Pro Tip: A quick test is to see if the log transformation increases the magnitude of the correlation&lt;/strong&gt; between “TotalCharges” and “Churn”. We’ll use a few &lt;a href=&#34;http://dplyr.tidyverse.org/&#34;&gt;dplyr&lt;/a&gt; operations along with the &lt;a href=&#34;https://github.com/drsimonj/corrr&#34;&gt;corrr&lt;/a&gt; package to perform a quick correlation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;correlate()&lt;/code&gt;: Performs tidy correlations on numeric data&lt;/li&gt;
&lt;li&gt;&lt;code&gt;focus()&lt;/code&gt;: Similar to &lt;code&gt;select()&lt;/code&gt;. Takes columns and focuses on only the rows/columns of importance.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fashion()&lt;/code&gt;: Makes the formatting aesthetically easier to read.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The correlation between “Churn” and “LogTotalCharges” is greatest in magnitude indicating the &lt;strong&gt;log transformation should improve the accuracy of the ANN model&lt;/strong&gt; we build. Therefore, we should perform the log transformation.&lt;/p&gt;
&lt;p&gt;x 和 y 之间的相关性, 经过转换后增强了, 这样有利于模型的训练&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-hot-encoding&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;One-Hot Encoding&lt;/h4&gt;
&lt;p&gt;One-hot encoding is the process of &lt;strong&gt;converting categorical data to sparse data&lt;/strong&gt;, which has columns of only zeros and ones (this is also called creating “dummy variables” or a “design matrix”). &lt;strong&gt;All non-numeric data will need to be converted to dummy variables&lt;/strong&gt;. This is simple for binary Yes/No data because we can simply convert to 1’s and 0’s. It becomes slightly more complicated with multiple categories, which requires creating new columns of 1’s and 0`s for each category (actually one less). &lt;strong&gt;We have four features that are multi-category: Contract, Internet Service, Multiple Lines, and Payment Method.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-14-1.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-scaling&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Feature Scaling&lt;/h4&gt;
&lt;p&gt;ANN’s typically perform faster and often times with higher accuracy when the features are scaled and/or normalized (aka centered and scaled, also known as &lt;strong&gt;standardizing&lt;/strong&gt;). &lt;strong&gt;Because ANNs use gradient descent, weights tend to update faster&lt;/strong&gt;. According to &lt;a href=&#34;https://sebastianraschka.com/&#34;&gt;&lt;em&gt;Sebastian Raschka&lt;/em&gt;&lt;/a&gt;, an expert in the field of Deep Learning, several examples when feature scaling is important are:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally&lt;/li&gt;
&lt;li&gt;k-means (see k-nearest neighbors)&lt;/li&gt;
&lt;li&gt;logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others&lt;/li&gt;
&lt;li&gt;linear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); &lt;strong&gt;you want to have features on the same scale since you’d emphasize variables on “larger measurement scales” more&lt;/strong&gt;. There are many more cases than I can possibly list here … I always recommend you to think about the algorithm and what it’s doing, and then it typically becomes obvious whether we want to scale your features or not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The interested reader can read &lt;a href=&#34;http://sebastianraschka.com/Articles/2014_about_feature_scaling.html&#34;&gt;Sebastian Raschka’s article&lt;/a&gt; for a full discussion on the scaling/normalization topic. &lt;font color=&#34;darkred&#34;&gt; &lt;strong&gt;Pro Tip: When in doubt, standardize the data&lt;/strong&gt;&lt;/font&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;preprocessing-with-recipes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preprocessing With Recipes&lt;/h3&gt;
&lt;p&gt;Let’s implement the preprocessing steps/transformations uncovered during our exploration. Max Kuhn (creator of &lt;a href=&#34;http://topepo.github.io/caret/index.html&#34;&gt;caret&lt;/a&gt;) has been putting some work into &lt;em&gt;Rlang ML tools&lt;/em&gt; lately, and the payoff is beginning to take shape. &lt;strong&gt;A new package, &lt;a href=&#34;https://topepo.github.io/recipes&#34;&gt;recipes&lt;/a&gt;, makes creating ML data preprocessing workflows a breeze&lt;/strong&gt;! It takes a little getting used to, but I’ve found that it really helps manage the preprocessing steps. We’ll go over the nitty gritty as it applies to this problem.&lt;/p&gt;
&lt;div id=&#34;step-1-create-a-recipe&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 1: Create A Recipe&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;A “recipe” is nothing more than a series of steps you would like to perform on the training, testing and/or validation sets&lt;/strong&gt;. Think of preprocessing data like baking a cake (I’m not a baker but stay with me). &lt;strong&gt;The recipe is our steps to make the cake&lt;/strong&gt;. It doesn’t do anything other than create the playbook for baking.&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;recipe()&lt;/code&gt; function to implement our preprocessing steps. The function takes a familiar &lt;code&gt;object&lt;/code&gt; argument, which is a modeling function such as &lt;code&gt;object = Churn ~ .&lt;/code&gt; meaning “Churn” is the outcome (aka response, predictor, target) and all other features are predictors. The function also takes the &lt;code&gt;data&lt;/code&gt; argument, which gives the “recipe steps” perspective on how to apply during baking (next).&lt;/p&gt;
&lt;p&gt;A recipe is not very useful until we add “steps”, which are used to transform the data during baking. The package contains a number of useful “step functions” that can be applied. The entire list of &lt;a href=&#34;https://topepo.github.io/recipes/reference/index.html&#34;&gt;Step Functions&lt;/a&gt; can be viewed here. For our model, we use:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;step_discretize()&lt;/code&gt; with the &lt;code&gt;option = list(cuts = 6)&lt;/code&gt; to cut the continuous variable for “tenure” (number of years as a customer) to group customers into cohorts. 数据离散化&lt;/li&gt;
&lt;li&gt;&lt;code&gt;step_log()&lt;/code&gt; to log transform “TotalCharges”.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;step_dummy()&lt;/code&gt; to one-hot encode the categorical data. Note that this adds columns of one/zero for categorical data with three or more categories.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;step_center()&lt;/code&gt; to mean-center the data. 所有数据减去平均值&lt;/li&gt;
&lt;li&gt;&lt;code&gt;step_scale()&lt;/code&gt; to scale the data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The last step is to prepare the recipe with the &lt;code&gt;prep()&lt;/code&gt; function. This step is used to “estimate the required parameters from a training set that can later be applied to other data sets”. &lt;strong&gt;This is important for centering and scaling and other functions that use parameters defined from the training set&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;prep()&lt;/code&gt;, 这一步非常重要, 可以评估需要的参数保存下来用于之后的 future raw data, 这对于数据标准化这一步是非常重要的.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Here’s how simple it is to implement the preprocessing steps that we went over!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can print the recipe object if we ever forget what steps were used to prepare the data. &lt;strong&gt;Pro Tip: We can save the recipe object as an RDS file using &lt;code&gt;saveRDS()&lt;/code&gt;, and then use it to &lt;code&gt;bake()&lt;/code&gt; (discussed next) future raw data into ML-ready data in production!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们可以保存下来这个 recipes 用于之后的数据准备, 这对于使用模型预测数据是非常重要的.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-baking-with-your-recipe&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 2: Baking With Your Recipe&lt;/h4&gt;
&lt;p&gt;Now for the fun part! We can apply the “recipe” to any data set with the &lt;code&gt;bake()&lt;/code&gt; function, and it processes the data following our recipe steps. We’ll apply to our training and testing data to convert from raw data to a machine learning dataset. Check our training set out with &lt;code&gt;glimpse()&lt;/code&gt;. &lt;strong&gt;Now that’s an ML-ready dataset prepared for ANN modeling!!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=&#34;darkred&#34;&gt;raw dataset to machine learning dataset&lt;/font&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-dont-forget-the-target&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Step 3: Don’t Forget The Target&lt;/h4&gt;
&lt;p&gt;One last step, we need to store the actual values (truth) as &lt;code&gt;y_train_vec&lt;/code&gt; and &lt;code&gt;y_test_vec&lt;/code&gt;, which are needed for modeling our ANN. We convert to a series of numeric ones and zeros which can be accepted by the Keras ANN modeling functions. We add “vec” to the name so we can easily remember the class of the object (it’s easy to get confused when working with tibbles, vectors, and matrix data types).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;keras-model-customer-churn-with-keras-deep-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;📌 Keras: Model Customer Churn With Keras (Deep Learning)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;This is super exciting!! Finally, Deep Learning with Keras in R!&lt;/strong&gt; The team at RStudio has done fantastic work recently to create the &lt;a href=&#34;https://tensorflow.rstudio.com/keras/&#34;&gt;keras&lt;/a&gt; package, which implements &lt;a href=&#34;https://keras.io/&#34;&gt;Keras&lt;/a&gt; in R. Very cool!&lt;/p&gt;
&lt;div id=&#34;background-on-artifical-neural-networks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Background On Artifical Neural Networks&lt;/h3&gt;
&lt;p&gt;For those unfamiliar with Neural Networks (and those that need a refresher), &lt;a href=&#34;https://www.xenonstack.com/blog/overview-of-artificial-neural-networks-and-its-applications&#34;&gt;read this article&lt;/a&gt;. It’s very comprehensive, and you’ll leave with a general understanding of the types of deep learning and how they work.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/Artificial-Neural-Network-Architecture.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Source: &lt;a href=&#34;https://www.xenonstack.com/blog/overview-of-artificial-neural-networks-and-its-applications&#34;&gt;Xenon Stack&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Deep Learning has been available in R for some time, but the primary packages used in the wild have not (this includes Keras, Tensor Flow, Theano, etc, which are all Python libraries). It’s worth mentioning that a number of other Deep Learning packages exist in R including &lt;code&gt;h2o&lt;/code&gt;, &lt;code&gt;mxnet&lt;/code&gt;, and others. The interested reader can check out &lt;a href=&#34;http://www.rblog.uni-freiburg.de/2017/02/07/deep-learning-in-r/&#34;&gt;this blog post for a comparison of deep learning packages in R&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-deep-learning-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Building A Deep Learning Model&lt;/h3&gt;
&lt;p&gt;We’re going to build a special class of ANN called a &lt;a href=&#34;https://en.wikipedia.org/wiki/Multilayer_perceptron&#34;&gt;Multi-Layer Perceptron (MLP)&lt;/a&gt;. MLPs are one of the simplest forms of deep learning, but they are both highly accurate and serve as a jumping-off point for more complex algorithms. MLPs are quite versatile as they can be used for regression, binary and multi classification (and are typically quite good at classification problems).&lt;/p&gt;
&lt;p&gt;We’ll build a three layer MLP with Keras. Let’s walk-through the steps before we implement in R.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;font color=&#34;darkred&#34;&gt;&lt;strong&gt;Initialize a sequential model&lt;/strong&gt;&lt;/font&gt; : The first step is to initialize a sequential model with &lt;code&gt;keras_model_sequential()&lt;/code&gt;, which is the beginning of our Keras model. The sequential model is composed of a linear stack of layers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Apply layers to the sequential model&lt;/strong&gt;: Layers consist of the input layer, hidden layers and an output layer. The &lt;strong&gt;input layer is the data and provided it’s formatted correctly there’s nothing more to discuss&lt;/strong&gt;. The hidden layers and output layers are what controls the ANN inner workings.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hidden Layers&lt;/strong&gt;: Hidden layers form the neural network nodes that enable non-linear activation using weights. The hidden layers are created using &lt;code&gt;layer_dense()&lt;/code&gt;. We’ll add two hidden layers. We’ll apply &lt;code&gt;units = 16&lt;/code&gt;, which is the &lt;strong&gt;number of nodes&lt;/strong&gt;. We’ll select &lt;code&gt;kernel_initializer = &#34;uniform&#34;&lt;/code&gt; and &lt;code&gt;activation = &#34;relu&#34;&lt;/code&gt; for both layers. The first layer needs to have the &lt;code&gt;input_shape = 35&lt;/code&gt;, which is the number of columns in the training set (the number of feature) . &lt;strong&gt;Key Point: While we are arbitrarily selecting the number of hidden layers, units, kernel initializers and activation functions, these parameters can be optimized through a process called hyperparameter tuning that is discussed in &lt;a href=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/#next-steps&#34;&gt;Next Steps&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;关键点：尽管我们可以任意选择隐藏层，单元，内核初始化程序和激活函数的数量，但是可以通过称为“超参数调整”的过程来优化这些参数，该过程将在[后续步骤]中进行讨论。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Dropout Layers&lt;/strong&gt;: Dropout layers are used to &lt;strong&gt;control overfitting&lt;/strong&gt;. This eliminates weights below a cutoff threshold to prevent low weights from overfitting the layers. We use the &lt;code&gt;layer_dropout()&lt;/code&gt; function add two drop out layers with &lt;code&gt;rate = 0.10&lt;/code&gt; to remove weights below 10%.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Output Layer&lt;/strong&gt;: The output layer specifies the shape of the output and the method of assimilating the learned information. The output layer is applied using the &lt;code&gt;layer_dense()&lt;/code&gt;. For binary values, the shape should be &lt;code&gt;units = 1&lt;/code&gt;. For multi-classification, the &lt;code&gt;units&lt;/code&gt; should correspond to the number of classes. We set the &lt;code&gt;kernel_initializer = &#34;uniform&#34;&lt;/code&gt; and the &lt;code&gt;activation = &#34;sigmoid&#34;&lt;/code&gt; (common for binary classification).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Compile the model&lt;/strong&gt;: The last step is to compile the model with &lt;code&gt;compile()&lt;/code&gt;. We’ll use &lt;code&gt;optimizer = &#34;adam&#34;&lt;/code&gt;, which is one of the most popular optimization algorithms. We select &lt;code&gt;loss = &#34;binary_crossentropy&#34;&lt;/code&gt; since this is a binary classification problem. We’ll select &lt;code&gt;metrics = c(&#34;accuracy&#34;)&lt;/code&gt; to be evaluated during training and testing. &lt;strong&gt;Key Point: The optimizer is often included in the tuning process&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;build-model-object&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Build model object&lt;/h4&gt;
&lt;p&gt;Let’s codify the discussion above to build our Keras MLP-flavored ANN model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fit Model&lt;/h4&gt;
&lt;p&gt;We use the fit() function to run the ANN on our training data. The object is our model, and x and y are our training data in matrix and numeric vector forms, respectively. The &lt;code&gt;batch_size = 50&lt;/code&gt; sets the number samples per gradient update within each epoch. We set &lt;code&gt;epochs = 35&lt;/code&gt; to control the number training cycles. Typically we want to keep the batch size high since this decreases the error within each training cycle (epoch). We also want epochs to be large, which is important in visualizing the training history (discussed below). We set &lt;code&gt;validation_split = 0.30&lt;/code&gt; to include 30% of the data for model validation, which prevents overfitting. The training process should complete in 15 seconds or so.&lt;/p&gt;
&lt;p&gt;我们将&#34; validation_split = 0.30&#34;设置为包括30％的数据用于模型验证，这可以防止过拟合。
验证集弄大一点可以防止模型过拟合??&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vis-history&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;vis history&lt;/h4&gt;
&lt;p&gt;We can inspect the training history. We want to make sure there is minimal difference between the validation accuracy and the training accuracy.&lt;/p&gt;
&lt;p&gt;We can visualize the Keras training history using the &lt;code&gt;plot()&lt;/code&gt; function. What we want to see is the validation accuracy and loss leveling off, which means the model has completed training. We see that there is some divergence between training loss/accuracy and validation loss/accuracy. This model indicates we can possibly stop training at an earlier epoch. &lt;font color=&#34;darkred&#34;&gt;&lt;strong&gt;Pro Tip: Only use enough epochs to get a high validation accuracy. Once validation accuracy curve begins to flatten or decrease, it’s time to stop training.&lt;/strong&gt;&lt;/font&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;专家提示：请仅使用足够的 epochs 以获得较高的验证准确性。验证准确性曲线开始变平或减小后，就该停止训练了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;making-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Making Predictions&lt;/h3&gt;
&lt;p&gt;We’ve got a good model based on the validation accuracy. Now let’s make some predictions from our &lt;a href=&#34;https://tensorflow.rstudio.com/keras/&#34;&gt;keras&lt;/a&gt; model on the test data set, which was unseen during modeling (we use this for the true performance assessment). We have two functions to generate predictions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;predict_classes()&lt;/code&gt;: Generates class values as a matrix of ones and zeros. Since we are dealing with binary classification, we’ll convert the output to a vector.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;predict_proba()&lt;/code&gt;: Generates the class probabilities as a numeric matrix indicating &lt;strong&gt;the probability of being a class&lt;/strong&gt;. Again, we convert to a numeric vector because there is only one column output.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;inspect-performance-with-yardstick&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inspect Performance With Yardstick&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;yardstick&lt;/code&gt; package has a collection of handy functions for measuring performance of machine learning models. We’ll overview some metrics we can use to understand the performance of our model.&lt;/p&gt;
&lt;p&gt;First, let’s get the data formatted for &lt;code&gt;yardstick&lt;/code&gt;. We create a data frame with the &lt;strong&gt;truth (actual values as factors)&lt;/strong&gt;, &lt;strong&gt;estimate (predicted values as factors)&lt;/strong&gt;, and &lt;strong&gt;the class probability (probability of yes as numeric)&lt;/strong&gt;. We use the &lt;code&gt;fct_recode()&lt;/code&gt; function from the &lt;a href=&#34;http://forcats.tidyverse.org/&#34;&gt;forcats&lt;/a&gt; package to assist with recoding as Yes/No values.&lt;/p&gt;
&lt;p&gt;Now that we have the data formatted, we can take advantage of the &lt;code&gt;yardstick&lt;/code&gt; package. The only other thing we need to do is to set &lt;code&gt;options(yardstick.event_first = FALSE)&lt;/code&gt;. As pointed out by &lt;a href=&#34;https://github.com/ad1729&#34;&gt;ad1729&lt;/a&gt; in &lt;a href=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/options(yardstick.event_first%20=%20FALSE)&#34;&gt;GitHub Issue 13&lt;/a&gt;, the default is to classify 0 as the positive class instead of 1.&lt;/p&gt;
&lt;p&gt;这个问题在最新版的 yardstick 已经得到解决了&lt;/p&gt;
&lt;div id=&#34;confusion-table&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Confusion Table&lt;/h4&gt;
&lt;p&gt;We can use the &lt;code&gt;conf_mat()&lt;/code&gt; function to get the confusion table. We see that the model was by no means perfect, but it did a decent job of identifying customers likely to churn.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accuracy&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Accuracy&lt;/h4&gt;
&lt;p&gt;We can use the &lt;code&gt;metrics()&lt;/code&gt; function to get an accuracy measurement from the test set. We are getting roughly 82% accuracy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;auc&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;AUC&lt;/h4&gt;
&lt;p&gt;We can also get the ROC Area Under the Curve (AUC) measurement. AUC is often a good metric used to compare different classifiers and to compare to randomly guessing (AUC_random = 0.50). Our model has AUC = 0.85, which is much better than randomly guessing. Tuning and testing different classification algorithms may yield even better results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;precision-and-recall&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Precision And Recall&lt;/h4&gt;
&lt;p&gt;Precision is when the model predicts “yes”, how often is it actually “yes”. Recall (also true positive rate or specificity) is when the actual value is “yes” how often is the model correct. We can get &lt;code&gt;precision()&lt;/code&gt; and &lt;code&gt;recall()&lt;/code&gt; measurements using &lt;code&gt;yardstick&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Precision and recall are very important to the business case: The organization is concerned with &lt;strong&gt;balancing the cost of targeting and retaining customers at risk of leaving with the cost of inadvertently targeting customers that are not planning to leave&lt;/strong&gt; (and potentially decreasing revenue from this group). The threshold above which to predict Churn = “Yes” can be adjusted to optimize for the business problem. This becomes an &lt;strong&gt;Customer Lifetime Value optimization problem&lt;/strong&gt; that is discussed further in &lt;a href=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/#next-steps&#34;&gt;Next Steps&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;f1-score&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;F1 Score&lt;/h4&gt;
&lt;p&gt;We can also get the F1-score, which is a &lt;strong&gt;weighted average between the precision and recall.&lt;/strong&gt; Machine learning classifier thresholds are often adjusted to maximize the F1-score. However, this is often not the optimal solution to the business problem.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;explain-the-model-with-lime&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;📌 Explain The Model With LIME&lt;/h2&gt;
&lt;p&gt;LIME stands for &lt;em&gt;Local Interpretable Model-agnostic Explanations&lt;/em&gt;, and is a method for explaining black-box machine learning model classifiers. For those new to LIME, this YouTube video does a really nice job explaining how LIME helps to identify feature importance with black box machine learning models (e.g. deep learning, stacked ensembles, random forest).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://i.loli.net/2020/04/18/5QsLvq1PdHUthGC.png&#34; alt=&#34;why should I trust you&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;why should I trust you&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hUnRCxnydCc&amp;amp;feature=emb_logo&#34;&gt;(28) KDD2016 paper 573 - YouTube&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Setup&lt;/h4&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/thomasp85/lime&#34;&gt;lime&lt;/a&gt; package implements &lt;a href=&#34;https://github.com/marcotcr/lime&#34;&gt;LIME&lt;/a&gt; in R. One thing to note is that it’s not setup out-of-the-box to work with &lt;code&gt;keras&lt;/code&gt;. The good news is with a few functions we can get everything working properly. We’ll need to make two custom functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;model_type&lt;/code&gt;: Used to tell &lt;code&gt;lime&lt;/code&gt; &lt;strong&gt;what type of model we are dealing with&lt;/strong&gt;. It could be classification, regression, survival, etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;predict_model&lt;/code&gt;: Used to allow &lt;code&gt;lime&lt;/code&gt; to perform predictions that its algorithm can interpret.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;model_type-fucntion-define&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;model_type fucntion define&lt;/h5&gt;
&lt;p&gt;The first thing we need to do is &lt;strong&gt;identify the class of our model object&lt;/strong&gt;. We do this with the &lt;code&gt;class()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Next we create our &lt;code&gt;model_type()&lt;/code&gt; function. It’s only input is x the keras model. &lt;strong&gt;The function simply returns “classification”, which tells LIME we are classifying.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predict_model-function-define&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;predict_model function define&lt;/h5&gt;
&lt;p&gt;Now we can create our &lt;code&gt;predict_model()&lt;/code&gt; function, which wraps &lt;code&gt;keras::predict_proba()&lt;/code&gt;. &lt;strong&gt;The trick here is to realize that it’s inputs must be x a model, newdata a dataframe object (this is important)&lt;/strong&gt;, and type which is not used but can be use to switch the output type. The output is also a little tricky because it must be in the format of probabilities by classification (this is important; shown next).&lt;/p&gt;
&lt;p&gt;Run this next script to show you what the output looks like and to test our predict_model() function. See how it’s the probabilities by classification. It must be in this form for &lt;code&gt;model_type = &#34;classification&#34;&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predict_model-function&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;predict_model() function&lt;/h5&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;enjoy-lime&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Enjoy LIME&lt;/h4&gt;
&lt;p&gt;Now the fun part, we create an explainer using the &lt;code&gt;lime()&lt;/code&gt; function. Just pass the training data set without the “Attribution column”. The form must be a data frame, which is OK since our &lt;code&gt;predict_model&lt;/code&gt; function will switch it to an keras object. Set &lt;code&gt;model = automl_leader&lt;/code&gt; our leader model, and &lt;code&gt;bin_continuous = FALSE&lt;/code&gt;. We could tell the algorithm to bin continuous variables, but this may not make sense for categorical numeric data that we didn’t change to factors.&lt;/p&gt;
&lt;p&gt;Now we run the &lt;code&gt;explain()&lt;/code&gt; function, which returns our explanation. This can take a minute to run so we limit it to just the first ten rows of the test data set.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We set &lt;code&gt;n_labels = 1&lt;/code&gt; because we care about explaining a single class.&lt;/li&gt;
&lt;li&gt;Setting &lt;code&gt;n_features = 4&lt;/code&gt; &lt;strong&gt;returns the top four features that are critical to each case&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Finally, setting &lt;code&gt;kernel_width = 0.5&lt;/code&gt; allows us to increase the “model_r2” value by shrinking the localized evaluation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-importance-visualization&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Feature Importance Visualization&lt;/h4&gt;
&lt;p&gt;The payoff for the work we put in using LIME is this &lt;strong&gt;feature importance plot&lt;/strong&gt;. This allows us to visualize each of the first ten cases (observations) from the test data. &lt;strong&gt;The top four features for each case are shown&lt;/strong&gt;. Note that &lt;strong&gt;they are not the same for each case&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The green bars mean that the feature &lt;strong&gt;supports the model conclusion&lt;/strong&gt;, and the red bars contradict.&lt;/p&gt;
&lt;p&gt;A few important features based on frequency in first ten cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tenure (7 cases)&lt;/li&gt;
&lt;li&gt;Senior Citizen (5 cases)&lt;/li&gt;
&lt;li&gt;Online Security (4 cases)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another excellent visualization can be performed using &lt;code&gt;plot_explanations()&lt;/code&gt;, which produces a facetted heatmap of all case/label/feature combinations. It’s a more condensed version of plot_features(), but we need to be careful because it does not provide exact statistics and it makes it less easy to investigate binned features (Notice that “tenure” would not be identified as a contributor even though it shows up as a top feature in 7 of 10 cases).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;check-explanations-with-correlation-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;📌 Check Explanations With Correlation Analysis&lt;/h2&gt;
&lt;p&gt;One thing we need to be careful with the LIME visualization is that we are only doing a sample of the data, in our case the first 10 test observations. Therefore, we are &lt;strong&gt;gaining a very localized understanding of how the ANN works&lt;/strong&gt;. However, we also want to know on from a global perspective what drives feature importance.&lt;/p&gt;
&lt;p&gt;We can perform a &lt;strong&gt;correlation analysis&lt;/strong&gt; on the training set as well to help glean what features correlate globally to “Churn”. We’ll use the &lt;code&gt;corrr&lt;/code&gt; package, which performs tidy correlations with the function &lt;code&gt;correlate()&lt;/code&gt;. We can get the correlations as follows.&lt;/p&gt;
&lt;p&gt;The correlation visualization helps in distinguishing which features are relavant to Churn.&lt;/p&gt;
&lt;p&gt;The correlation analysis helps us quickly disseminate which features that the LIME analysis may be excluding. We can see that the following features are highly correlated (magnitude &amp;gt; 0.25):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Increases Likelihood of Churn (Red): - Tenure = Bin 1 (&amp;lt;12 Months) - Internet Service = “Fiber Optic” - Payment Method = “Electronic Check”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Decreases Likelihood of Churn (Blue): - Contract = “Two Year” - Total Charges (Note that this may be a biproduct of additional services such as Online Security)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-investigation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;📌 Feature Investigation&lt;/h2&gt;
&lt;p&gt;We can investigate features that are &lt;strong&gt;most frequent&lt;/strong&gt; in the LIME feature importance visualization along with those that the &lt;strong&gt;correlation analysis shows an above normal magnitude&lt;/strong&gt;. We’ll investigate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tenure (7/10 LIME Cases, Highly Correlated)&lt;/li&gt;
&lt;li&gt;Contract (Highly Correlated)&lt;/li&gt;
&lt;li&gt;Internet Service (Highly Correlated)&lt;/li&gt;
&lt;li&gt;Payment Method (Highly Correlated)&lt;/li&gt;
&lt;li&gt;Senior Citizen (5/10 LIME Cases)&lt;/li&gt;
&lt;li&gt;Online Security (4/10 LIME Cases)&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;tenure-710-lime-cases-highly-correlated&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Tenure (7/10 LIME Cases, Highly Correlated)&lt;/h4&gt;
&lt;p&gt;LIME cases indicate that the ANN model is using this feature frequently and high correlation agrees that this is important. Investigating the feature distribution, it appears that customers with lower tenure (bin 1) are more likely to leave. &lt;strong&gt;Opportunity: Target customers with less than 12 month tenure.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-45-1.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;contract-highly-correlated&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Contract (Highly Correlated)&lt;/h4&gt;
&lt;p&gt;While LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with one and two year contracts are much less likely to churn. &lt;strong&gt;Opportunity: Offer promotion to switch to long term contracts.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-46-1.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;internet-service-highly-correlated&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Internet Service (Highly Correlated)&lt;/h4&gt;
&lt;p&gt;While LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with fiber optic service are more likely to churn while those with no internet service are less likely to churn. &lt;strong&gt;Improvement Area: Customers may be dissatisfied with fiber optic service.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-47-1.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;payment-method-highly-correlated&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Payment Method (Highly Correlated)&lt;/h4&gt;
&lt;p&gt;While LIME did not indicate this as a primary feature in the first 10 cases, the feature is clearly correlated with those electing to stay. Customers with electronic check are more likely to leave. &lt;strong&gt;Opportunity: Offer customers a promotion to switch to automatic payments&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-48-1.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;senior-citizen-510-lime-cases&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Senior Citizen (5/10 LIME Cases)&lt;/h4&gt;
&lt;p&gt;Senior citizen appeared in several of the LIME cases indicating it was important to the ANN for the 10 samples. However, it was not highly correlated to Churn, which may indicate that the ANN is using in an more sophisticated manner (e.g. as an interaction). It’s difficult to say that senior citizens are more likely to leave, but non-senior citizens appear less at risk of churning. &lt;strong&gt;Opportunity: Target users in the lower age demographic.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-49-1.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;online-security-410-lime-cases&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Online Security (4/10 LIME Cases)&lt;/h4&gt;
&lt;p&gt;Customers that did not sign up for online security were more likely to leave while customers with no internet service or online security were less likely to leave. &lt;strong&gt;Opportunity: Promote online security and other packages that increase retention rates.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/figure-html/unnamed-chunk-50-1.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps-business-science-universitydomain-knowledge&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;📌 Next Steps: Business Science University(Domain knowledge)&lt;/h2&gt;
&lt;p&gt;We’ve just scratched the surface with the solution to this problem, but unfortunately there’s only so much ground we can cover in an article. Here are a few next steps that I’m pleased to announce will be covered in a &lt;a href=&#34;https://university.business-science.io/&#34;&gt;&lt;strong&gt;Business Science University&lt;/strong&gt;&lt;/a&gt; course coming in 2018!&lt;/p&gt;
&lt;div id=&#34;customer-lifetime-value&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Customer Lifetime Value&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Your organization needs to see the financial benefit so always tie your analysis to sales, profitability or ROI.&lt;/strong&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Customer_lifetime_value&#34;&gt;Customer Lifetime Value (&lt;em&gt;CLV&lt;/em&gt;)&lt;/a&gt; is a methodology that ties the business profitability to the retention rate. While we did not implement the CLV methodology herein, a full customer churn analysis would tie the churn to an classification cutoff (threshold) optimization to maximize the CLV with the predictive ANN model.&lt;/p&gt;
&lt;p&gt;The simplified CLV model is:&lt;/p&gt;
&lt;p&gt;CLV=GC∗11+d−rCLV=GC∗11+d−r&lt;/p&gt;
&lt;p&gt;Where,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;GC&lt;/em&gt; is the gross contribution per customer&lt;/li&gt;
&lt;li&gt;&lt;em&gt;d&lt;/em&gt; is the annual discount rate&lt;/li&gt;
&lt;li&gt;&lt;em&gt;r&lt;/em&gt; is the retention rate&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ann-performance-evaluation-and-improvement&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ANN Performance Evaluation and Improvement&lt;/h3&gt;
&lt;p&gt;The ANN model we built is good, but it could be better. How we understand our model accuracy and improve on it is through the combination of two techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;K-Fold Cross-Fold Validation&lt;/strong&gt;: Used to obtain bounds for accuracy estimates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hyper Parameter Tuning&lt;/strong&gt;: Used to improve model performance by searching for the best parameters possible.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We need to implement &lt;em&gt;K-Fold Cross Validation&lt;/em&gt; and &lt;em&gt;Hyper Parameter Tuning&lt;/em&gt; if we want a best-in-class model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distributing-analytics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Distributing Analytics&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;It’s critical to communicate data science insights to decision makers in the organization&lt;/strong&gt;. Most decision makers in organizations are not data scientists, but these individuals make important decisions on a day-to-day basis. The Shiny application below includes a &lt;strong&gt;Customer Scorecard&lt;/strong&gt; to monitor customer health (risk of churn).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jjallaire.shinyapps.io/keras-customer-churn/&#34;&gt;&lt;img src=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/images/shiny-application.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;business-science-university&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Business Science University&lt;/h3&gt;
&lt;p&gt;You’re probably wondering why we are going into so much detail on next steps. We are happy to announce a new project for 2018: &lt;a href=&#34;https://university.business-science.io/&#34;&gt;&lt;strong&gt;Business Science University&lt;/strong&gt;&lt;/a&gt;, an online school dedicated to helping data science learners.&lt;/p&gt;
&lt;p&gt;Benefits to learners:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Build your own &lt;strong&gt;online GitHub portfolio&lt;/strong&gt; of data science projects to market your skills to future employers!&lt;/li&gt;
&lt;li&gt;Learn &lt;strong&gt;real-world applications&lt;/strong&gt; in People Analytics (HR), Customer Analytics, Marketing Analytics, Social Media Analytics, Text Mining and Natural Language Processing (NLP), Financial and Time Series Analytics, and more!&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;advanced machine learning techniques&lt;/strong&gt; for both high accuracy modeling and explaining features that have an effect on the outcome!&lt;/li&gt;
&lt;li&gt;Create &lt;strong&gt;ML-powered web-applications&lt;/strong&gt; that can be distributed throughout an organization, enabling non-data scientists to benefit from algorithms in a user-friendly way!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Enrollment is open&lt;/strong&gt; so please signup for special perks. Just go to &lt;a href=&#34;https://university.business-science.io/&#34;&gt;&lt;strong&gt;Business Science University&lt;/strong&gt;&lt;/a&gt; and select enroll.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Customer churn is a costly problem&lt;/strong&gt;. The good news is that &lt;strong&gt;machine learning can solve churn problems&lt;/strong&gt;, making the organization more profitable in the process. In this article, we saw how &lt;strong&gt;Deep Learning can be used to predict customer churn&lt;/strong&gt;. We built an ANN model using the new &lt;a href=&#34;https://tensorflow.rstudio.com/keras/&#34;&gt;keras&lt;/a&gt; package that achieved &lt;strong&gt;82% predictive accuracy&lt;/strong&gt; (without tuning)! We used three new machine learning packages to help with preprocessing and measuring performance: &lt;a href=&#34;https://topepo.github.io/recipes&#34;&gt;recipes&lt;/a&gt;, &lt;a href=&#34;https://topepo.github.io/rsample/&#34;&gt;rsample&lt;/a&gt; and &lt;a href=&#34;https://github.com/topepo/yardstick&#34;&gt;yardstick&lt;/a&gt;. Finally we used &lt;a href=&#34;https://github.com/thomasp85/lime&#34;&gt;lime&lt;/a&gt; to explain the Deep Learning model, which &lt;strong&gt;traditionally was impossible&lt;/strong&gt;! We checked the LIME results with a &lt;strong&gt;Correlation Analysis&lt;/strong&gt;, which brought to light other features to investigate. For the IBM Telco dataset, tenure, contract type, internet service type, payment menthod, senior citizen status, and online security status were useful in diagnosing customer churn. We hope you enjoyed this article!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;links&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/&#34;&gt;RStudio AI Blog: Deep Learning With Keras To Predict Customer Churn&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/bennwei/Machine_Learning_projects/blob/6d3814a596bc2f9dee09fffbf45a8e052dcefc09/Churn_analysis_R/churn_analysis.Rmd&#34;&gt;Machine_Learning_projects/churn_analysis.Rmd at 6d3814a596bc2f9dee09fffbf45a8e052dcefc09 · bennwei/Machine_Learning_projects&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Error&lt;/h2&gt;
&lt;p&gt;“Error in &lt;code&gt;dimnames&amp;lt;-.data.frame&lt;/code&gt;(&lt;code&gt;*tmp*&lt;/code&gt;, value = list(n)) : invalid ‘dimnames’ given for data frame”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/thomasp85/lime/issues/139&#34;&gt;keras explanations with dataframe data · Issue #139 · thomasp85/lime&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I had the same error, and I managed to solve it by overwriting the correct output of class(model). It seems the newer version uses different name. So, it is fixed for by:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class(model_keras)
# [1] &amp;quot;keras.engine.sequential.Sequential&amp;quot;
# [2] &amp;quot;keras.engine.training.Model&amp;quot;       
# [3] &amp;quot;keras.engine.network.Network&amp;quot;      
# [4] &amp;quot;keras.engine.base_layer.Layer&amp;quot;     
# [5] &amp;quot;python.builtin.object&amp;quot;    &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then use &lt;code&gt;keras.engine.sequential.Sequential&lt;/code&gt; instead of &lt;code&gt;keras.models.Sequential&lt;/code&gt;, i.e.:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# gives error
model_type.keras.models.Sequential &amp;lt;- function(x, ...) {
  &amp;quot;classification&amp;quot;
}

# works!
model_type.keras.engine.sequential.Sequential &amp;lt;- function(x, ...) {
  &amp;quot;classification&amp;quot;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same for &lt;code&gt;predict_model()&lt;/code&gt;, use &lt;code&gt;keras.engine.sequential.Sequential&lt;/code&gt;, or what is the output of &lt;code&gt;class(model)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
